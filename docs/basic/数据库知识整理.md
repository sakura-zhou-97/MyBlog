# Mysql架构设计

## 1.MySQL各部分的作用

<img src="picture/数据库知识整理.assets/5148507-ca8930bca4e10d05.png" alt="img" style="zoom: 67%;" />

## 2.Innodb的优缺点?(围绕索引数展开讲)

### InnoDB

>  MySQL 默认的事务型存储引擎，只有在需要它不支持的特性时，才考虑使用其它存储引擎。
>
> 1.**在可重复读隔离级别下，通过多版本并发控制（MVCC）+ Next-Key Locking 防止幻影读。**
>
> 2.主索引是聚簇索引，在索引中保存了数据，从而避免直接读取磁盘，因此对查询性能有很大的提升。
>
> 3.(忽略)内部做了很多优化，包括从磁盘读取数据时采用的可预测性读、能**够加快读操作并且自动创建的自适应哈希索引**、能够加速插入操作的插入缓冲区等。
>
> 4.（忽略）**支持真正的在线热备份**。其它存储引擎不支持在线热备份，要获取一致性视图需要停止对所有表的写入，而在读写混合场景中，停止写入可能也意味着停止读取。

### MyISAM

> [设计简单，数据以紧密格式存储。对于只读数据，或者表比较小、可以容忍修复操作]()，则依然可以使用它。
>
> 1.提供了大量的特性，包括压缩表、空间数据索引等。
>
> 2.**不支持事务。**
>
> 3.**不支持行级锁，只能对整张表加锁，**读取时会对需要读到的所有表加共享锁，写入时则对表加排它锁。但在表有读取操作的同时，也可以往表中插入新的记录，这被称为并发插入（CONCURRENT INSERT）。
>
> 4.可以手工或者自动执行检查和修复操作，但是和事务恢复以及崩溃恢复不同，[可能导致一些数据丢失，而且修复操作是非常慢的]()。
>
> 如果指定了 DELAY_KEY_WRITE 选项，在每次修改执行完成时，不会立即将修改的索引数据写入磁盘，而是会写到内存中的键缓冲区，只有在清理键缓冲区或者关闭表的时候才会将对应的索引块写入磁盘。这种方式可以极大的提升写入性能，但是在数据库或者主机崩溃时会造成索引损坏，需要执行修复操作。

### 使用场景

INNODB

>    ①.需要事务支持
>
>    ②.行级锁定对高并发有很好的适应能力，但需要确保查询是通过索引完成
>
>    ③.数据更新较为频繁的场景
>
>    ④.数据一致性要求较高
>
>    ⑤.硬件设备内存较大，可以利用InnoDB较好的缓存能力来提高内存利用率，尽可能减少磁盘 IO

MYISAM

> ①.不需要事务支持
>
> ②.并发相对较低（锁定机制问题）
>
> ③.数据修改相对较少（阻塞问题）
>
> ④.**以读为主**
>
> ⑤.数据一致性要求不是非常高

### InnoDB的缺陷

1. 占用资源多，索引是二级

2. 读的速度可能没有MYISAM快

3. 不支持全文索引，空间索引等

## 3.Mysql执行流程

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200609180924690.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2x6YjM0ODExMDE3NQ==,size_16,color_FFFFFF,t_70)

MySQL 执行流程，分为 5 步：

> 1. 客户端发送一条查询给服务器；
> 2. MySQL 服务器端先检查缓存。如果命中缓存，则直接返回结果；否则进入下一阶段；
> 3. MySQL 服务器端进行 SQL 解析、预处理，再由优化器生成对应的执行计划；
> 4. MySQL 根据优化器生成的执行计划，调用存储引擎的 API 来执行查询；
> 5. 将查询结果返回客户端。

### 1.MySQL 客户端/服务端通信

  MySQL 客户端与服务端的通信，采用的是"半双工"的方式。

> 单工：数据传输只能在在一个方向上传输，一方固定为发送端，另一方固定为接收端；举例：电视机
>
> 半双工：数据允许在两个方向上传输。在某一时刻，只允许数据在一个方向上传输，即：同一时间只可以有一方接收或者发送消息，可以实现双向通信。   举例：对讲机
>
> 全双工：数据允许 同时 在两个方向上传输。即：发送设备和接收设备都具有独立的接收和发送能力，在同一时间可以同时接收和发送消息，实现双向通信。举例：电话通信

### 2.MySQL 查询缓存

  数据库连接成功，这个时候就可以执行SQL语句了。[MySQL 默认是关闭缓存的，如需使用缓存，需要我们手动开启。]()

  执行语句时，MySQL 首先会去查询缓存，查看之前有没有执行过同样的语句[（同样的语句指：SQL语句必须一模一样，多一个空格，少一个空格都认为不是一个同样的语句）]()。

​		如果缓存不存在，MySQL 会将执行的语句和结果 以key-value的形式存储起来；(简单理解为 key=sql，value=结果集)

缓存失效问题：

  MySQL 只要表中的数据发生变化，缓存便会失效。

  比如：我们使用语句select * from user where id = 1查询，此时 Qcache_inserts会加 1；当再次使用该语句查询，此时Qcache_hits命中数会加 1。

  此时，我们使用语句update user set name=xxx where id = 5修改表中数据，尽管修改的是 id = 5 的这条，和 id = 1 没有任何关系，但是此时你再来 select * from user where id = 1查询时， Qcache_inserts会加 1，说明缓存已经失效。

  [MySQL缓存缺点： 只要修改表中数据，尽管与已缓存的数据无关联，但是所有缓存都会失效，这是一个不好的点。]()


#### （忽略）什么情况下，数据不会被缓存？

> 加上SQL_NO_CACHE 参数将不缓存
> 有 now()、current_date() 等函数的查询，不会缓存（select *,now() from user； 这种是不会进入缓存的）
> 查询语句不涉及表，不会缓存(select version(); ----查询MySQL版本号 )
> 查询的语句是系统表，不会缓存（select * from mysql.user;）
> 查询的语句的结果，超过了 query_cache_limit 时，也不会缓存

#### MySQL默认关闭缓存原因？

> 1.在查询之前，必须先检查是否命中缓存，浪费计算资源；
>
> 2.如果查询可以被缓存，name执行完成后，MySQL发现缓存中并没有这条数据，则会将结果存入缓存，这将会带来额外的系统消耗；
>
> 3.针对表的写入或更新数据时，对应表的所有缓存将都会失效；
>
> 4.如果查询缓存很大或者碎片很多时，这个操作可能带来很大的[**系统消耗**]()。

#### MySQL缓存适用场景：

> 适用于以读为主（读写分离），数据生成之后就不常改变的业务。比如门户类、新闻类、BI 报表类、论坛类等。

### 3.MySQL 查询优化处理阶段

MySQL 的查询优化处理，分为以下三个阶段：

> **1.解析SQL**
> 通过 Lex 此法分析、Yacc语法分析，[**将 SQL 语句解析成解析树**]()。
>
> **2.预处理阶段**
>
> 根据 MySQL 的语法规则，进一步[**检查解析树的合法性**]()。
>
> 如：检查数据的表和列是否存在，解析名字和别名的设置等，还会进行权限的相关验证操作
>
> **查询优化器阶段**
> 查询优化器，可以将[**解析树转化为执行计划**]()。一条查询可以由很多种执行方式，最后都返回相同的结果。优化器的作用就是找到这其中最好的执行计划。

#### MySQL执行计划

> 执行计划，通常是开发者优化SQL语句的第一步。
>
> MySQL 在解析SQL语句时，会生成多套执行方案，它的查询优化器是基于成本计算原则，它会尝试各种执行计划，以数据抽样的方式进行试验（随机的读取一个 4K 的数据块进行分析），最终选择一个最优的方案执行。然后根据这个方案来生成一个执行计划。
>
> 开发者通过[**查看 SQL 语句的执行计划（explain）**]()，可以直观的了解到 MySQL 是如何解析这条 SQL 语句的，然后再针对性的进行优化操作。

### 4.查询执行引擎

  根据执行计划来完成整个查询。这里执行计划是一个数据结构。

### 5.返回结果到客户端

  查询执行的最后一个阶段是将结果返回给客户端。[**即使查询不需要返回结果给客户端，MySQL仍然会返回这个查询的一些信息，如查询影响到的行数。**]()

  1. 如果查询可以被缓存，那么MySQL在这个阶段，会将结果存放到查询缓存中。

  2. 如果 MySQL 将结果返回客户端是一个[**增量、逐步返回**]()的过程。例如，在关联表操作时，一旦服务器处理完最后一个关联表，开始生成第一条结果时，MySQL就可以开始向客户端逐步返回结果集了。这样处理有两个好处：① 服务器无需存储太多的结果，也就不会因为要返回太多的结果而消耗太多的内存。② 这样的处理也让MySQL客户端第一时间获得返回的结果。

  结果集中的每一行都会以一个满足MySQL客户端/服务器通信协议的封包发送，再通过TCP协议进行传输，在TCP传输过程中，可能对MySQL的封包进行缓存然后批量传输。

# 基础

## 1.聚合函数

### 一、MySQL 5.7中的聚合函数

　　MySQL 5.7中的聚合函数如下：

<img src="picture/数据库知识整理.assets/1190675-20190530222319658-845813868.png" alt="img" style="zoom: 50%;" />

　　除非另有说明，否则组合函数会忽略NULL值。

### 二、聚合函数详解

### 　　2.1 AVG()

```sql
AVG([DISTINCT] expr)
```

　　函数返回expr的平均值。  DISTINCT则用于返回expr的不同值的平均值。

　　如果没有匹配的行，AVG()返回null。

### 　　2.2 COUNT()

```sql
COUNT(expr)
```

　　返回SELECT语句检索的行中expr的非NULL值的计数。

　　返回结果是BIGINT值。

　　如果没有匹配的行，count()返回0.

　　count(*)有些不同，它返回取回的行的行数的计数，无论它们是否包含NULL值。

　　对于诸如InnoDB之类的事务存储引擎，存储精确的行数是有问题的。多个事务可能同时发生，每个事务都可能影响计数。所以InnoDB不在内部保留表的行数，因为并发事务可能同时“看到”不同数量的行。因此，[SELECT COUNT(*)语句只计算当前事务可见的行]()。

### 　　2.3 COUNT(DISTINCT ...)

```sql
COUNT(DISTINCT expr,[expr...])
```

　　函数返回返回不相同且非NULL的expr值的行数。

　　如果没有匹配的行，则COUNT(DISTINCT)返回0。

　　在MySQL中，您可以通过提供表达式列表，来获取不包含NULL的不同表达式组合的数量。而在标准表达式中，必须在COUNT(DISTINCT ...)中对所有表达式进行连接。

### 　　2.4 GROUP_CONCAT()

```sql
GROUP_CONCAT([DISTINCT] expr [,expr ...]
             [ORDER BY {unsigned_integer | col_name | expr}
                 [ASC | DESC] [,col_name ...]]
             [SEPARATOR str_val])
```

　　这个函数把来自同一个组的某一列（或者多列）的数据连接起来成为一个字符串。

　　如果没有非NULL值，返回NULL。

　　示例如下：

```sql
/*001*/
/*成绩表中只对学生ID分组*/
select SId, group_concat(cId),group_concat(score) from sc group by SId;
```

　　结果如下，并没有排序

<img src="picture/数据库知识整理.assets/1190675-20190530234247348-241694121.png" alt="img" style="zoom:50%;" />

```sql
/*002*/
/*排序后连接，改变分隔符*/
select SId, group_concat(cId),group_concat(score order by score desc separator '  ') 
from sc group by SId;
```

　　结果如下：

<img src="picture/数据库知识整理.assets/1190675-20190530234626091-1429876757.png" alt="img" style="zoom:50%;" />

　　至于对多个expr的连接，试了试，会把两个字段无缝连在一起。

```sql
select SId, group_concat(cId,score),group_concat(score) from sc group by SId;
```

<img src="picture/数据库知识整理.assets/1190675-20190530234956488-1579352376.png" alt="img" style="zoom:50%;" />

　　返回的结果类型为TEXT或BLOB，除非group_concat_max_len小于或等于512，这种情况下，结果类型为VARCHAR或VARBINARY。

### 　　2.5 JSON_ARRAYAGG(col or expr)

　　将结果集聚合为单个JSON数组，其元素由参数列的值组成。此数组中元素的顺序未定义。该函数作用于计算为单个值的列或表达式。

　　异常返回NULL。

　　示例如下：

<img src="picture/数据库知识整理.assets/1190675-20190531000039283-1561258657.png" alt="img" style="zoom: 50%;" />

### 　　2.6 JSON_OBJECTAGG(key,value)

　　两个列名或表达式作为参数，第一个用作键，第二个用作值，并返回包含键值对的JSON对象。

　　如果结果不包含任何行，或者出现错误，则返回NULL。如果任何键名称为NULL或参数数量不等于2，则会发生错误。

 <img src="picture/数据库知识整理.assets/1190675-20190531000338812-1001916248.png" alt="img" style="zoom:50%;" />

## 2.窗口函数

![在这里插入图片描述](picture/数据库知识整理.assets/2019022312202720.png)

### 1.窗口函数和普通聚合函数的区别

①聚合函数是将多条记录聚合为一条；窗口函数是每条记录都会执行，有几条记录执行完还是几条。
②聚合函数也可以用于窗口函数。

### 2.窗口函数的基本用法

> 函数名 OVER 子句

over关键字用来指定函数执行的窗口范围，若后面括号中什么都不写，则意味着窗口包含满足WHERE条件的所有行，窗口函数基于所有行进行计算；如果不为空，则支持以下4中语法来设置窗口。
①window_name：给窗口指定一个别名。如果SQL中涉及的窗口较多，采用别名可以看起来更清晰易读；
②PARTITION BY 子句：窗口按照哪些字段进行分组，窗口函数在不同的分组上分别执行；
③ORDER BY子句：按照哪些字段进行排序，窗口函数将按照排序后的记录顺序进行编号；
④FRAME子句：FRAME是当前分区的一个子集，子句用来定义子集的规则，通常用来作为滑动窗口使用。

### 3.序号函数

序号函数：ROW_NUMBER()、RANK()、DENSE_RANK()
用途：显示分区中的当前行号[（**经典面试问题-topN问题**）]()
应用场景：查询每个学生的分数最高的前3门课程

```sql
mysql> SELECT *
    -> FROM(
    ->     SELECT stu_id,
    ->     ROW_NUMBER() OVER (PARTITION BY stu_id ORDER BY score DESC) AS score_
order,
    ->     lesson_id, score
    ->     FROM t_score) t
    -> WHERE score_order <= 3
    -> ;
+--------+-------------+-----------+-------+
| stu_id | score_order | lesson_id | score |
+--------+-------------+-----------+-------+
|      1 |           1 | L005      |    98 |
|      1 |           2 | L001      |    98 |
|      1 |           3 | L004      |    88 |
|      2 |           1 | L002      |    90 |
|      2 |           2 | L003      |    86 |
|      2 |           3 | L001      |    84 |
|      3 |           1 | L001      |   100 |
|      3 |           2 | L002      |    91 |
|      3 |           3 | L003      |    85 |
|      4 |           1 | L001      |    99 |
|      4 |           2 | L005      |    98 |
|      4 |           3 | L002      |    88 |
+--------+-------------+-----------+-------+

```

> 对于stu_id=1的同学，有两门课程的成绩均为98，序号随机排了1和2。但很多情况下二者应该是并列第一，则他的成绩为88的这门课的序号可能是第2名，也可能为第3名。
> 这时候，ROW_NUMBER()就不能满足需求，需要RANK()和DENSE_RANK()出场，它们和ROW_NUMBER()非常类似，只是在出现重复值时处理逻辑有所不同。

```java
mysql> SELECT *
    -> FROM(
    ->     SELECT
    ->     ROW_NUMBER() OVER (PARTITION BY stu_id ORDER BY score DESC) AS score_order1,
    ->     RANK() OVER (PARTITION BY stu_id ORDER BY score DESC) AS score_order2,
    ->     DENSE_RANK() OVER (PARTITION BY stu_id ORDER BY score DESC) AS score_order3,
    ->     stu_id, lesson_id, score
    ->     FROM t_score) t
    -> WHERE stu_id = 1 AND score_order1 <= 3 AND score_order2 <= 3 AND score_order3 <= 3
    -> ;
+--------------+--------------+--------------+--------+-----------+-------+
| score_order1 | score_order2 | score_order3 | stu_id | lesson_id | score |
+--------------+--------------+--------------+--------+-----------+-------+
|            1 |            1 |            1 |      1 | L005      |    98 |
|            2 |            1 |            1 |      1 | L001      |    98 |
|            3 |            3 |            2 |      1 | L004      |    88 |
+--------------+--------------+--------------+--------+-----------+-------+

```

ROW_NUMBER()：顺序排序——1、2、3
RANK()：并列排序，跳过重复序号——1、1、3
DENSE_RANK()：并列排序，不跳过重复序号——1、1、2

## 3.count（*）、count（1）、count(id)、count(字段)的区别和性能分析

![img](picture/数据库知识整理.assets/20190827142327619.png)

count(*)是找一列NOT NULL的列，如果该列有索引，则使用该索引，当然，为了性能，SQL Server会选择最窄的索引以减少IO。因此，如果某个表上Count（）用的比较多时，考虑在一个最短的列建立一个单列索引，会极大的提升性能。

# 索引

## 1.MySql的四大索引

> B+索引
>
> Hash索引
>
> FULLTEXT索引
>
> 空间索引

**B+树索引**

相对Hash索引，B+树在查找单条记录的速度比不上Hash索引，但是因为更适合排序，区间查找等操作

> **1.带顺序访问指针的B+Tree**
>
> B+Tree所有索引数据都在叶子结点上，并且增加了顺序访问指针,每个叶子节点都有指向相邻叶子节点的指针。
>
> 这样做是为了提高区间查询效率，例如查询key为从18到49的所有数据记录，当找到18后，只需顺着节点和指针顺序遍历就可以一次性访问到所有数据节点，极大提到了区间查询效率。
>
> **2.大大减少磁盘I/O读取**
>
> 数据库系统的设计者巧妙利用了磁盘预读原理，将一个节点的大小设为等于一个页，这样每个节点只需要一次I/O就可以完全载入。

**Hash索引**

> 1. 仅仅能满足"=","IN"和"<=>"查询，不能使用范围查询和排序
> 2. 其检索效率非常高，索引的检索可以一次定位，所以 Hash 索引的查询效率要远高于 B-Tree 索引
> 3. 只有Memory存储引擎支持hash索引

**FULLTEXT索引**

> 主要用来查找文本中的关键字，而不是直接与索引中的值相比较。
>
> 全文索引使用倒排索引实现，它记录着关键词到其所在文档的映射。
>
> fulltext索引配合match against操作使用，而不是一般的where语句加like。

**R-Tree空间索引**

（用于对GIS数据类型创建SPATIAL索引）

> MyISAM 存储引擎支持空间数据索引（R-Tree），可以用于地理数据存储。
>
> 空间数据索引会从所有维度来索引数据，可以有效地使用任意维度来进行组合查询。必须使用 GIS 相关的函数来维护数据。

## 2.Mysql的常见索引类型

```sql
create index index_name on table_name(column_list); 
```

**Mysql常见索引有：**主键索引、唯一索引、普通索引、全文索引、组合索引

```sql
PRIMARY KEY（主键索引） ALTER TABLE `table_name` ADD PRIMARY KEY (`col`) 

UNIQUE(唯一索引)       ALTER TABLE `table_name` ADD UNIQUE (`col`)

INDEX(普通索引)        ALTER TABLE `table_name` ADD INDEX index_name (`col`)

FULLTEXT(全文索引)     ALTER TABLE `table_name` ADD FULLTEXT (`col`)

组合索引  ALTER TABLE `table_name` ADD INDEX index_name (`col1`, `col2`, `col3`) 
```

**Mysql各种索引区别：**

```
普通索引：最基本的索引，没有任何限制
唯一索引：与"普通索引"类似，不同的就是：索引列的值必须唯一，且允许有空值。
主键索引：它是一种特殊的唯一索引，不允许有空值。 
全文索引：仅可用于 MyISAM 表，针对较大的数据，生成全文索引很耗时耗空间。
联合索引：为了更多的提高mysql效率可建立组合索引，遵循”最左前缀“原则。创建复合索引时应该将最常用（频率）作限制条件的列放在最左边，依次递减。
联合索引的好处：覆盖索引，这一点是最重要的，覆盖索引可以直接在非主键索引上拿到相应的值，减少一次查询。
```

## 3.什么是聚簇索引?什么是非聚簇索引?

```
聚簇索引：将数据存储与索引放到了一块，找到索引也就找到了数据。只要索引是相邻的，那么对应的数据一定也是相邻地存放在磁盘上的。聚簇索引要比非聚簇索引查询效率高很多。聚集索引这种主+辅索引的好处是，当发生数据行移动或者页分裂时，辅助索引树不需要更新，因为辅助索引树存储的是主索引的主键关键字，而不是数据具体的物理地址。

非聚簇索引：B+Tree的叶子节点上的data，并不是数据本身，而是数据存放的地址。主索引和辅助索引没啥区别，只是主索引中的key一定得是唯一的。
```

![img](https:////upload-images.jianshu.io/upload_images/10154499-5244179cc19a1c21.PNG?imageMogr2/auto-orient/strip|imageView2/2/w/164/format/webp)

![img](https:////upload-images.jianshu.io/upload_images/10154499-5772dddedb909374.PNG?imageMogr2/auto-orient/strip|imageView2/2/w/633/format/webp)

1. InnoDB使用的是聚簇索引，将主键组织到一棵B+树中，而行数据就储存在叶子节点上，若使用"where id = 14"这样的条件查找主键，则按照B+树的检索算法即可查找到对应的叶节点，之后获得行数据。
2. 若对Name列进行条件搜索，则需要两个步骤：第一步在辅助索引B+树中检索Name，到达其叶子节点获取对应的主键。第二步使用主键在主索引B+树种再执行一次B+树检索操作，最终到达叶子节点即可获取整行数据。（重点在于通过其他键需要建立辅助索引）

**优势：**

> - 可以提高数据检索的效率
> - **降低数据排序的成本**，降低了CPU的消耗。
>   - 被索引的列会自动进行排序，包括【单列索引】和【组合索引】，只是组合索引的排序要复杂一些。
>   - 如果按照索引列的顺序进行排序，对应order by语句来说，效率就会提高很多。

**劣势：**

> - 索引会占据磁盘空间
> - 索引虽然会提高查询效率，但是会**降低更新表的效率**。

## 4.为什么会有最左匹配原则?(根据 B+树规则去回答)

当b+树的数据项是复合的数据结构，比如(name,age,sex)的时候，b+数是按照从左到右的顺序来建立搜索树的。

比如当(张三,20,F)这样的数据来检索的时候，b+树会优先比较name来确定下一步的所搜方向，如果name相同再依次比较age和sex，最后得到检索的数据；

但当(20,F)这样的没有name的数据来的时候，b+树就不知道下一步该查哪个节点，因为建立搜索树的时候name就是第一个比较因子，必须要先根据name来搜索才能知道下一步去哪里查询。比如当(张三,F)这样的数据来检索时，b+树可以用name来指定搜索方向，但下一个字段age的缺失，所以只能把名字等于张三的数据都找到，然后再匹配性别是F的数据了， 这个是非常重要的性质，即索引的最左匹配特性。

<img src="picture/数据库知识整理.assets/20210116225538141.png" alt="在这里插入图片描述" style="zoom:50%;" />

我们看到 最左边的a 都是有序的，分别是 ： 1，1，2，2，3，3 但是右边的b 不一定有序： 1，2，1，4，1，2

但是在a都为 1 的情况下 b是有序的， 如： a=1时 b =1，2 ； a=2时， b= 1，4； a=3时 ，b=1，2；

如果我们筛选数据的时候， 直接筛选b ，整个就是无序的，需要做全表扫描

如果先a，再b 那么 ，就可以利用树来加快查找速度。

## 5.索引覆盖?唯一索引?非唯一索引?最左原则?

这先要从InnoDB的索引实现说起，InnoDB有两大类索引：

- 聚集索引(clustered index)
- 普通索引(secondary index)

**InnoDB聚集索引和普通索引有什么差异？**

InnoDB**聚集索引**的叶子节点存储行记录，因此， InnoDB必须要有，且只有一个聚集索引：

> （1）如果表定义了PK，则PK就是聚集索引；
>
> （2）如果表没有定义PK，则第一个not NULL unique列是聚集索引；
>
> （3）否则，InnoDB会创建一个隐藏的row-id作为聚集索引；
>

*画外音：所以PK查询非常快，直接定位行记录。*

InnoDB**普通索引**的叶子节点存储主键值。

　*画外音：注意，不是存储行记录头指针，MyISAM的索引叶子节点存储记录指针。*

### **1、什么是索引覆盖(Covering index)？**

 ![img](picture/数据库知识整理.assets/885859-20190729184925229-1323143660.png)

MySQL官网，类似的说法出现在explain查询计划优化章节，即explain的输出结果Extra字段为**Using index**时，能够触发索引覆盖。

![img](picture/数据库知识整理.assets/885859-20190729184944177-1702731545.png)

> **只需要在一棵索引树上就能获取SQL所需的所有列数据，无需回表，速度更快。**

```
如果一个索引包含(或覆盖)所有需要查询的字段的值，称为覆盖索引。即只需扫描索引而无须回表。
```

### **2、如何实现索引覆盖？**

常见的方法是：将被查询的字段，建立到联合索引里去。

仍是之前中的例子：

```sql
create table user (
    id int primary key,
    name varchar(20),
    sex varchar(5),
    index(name)
)engine=innodb;
```

第一个SQL语句：　　

![img](picture/数据库知识整理.assets/885859-20190729185028557-1703422478.png)

```sql
select id,name from user where name='shenjian';
```

能够命中name索引，索引叶子节点存储了主键id，通过name的索引树即可获取id和name，无需回表，符合索引覆盖，效率较高。

*画外音，Extra：**Using index**。*

第二个SQL语句：         

![img](picture/数据库知识整理.assets/885859-20190729185053070-767208274.png)

```sql
select id,name,sex from user where name='shenjian';
```

能够命中name索引，索引叶子节点存储了主键id，但sex字段必须回表查询才能获取到，不符合索引覆盖，需要再次通过id值扫聚集索引获取sex字段，效率会降低。

*画外音，Extra：**Using index condition**。*

如果把(name)单列索引升级为联合索引(name, sex)就不同了。

![img](picture/数据库知识整理.assets/885859-20190729185140811-2063536201.png)

可以看到：

```sql
create table user (
    id int primary key,
    name varchar(20),
    sex varchar(5),
    index(name, sex)
)engine=innodb;
```

都能够命中索引覆盖，无需回表。

*画外音，Extra：**Using index**。*

### **3、哪些场景可以利用索引覆盖来优化SQL？**

**场景1：全表count查询优化**

![img](picture/数据库知识整理.assets/885859-20190729185205243-1779249721.png)

原表为：

*user(PK id, name, sex)；*

直接：

```sql
select count(name) from user;
```

不能利用索引覆盖

添加索引：

```sql
alter table user add key(name);
```

就能够利用索引覆盖提效。

**场景2：列查询回表优化**

```sql
select id,name,sex ... where name='shenjian';
```

这个例子不再赘述，将单列索引(name)升级为联合索引(name, sex)，即可避免回表。

**场景3：分页查询**

```sql
select id,name,sex ... order by name limit 500,100;
```

将单列索引(name)升级为联合索引(name, sex)，也可以避免回表。

## 6.索引的一些基本概念

### 什么是回表？

所以这里就会引申出一个概念叫回表，比如这个时候我们进行一个查询操作

```sql
select name from test where a = 30;
```

我们知道因为条件 MySQL 是会走 a 的索引的，但是 a 索引上并没有存储 name 的值，此时我们就需要拿到相应 a 上的主键值，然后通过这个主键值去走聚簇索引 最终拿到其中的name值，这个过程就叫回表。

我们来总结一下回表是什么？

> MySQL在辅助索引上找到对应的主键值并通过主键值在聚簇索引上查找所要的数据就叫回表。

### 索引维护

我们知道索引是需要占用空间的，索引虽能提升我们的查询速度但是也是不能滥用。

比如我们在用户表里用身份证号做主键，那么每个二级索引的叶子节点占用约20个字节，而如果用整型做主键，则只要4个字节，如果是长整型（bigint）则是8个字节。

> 所以我们可以通过缩减索引的大小来减少索引所占空间。
>
> 当然B+树为了维护索引的有序性会在删除，插入的时候进行一些必要的维护(在InnoDB中删除会将节点标记为“可复用”以减少对结构的变动)。
>

比如在增加一个节点的时候可能会遇到数据页满了的情况，这个时候就需要做页的分裂，这是一个比较耗时的工作，而且页的分裂还会导致数据页的利用率变低，比如原来存放三个数据的数据页再次添加一个数据的时候需要做页分裂，这个时候就会将现有的四个数据分配到两个数据页中，这样就减少了数据页利用率。

### 最左前缀原则

这个是以 联合索引 作为基础的，是一种联合索引的匹配规则。

最左匹配原则还有这些规则：

- 全值匹配的时候优化器会改变顺序，也就是说你全值匹配时的顺序和原先的联合索引顺序不一致没有关系，优化器会帮你调好。
- 索引匹配从最左边的地方开始，如果没有则会进行全表扫描，比如你设计了一个(a,b,c)的联合索引，然后你可以使用(a),(a,b),(a,b,c) 而你使用 (b),(b,c),(c)就用不到索引了。
- 遇到范围匹配会取消索引。比如这个时候你进行一个这样的 select 操作

```sql
select * from stu where class > 100 and name = '张三';
```

这个时候 InnoDB 就会放弃索引而进行全表扫描，因为这个时候 InnoDB 会不知道怎么进行遍历索引，所以进行全表扫描。

### 索引下推

> - 索引条件下推(Index Condition Pushdown)。MySQL5.6新添加，用于优化数据的查询。
> - [当你不使用ICP,]()**通过使用通索引进行查询**，存储引擎通过索引检索数据，然后返回给MySQL服务器，服务器再判断是否符合条件。
> - [使用ICP]()，当存在索引的列做为判断条件时，MySQL服务器将这一部分判断条件传递给存储引擎，然后存储引擎通过判断索引是否符合MySQL服务器传递的条件，只有当索引符合条件时才会将数据检索出来返回给MySQL服务器。

**适用场景**

> - 当需要整表扫描，e.g.:range,ref,eq_ref....
> - InnoDB引擎仅仅适用二级索引。（原因InnoDB聚簇索引将整行数据读到InnoDB缓冲区）。
> - 子查询条件不能下推。触发条件不能下推，调用存储过程条件不能下推。

- 在开始之前先先准备一张用户表(user)，其中主要几个字段有：id、name、age、address。建立**联合索引（name，age）**。
- 假设有一个需求，要求匹配姓名第一个为陈的所有用户，sql语句如下：

```sql
　　SELECT * from user where  name like '陈%'
```

- 根据 "最佳左前缀" 的原则，这里使用了联合索引（name，age）进行了查询，性能要比全表扫描肯定要高。
- **问题来了，如果有其他的条件呢？假设又有一个需求，要求匹配姓名第一个字为陈，年龄为20岁的用户**，此时的sql语句如下：

```sql
　　SELECT * from user where  name like '陈%' and age=20
```

- 这条sql语句应该如何执行呢？下面对Mysql5.6之前版本和之后版本进行分析。

#### Mysql5.6之前的版本

- 5.6之前的版本是没有索引下推这个优化的，因此执行的过程如下图：

<img src="review/picture/数据库成神之路.assets/1.png" alt="img" style="zoom:40%;" />

- 会忽略age这个字段，直接通过name进行查询，在(name,age)这课树上查找到了两个结果，id分别为2,1，然后拿着取到的id值一次次的回表查询，因此这个过程需要**回表两次**。
- ![img](picture/数据库知识整理.assets/aHR0cHM6Ly91c2VyLWdvbGQtY2RuLnhpdHUuaW8vMjAxOS8xMi8xMC8xNmVlZDY2N2YzOGZkM2Vl)

#### Mysql5.6及之后版本

- 5.6版本添加了索引下推这个优化，执行的过程如下图：

<img src="review/picture/数据库成神之路.assets/2.png" alt="img" style="zoom:40%;" />

- InnoDB并没有忽略age这个字段，**而是在索引内部就判断了age是否等于20，对于不等于20的记录直接跳过，因此在(name,age)这棵索引树中只匹配到了一个记录，此时拿着这个id去主键索引树中回表查询全部数据，这个过程只需要回表一次。**
- ![img](picture/数据库知识整理.assets/aHR0cHM6Ly91c2VyLWdvbGQtY2RuLnhpdHUuaW8vMjAxOS8xMi8xMC8xNmVlZDY2YWUwZDI2OThh)

#### 实践

- 当然上述的分析只是原理上的，我们可以实战分析一下，因此陈某装了Mysql5.6版本的Mysql，解析了上述的语句，如下图：

![img](picture/数据库知识整理.assets/3.png)

- 根据explain解析结果可以看出Extra的值为**Using index condition**，表示已经使用了索引下推。

### 一些最佳实践

哪些情况需要创建索引？

- **频繁作为查询条件**的字段应创建索引。
- 多表关联查询的时候，关联字段应该创建索引。
- 查询中的**排序字段**，应该创建索引。
- **统计或者分组字段**需要创建索引。

哪些情况不需要创建索引？

1. **尽量选择区分度高的列作为索引。**
2. **不要对索引进行一些函数操作**，还应注意隐式的类型转换和字符编码转换。
3. **尽可能的扩展索引，不要新建立索引**。比如表中已经有了a的索引，现在要加（a,b）的索引，那么只需要修改原来的索引即可。
4. **多考虑覆盖索引，索引下推，最左匹配。**

## 7.为什么使用B+树

不全面，还有比如说mysql是关系型数据库

![img](picture/数据库知识整理.assets/97A6DE98FF0359D70255CBB5EF053DF5)

# Mysql日志

## 1.binlog、undolog、redolog

### 一、什么是binlog

> 什么是`binlog`？

`binlog`记录了数据库表结构和表数据变更，比如`update/delete/insert/truncate/create`。它不会记录`select`

> `binlog`长什么样？

`binlog`我们可以简单理解为：存储着每条变更的`SQL`语句（不止SQL，还有[**XID「事务Id」**]()等等）

![image-20210312102351120](picture/数据库知识整理.assets/image-20210312102351120.png)

> `binlog`一般用来做什么

主要有两个作用：**复制和恢复数据**

- 从服务器需要与主服务器的数据保持一致，这就是通过`binlog`来实现的
- 数据库的数据被干掉了，我们可以通过`binlog`来对数据进行恢复。

### 二、什么是redo log

假设我们有一条sql语句：

```sql
update user_table set name='java3y' where id = '3'
```

实际上Mysql的基本存储结构是**页**(记录都存在页里边)，所以MySQL是先把这条记录所在的**页**找到，然后把该页加载到内存中，将对应记录进行修改。

**如果在内存中把数据改了，还没来得及落磁盘，而此时的数据库挂了怎么办**？

![image-20210312102655439](picture/数据库知识整理.assets/image-20210312102655439.png)

如果每个请求都需要将数据**立马**落磁盘之后，那速度会很慢，MySQL可能也顶不住。所以MySQL是怎么做的呢？

MySQL引入了`redo log`，内存写完了，然后会写一份`redo log`，这份`redo log`记载着这次**在某个页上做了什么修改**。

![image-20210312102727864](picture/数据库知识整理.assets/image-20210312102727864.png)

其实写`redo log`的时候，也会有`buffer`，是先写`buffer`，再真正落到磁盘中的。至于从`buffer`什么时候落磁盘，会有配置供我们配置。

![image-20210312102749451](picture/数据库知识整理.assets/image-20210312102749451.png)

写`redo log`也是需要写磁盘的，但它的好处就是`顺序IO`（我们都知道顺序IO比随机IO快非常多）。

> 当我们修改的时候，写完内存了，但数据还没真正写到磁盘的时候。此时我们的数据库挂了，我们可以根据`redo log`来对数据进行恢复。
>
> 因为`redo log`是顺序IO，所以**写入的速度很快**，并且`redo log`记载的是**物理变化（xxxx页做了xxx修改），文件的体积很小，恢复速度很快**。

### 三、binlog和redo log

#### 存储的内容

`binlog`记载的是`update/delete/insert`这样的SQL语句，而`redo log`记载的是物理修改的内容（xxxx页修改了xxx）。

[`redo log` 记录的是数据的**物理变化**，`binlog` 记录的是数据的**逻辑变化**]()

#### 功能

`redo log`的作用是为**持久化**而生的。`redo log`不会存储着**历史**所有数据的变更，**文件的内容会被覆盖的**。

`binlog`的作用是复制和恢复而生的。

- 主从服务器需要保持数据的一致性，通过`binlog`来同步数据。
- 如果整个数据库的数据都被删除了，`binlog`存储着所有的数据变更情况，那么可以通过`binlog`来对数据进行恢复。

#### binlog和redo log 写入的细节

`redo log`是MySQL的InnoDB引擎所产生的。

`binlog`无论MySQL用什么引擎，都会有的。

那他们的写入顺序是什么样的呢？

`redo log`**事务开始**的时候，就开始记录每次的变更信息，而`binlog`是在**事务提交**前才记录。

于是新有的问题又出现了：我写其中的某一个`log`，失败了，那会怎么办？现在我们的前提是先写`redo log`，再写`binlog`，我们来看看：

- 如果写`redo log`失败了，那我们就认为这次事务有问题，回滚，不再写`binlog`。
- 如果写`redo log`成功了，写`binlog`，写`binlog`写一半了，但失败了怎么办？我们还是会对这次的**事务回滚**，将无效的`binlog`给删除（因为`binlog`会影响从库的数据，所以需要做删除操作）
- 如果写`redo log`和`binlog`都成功了，那这次算是事务才会真正成功。

简单来说：MySQL需要保证`redo log`和`binlog`的**数据是一致**的，如果不一致，那就乱套了。

- 如果`redo log`写失败了，而`binlog`写成功了。那假设内存的数据还没来得及落磁盘，机器就挂掉了。那主从服务器的数据就不一致了。（从服务器通过`binlog`得到最新的数据，而主服务器由于`redo log`没有记载，没法恢复数据）
- 如果`redo log`写成功了，而`binlog`写失败了。那从服务器就拿不到最新的数据了。

MySQL通过**两阶段提交**来保证`redo log`和`binlog`的数据是一致的。

顺序：[**内存 -> redolog(prepare) -> binlog -> commit -> redolog(commit)**]()

![img](picture/数据库知识整理.assets/5148507-1a29473c24f0c5b7.png)

过程：

- 阶段1：InnoDB`redo log` 写盘，InnoDB 事务进入 `prepare` 状态
- 阶段2：`binlog` 写盘，InooDB 事务进入 `commit` 状态
- 每个事务`binlog`的末尾，会记录一个 `XID event`，标志着事务是否提交成功，也就是说，恢复过程中，`binlog` 最后一个 XID event 之后的内容都应该被 purge。

![image-20210312104333068](picture/数据库知识整理.assets/image-20210312104333068.png)

### 四、什么是undo log

> `undo log`有什么用？

`undo log`主要有两个作用：回滚和多版本控制(MVCC)[**(原子性)**]()

在数据修改的时候，不仅记录了`redo log`，还记录`undo log`，如果因为某些原因导致事务失败或回滚了，可以用`undo log`进行回滚

`undo log`主要存储的也是逻辑日志，比如我们要`insert`一条数据了，那`undo log`会记录的一条对应的`delete`日志。我们要`update`一条记录时，它会记录一条对应**相反**的update记录。

因为`undo log`存储着修改之前的数据，相当于一个**前版本**，MVCC实现的是读写不阻塞，读的时候只要返回前一个版本的数据就行了。

## 2.Innodb 是怎么保证崩溃恢复能力的?(两阶段日志提交)

![5353c53e13c55ada897ebae0711474f1.png](picture/数据库知识整理.assets/5353c53e13c55ada897ebae0711474f1.png)

lsn: 

> 可以理解为数据库从创建以来产生的redo日志量，这个值越大，说明数据库的更新越多，也可以理解为更新的时刻。
>
> 此外，每个数据页上也有一个lsn，表示最后被修改时的lsn，值越大表示越晚被修改。比如，数据页A的lsn为100，数据页B的lsn为200，checkpoint lsn为150，系统lsn为300，表示当前系统已经更新到300，小于150的数据页已经被刷到磁盘上，因此数据页A的最新数据一定在磁盘上，而数据页B则不一定，有可能还在内存中。

redo日志: 

> 现代数据库都需要写redo日志，例如修改一条数据，首先写redo日志，然后再写数据。在写完redo日志后，就直接给客户端返回成功。
>
> 这样虽然多写了一次盘，但是由于把对磁盘的随机写入(写数据)转换成了顺序的写入(写redo日志)，性能有很大幅度的提高。
>
> 当数据库挂了之后，通过扫描redo日志，就能找出那些没有刷盘的数据页(在崩溃之前可能数据页仅仅在内存中修改了，但是还没来得及写盘)，保证数据不丢。

undo日志: 

> 数据库还提供类似撤销的功能，当你发现修改错一些数据时，可以使用rollback指令回滚之前的操作。这个功能需要undo日志来支持。此外，现代的关系型数据库为了提高并发，都实现了类似MVCC的机制，在InnoDB中，这个也依赖undo日志。
>
> 为了实现统一的管理，与redo日志不同，undo日志在Buffer Pool中有对应的数据页，与普通的数据页一起管理，依据LRU规则也会被淘汰出内存，后续再从磁盘读取。
>
> 与普通的数据页一样，对undo页的修改，也需要先写redo日志。

检查点: 英文名为checkpoint。

> 数据库为了提高性能，数据页在内存修改后并不是每次都会刷到磁盘上。
>
> checkpoint之前的数据页保证一定落盘了，这样之前的日志就没有用了(由于InnoDB redolog日志循环使用，这时这部分日志就可以被覆盖)，所以checkpoint之后的日志在崩溃恢复的时候还是需要被使用的。
>
> InnoDB会依据脏页的刷新情况，定期推进checkpoint，从而减少数据库崩溃恢复的时间。检查点的信息在第一个日志文件的头部。

崩溃恢复: 

> 用户修改了数据，并且收到了成功的消息，然而对数据库来说，可能这个时候修改后的数据还没有落盘，如果这时候数据库挂了，重启后，数据库需要从日志中把这些修改后的数据给捞出来，重新写入磁盘，保证用户的数据不丢。这个从日志中捞数据的过程就是崩溃恢复的主要任务，也可以称为数据库前滚。
>
> **当然，在崩溃恢复中还需要回滚没有提交的事务，提交没有提交成功的事务。由于回滚操作需要undo日志的支持，undo日志的完整性和可靠性需要redo日志来保证，所以崩溃恢复先做redo前滚，然后做undo回滚。**

```
为什么需要redolog？（选读）
Undo Log是为了实现事务的原子性，在MySQL数据库InnoDB存储引擎中，还用了Undo Log来实现多版本并发控制(简称：MVCC)。

		事务的原子性(Atomicity)事务中的所有操作，要么全部完成，要么不做任何操作，不能只做部分操作。如果在执行的过程中发生了错误，要回滚(Rollback)到事务开始前的状态，就像这个事务从来没有执行过。
		
		Undo Log的原理很简单，为了满足事务的原子性，在操作任何数据之前，首先将数据备份到一个地方(这个存储数据备份的地方称为UndoLog)。然后进行数据的修改。如果出现了错误或者用户执行了ROLLBACK语句，系统可以利用Undo Log中的备份将数据恢复到事务开始之前的状态。
		
		之所以能同时保证原子性和持久化，是因为以下特点：
		1.更新数据前记录Undo log。
		2.为了保证持久性，必须将数据在事务提交前写到磁盘。只要事务成功提交，数据必然已经持久化。
		3.Undo log必须先于数据持久化到磁盘。如果在G,H之间系统崩溃，undo log是完整的， 可以用来回滚事务。
		如果在A-F之间系统崩溃,因为数据没有持久化到磁盘。所以磁盘上的数据还是保持在事务开始前的状态。
		
		缺陷：每个事务提交前将数据和Undo Log写入磁盘，这样会导致大量的磁盘IO，因此性能很低。
如果能够将数据缓存一段时间，就能减少IO提高性能。但是这样就会丧失事务的持久性。因此引入了另外一种机制来实现持久化，即Redo Log。

Redo Log:
原理和Undo Log相反，Redo Log记录的是新数据的备份。在事务提交前，只要将Redo Log持久化即可，不需要将数据持久化。当系统崩溃时，虽然数据没有持久化，但是Redo Log已经持久化。系统可以根据Redo Log的内容，将所有数据恢复到最新的状态。
```

## 3.Mysql怎么保证ACID

1.原子性(Automic)：一个动作要么做完，要么不做。

2.一致性(Consistency)：保证数据处于一致性的状态，我理解就是保证数据有意义的。

3.隔离性(Isolation)：多个事务并行的结果，应该和多个事务串行的结果一致。

4.持久性(Duration)：一个事务一旦成功提交，对数据改变是永久性的。

这个四个属性中，最重要的是一致性，也就是说其他的三个属性都是为了保证一致性而存在。

**下面引用选读**

> ### 原子性
>
> **如果无法保证原子性会怎么样？**
> B账户增加50元操作失败。系统将无故丢失50元~
>
> ### 隔离性
>
> **如果无法保证隔离性会怎么样？**
> 假设A账户有200元，B账户0元。A账户往B账户转账两次，金额为50元，分别在两个事务中执行。
>
> A扣款两次，而B只加款一次，凭空消失了50元，依然出现了**数据不一致**的情形！
>
> （A，C账户各向B转账100元，先扣款AC，然后同事读取到B为0元，加上100写入磁盘，结果B账户只有100元）
>
> ![image-20210312111516799](picture/数据库知识整理.assets/image-20210312111516799.png)
>
> ### 持久性
>
> **如果无法保证持久性会怎么样？**
>
> 设想一下，系统提示你转账成功。但是你发现金额没有发生任何改变。
>
> 此时数据出现了不合法的数据状态，我们将这种状态认为是**数据不一致**的情形。
>
> ### 一致性
>
> 根据定义，一致性是指事务执行前后，数据处于一种合法的状态，这种状态是语义上的而不是语法上的。
> 那什么是合法的数据状态呢？
> 这状态是由你自己来定义的。满足这个状态，数据就是一致的，不满足这个状态，数据就是不一致的！
>
> **如果无法保证一致性会怎么样？**
> 例一:A账户有200元，转账300元出去，此时A账户余额为-100元。你自然就发现了此时数据是不一致的，为什么呢？因为你定义了一个状态，余额这列必须大于0。
> 例二:A账户200元，转账50元给B账户，A账户的钱扣了，但是B账户因为各种意外，余额并没有增加。你也知道此时数据是不一致的，为什么呢？因为你定义了一个状态，要求A+B的余额必须不变。

#### *问题一：Mysql怎么保证一致性的？*


从数据库层面，数据库通过原子性、隔离性、持久性来保证一致性。也就是说ACID四大特性之中，[C(一致性)是目的，A(原子性)、I(隔离性)、D(持久性)是手段，是为了保证一致性，数据库提供的手段。]()

但是，如果你在事务里故意写出违反约束的代码，一致性还是无法保证的。例如，你在转账的例子中，你的代码里故意不给B账户加钱，那一致性还是无法保证。因此，还必须从应用层角度考虑。

从应用层面，通过代码判断数据库数据是否有效，然后决定回滚还是提交数据！

#### *问题二: Mysql怎么保证原子性的？*（undolog）

`undo log`名为回滚日志，是实现原子性的关键，当事务回滚时能够撤销所有已经成功执行的sql语句，他需要记录你要回滚的相应日志信息。
例如

- (1)当你delete一条数据的时候，就需要记录这条数据的信息，回滚的时候，insert这条旧数据
- (2)当你update一条数据的时候，就需要记录之前的旧值，回滚的时候，根据旧值执行update操作
- (3)当年insert一条数据的时候，就需要这条记录的主键，回滚的时候，根据主键执行delete操作

`undo log`记录了这些回滚需要的信息，当事务执行失败或调用了rollback，导致事务需要回滚，便可以利用undo log中的信息将数据回滚到修改之前的样子。

#### *问题三: Mysql怎么保证持久性的？*（redolog）

*采用redo log的好处？*
其实好处就是将`redo log`进行刷盘比对数据页刷盘效率高，具体表现如下

- `redo log`体积小，毕竟只记录了哪一页修改了啥，因此体积小，刷盘快。
- `redo log`是一直往末尾进行追加，属于顺序IO。效率显然比随机IO来的快。

#### *问题四: Mysql怎么保证隔离性的？*（MVCC）

一个行记录数据有多个版本对快照数据，这些快照数据在`undo log`中。

如果一个事务读取的行正在做DELELE或者UPDATE操作，读取操作不会等行上的锁释放，而是读取该行的快照版本。
但是有一点说明一下，在事务隔离级别为读已提交(Read Commited)时，一个事务能够读到另一个事务已经提交的数据，是不满足隔离性的。但是当事务隔离级别为可重复读(Repeateable Read)中，是满足隔离性的。

## 4.binlog日志的格式？row格式的优点

<img src="picture/数据库知识整理.assets/046a517fe5dd4316a64401bfa52e85f4.jpeg" alt="img" style="zoom: 80%;" />

### 1.Statement（记录sql）

> 每一条会修改数据的sql都会记录在binlog中

优点:

- binlog文件较小

- 日志是包含用户执行的原始SQL,方便统计和审计

- 出现最早，兼容较好


缺点：

- 存在安全隐患，可能导致主从不一致

- 对一些系统函数不能准确复制或是不能复制


### 2.ROW（记录修改的数据）

> 仅保存哪条记录被修改。
>
> 所以比如update age=20 from user where sex=男就需要记录很多条修改记录而statement就只需要记录一条sql

优点:

- 相比statement更加安全的复制格式

- 在某些情况下复制速度更快(SQL复杂，表有主键)

- 系统的特殊函数也可以复制

- 更少的锁

- 更新和删除语句检查是否有主键，如果有则直接执行，如果没有，看是否有二级索引，如再没有，则全表扫描


缺点：

- binlog比较大，单语句更新(删除)表的行数过多，会形成大量binlog (myql5.6支持binlog_row_image)

- 无法从binlog看见用户执行SQL (5.6中增加binlog_row_query_log_events记录用户的query)


### 3.Mixed

> 是以上两种level的混合使用

一般的语句修改使用statment格式保存binlog，如一些函数，statement无法完成主从复制的操作，则采用row格式保存binlog。

MySQL会根据执行的每一条具体的sql语句来区分对待记录的日志形式，也就是在Statement和Row之间选择一种。

> 新版本的MySQL中对row level模式也被做了优化，并不是所有的修改都会以row level来记录，像遇到表结构变更的时候就会以statement模式来记录。至于update或者delete等修改数据的语句，还是会记录所有行的变更。

优点:

- 混合使用row和statement格式，对于DDL记录statument,对于table里的行操作记录为row格式。

- **如果使用innodb表，事务级别使用了READ_COMMITTED or READ_UMCOMMITTED日志级别只能使用row格式。**

- 但是使用ROW格式中DDL语句还是会记录成statement格式。

# 数据库锁

## 1.一次封锁or两段锁？

因为有大量的并发访问，为了预防死锁，一般应用中推荐使用一次封锁法，就是在方法的开始阶段，已经预先知道会用到哪些数据，然后全部锁住，在方法运行之后，再全部解锁。这种方式可以有效的避免循环死锁，但在数据库中却不适用，因为在事务开始阶段，数据库并不知道会用到哪些数据。

数据库遵循的是两段锁协议，将事务分成两个阶段，加锁阶段和解锁阶段（所以叫两段锁）

- 加锁阶段：在该阶段可以进行加锁操作。在对任何数据进行读操作之前要申请并获得S锁（共享锁，其它事务可以继续加共享锁，但不能加排它锁），在进行写操作之前要申请并获得X锁（排它锁，其它事务不能再获得任何锁）。加锁不成功，则事务进入等待状态，直到加锁成功才继续执行。
- 解锁阶段：当事务释放了一个封锁以后，事务进入解锁阶段，在该阶段只能进行解锁操作不能再进行加锁操作。

| 事务                 | 加锁/解锁处理                                      |
| :------------------- | :------------------------------------------------- |
| begin；              |                                                    |
| insert into test ….. | 加insert对应的锁                                   |
| update test set…     | 加update对应的锁                                   |
| delete from test ….  | 加delete对应的锁                                   |
| commit;              | 事务提交时，同时释放insert、update、delete对应的锁 |

这种方式虽然无法避免死锁，但是两段锁协议可以保证事务的并发调度是串行化（串行化很重要，尤其是在数据恢复和备份的时候）的。

## 2.事务中的加锁方式

### 事务的四种隔离级别

在数据库操作中，为了有效保证并发读取数据的正确性，提出的事务隔离级别。我们的数据库锁，也是为了构建这些隔离级别存在的。

![img](picture\数据库.assets\68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f696d6167652d32303139313230373232333430303738372e706e67?lastModify=1617001909)

- 未提交读(Read Uncommitted)：允许脏读，也就是可能读取到其他会话中未提交事务修改的数据
- 提交读(Read Committed)：只能读取到已经提交的数据。Oracle等多数数据库默认都是该级别 (不重复读)
- 可重复读(Repeated Read)：可重复读。在同一个事务内的查询都是事务开始时刻一致的，InnoDB默认级别。在SQL标准中，该隔离级别消除了不可重复读，但是还存在幻象读
- 串行读(Serializable)：完全串行化的读，每次读都需要获得表级共享锁，读写相互都会阻塞

### Read Committed（读取提交内容）

在RC级别中，数据的读取都是不加锁的，但是数据的写入、修改和删除是需要加锁的。效果如下

```sql
MySQL> show create table class_teacher \G\
Table: class_teacher
Create Table: CREATE TABLE `class_teacher` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `class_name` varchar(100) COLLATE utf8mb4_unicode_ci NOT NULL,
  `teacher_id` int(11) NOT NULL,
  PRIMARY KEY (`id`),
  KEY `idx_teacher_id` (`teacher_id`)
) ENGINE=InnoDB AUTO_INCREMENT=5 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci
1 row in set (0.02 sec)

MySQL> select * from class_teacher;
+----+--------------+------------+
| id | class_name   | teacher_id |
+----+--------------+------------+
|  1 | 初三一班     |          1 |
|  3 | 初二一班     |          2 |
|  4 | 初二二班     |          2 |
+----+--------------+------------+
```

由于MySQL的InnoDB默认是使用的RR级别，所以我们先要将该session开启成RC级别，并且设置binlog的模式

```sql
SET session transaction isolation level read committed;
SET SESSION binlog_format = 'ROW';（或者是MIXED）
```

| 事务A                                                        | 事务B                                                        |
| :----------------------------------------------------------- | :----------------------------------------------------------- |
| begin;                                                       | begin;                                                       |
| update class_teacher set class_name=‘初三二班’ where teacher_id=1; | update class_teacher set class_name=‘初三三班’ where teacher_id=1; |
| commit;                                                      |                                                              |

为了防止并发过程中的修改冲突，事务A中MySQL给teacher_id=1的数据行加锁，并一直不commit（释放锁），那么事务B也就一直拿不到该行锁，wait直到超时。

这时我们要注意到，teacher_id是有索引的，如果是没有索引的class_name呢？update class_teacher set teacher_id=3 where class_name = ‘初三一班’; 那么MySQL会给整张表的所有数据行的加行锁。

这里听起来有点不可思议，但是当sql运行的过程中，MySQL并不知道哪些数据行是 class_name = ‘初三一班’的（没有索引嘛），[如果一个条件无法通过索引快速过滤，存储引擎层面就会将所有记录加锁后返回，再由MySQL Server层进行过滤。]()

但在实际使用过程当中，[MySQL做了一些改进，在MySQL Server过滤条件，发现不满足后，会调用unlock_row方法，把不满足条件的记录释放锁 (违背了二段锁协议的约束)。]()这样做，保证了最后只会持有满足条件记录上的锁，但是每条记录的加锁操作还是不能省略的。可见即使是MySQL，为了效率也是会违反规范的。

这种情况同样适用于MySQL的默认隔离级别RR。所以对一个数据量很大的表做批量修改的时候，如果无法使用相应的索引，MySQL Server过滤数据的的时候特别慢，就会出现虽然没有修改某些行的数据，但是它们还是被锁住了的现象。

### RC（不可重读）模式下的展现

读就是可重读，可重读这个概念是一事务的多个实例在并发读取数据时，会看到同样的数据行，有点抽象，我们来看一下效果。

RC（不可重读）模式下的展现

![image-20210329152105001](picture/数据库知识整理.assets/image-20210329152105001.png)

事务B修改id=1的数据提交之后，事务A同样的查询，后一次和前一次的结果不一样，这就是不可重读（重新读取产生的结果不一样）。这就很可能带来一些问题，那么我们来看看在RR级别中MySQL的表现：

![image-20210329152207002](picture/数据库知识整理.assets/image-20210329152207002.png)

我们注意到，当teacher_id=1时，事务A先做了一次读取，事务B中间修改了id=1的数据，并commit之后，事务A第二次读到的数据和第一次完全相同。所以说它是可重读的。那么MySQL是怎么做到的呢？这里姑且卖个关子，我们往下看。

### 不可重复读和幻读的区别

> 很多人容易搞混不可重复读和幻读，确实这两者有些相似。但不可重复读重点在于update和delete，而幻读的重点在于insert。

如果使用锁机制来实现这两种隔离级别，在可重复读中，该sql第一次读取到数据后，就将这些数据加锁，其它事务无法修改这些数据，就可以实现可重复读了。

但这种方法却无法锁住insert的数据，所以当事务A先前读取了数据，或者修改了全部数据，事务B还是可以insert数据提交，这时事务A就会发现莫名其妙多了一条之前没有的数据，这就是幻读，不能通过行锁来避免。

需要Serializable隔离级别 ，读用读锁，写用写锁，读锁和写锁互斥，这么做可以有效的避免幻读、不可重复读、脏读等问题，但会极大的降低数据库的并发能力。

所以说不可重复读和幻读最大的区别，就在于如何通过锁机制来解决他们产生的问题。

### 悲观锁和乐观锁

- 悲观锁

正如其名，它指的是对数据被外界（包括本系统当前的其他事务，以及来自外部系统的事务处理）修改持保守态度，因此，在整个数据处理过程中，将数据处于锁定状态。

悲观锁的实现，[往往依靠数据库提供的锁机制]()（也只有数据库层提供的锁机制才能真正保证数据访问的排他性，否则，即使在本系统中实现了加锁机制，也无法保证外部系统不会修改数据）。

在悲观锁的情况下，为了保证事务的隔离性，就需要一致性锁定读。[读取数据时给加锁，其它事务无法修改这些数据。修改删除数据时也要加锁，其它事务无法读取这些数据。]()

- 乐观锁

相对悲观锁而言，乐观锁机制采取了更加宽松的加锁机制。悲观锁大多数情况下依靠数据库的锁机制实现，以保证操作最大程度的独占性。但随之而来的就是数据库性能的大量开销，特别是对长事务而言，这样的开销往往无法承受。

而乐观锁机制在一定程度上解决了这个问题。[乐观锁，大多是基于数据版本（ Version ）记录机制实现。]()何谓数据版本？即为数据增加一个版本标识，在基于数据库表的版本解决方案中，一般是通过为数据库表增加一个 “version” 字段来实现。读取出数据时，将此版本号一同读出，之后更新时，对此版本号加一。此时，将提交数据的版本数据与数据库表对应记录的当前版本信息进行比对，如果提交的数据版本号大于数据库表当前版本号，则予以更新，否则认为是过期数据。

要说明的是，MVCC的实现没有固定的规范，每个数据库都会有不同的实现方式，这里讨论的是InnoDB的MVCC。

### MVCC在MySQL的InnoDB中的实现

在InnoDB中，会在每行数据后添加两个额外的隐藏的值来实现MVCC，这两个值一个记录这行数据何时被创建，另外一个记录这行数据何时过期（或者被删除）。 

在实际操作中，存储的是事务的版本号，每开启一个新事务，事务的版本号就会递增。 在可重读Repeatable reads事务隔离级别下：

- SELECT时，读取创建版本号 <= 当前事务版本号，删除版本号为空或>当前事务版本号。
- INSERT时，保存当前事务版本号为行的创建版本号
- DELETE时，保存当前事务版本号为行的删除版本号
- UPDATE时，插入一条新纪录，保存当前事务版本号为行创建版本号，同时保存当前事务版本号到原来删除的行

通过MVCC，虽然每行记录都需要额外的存储空间，更多的行检查工作以及一些额外的维护工作，但可以减少锁的使用，大多数读操作都不用加锁，读数据操作很简单，性能很好，并且也能保证只会读取到符合标准的行，也只锁住必要行。

在事务C中添加了一条teacher_id=1的数据commit，RR级别中应该会有幻读现象，事务A在查询teacher_id=1的数据时会读到事务C新加的数据。但是测试后发现，在MySQL中是不存在这种情况的，在事务C提交后，事务A还是不会读到这条数据。可见在MySQL的RR级别中，是解决了幻读的读问题的。参见下图

![innodb_lock_1](picture/数据库知识整理.assets/6eb5d3b1.png)

读问题解决了，根据MVCC的定义，并发提交数据时会出现冲突，那么冲突时如何解决呢？我们再来看看InnoDB中RR级别对于写数据的处理。

### 什么是当前读和快照读？

对于这种读取历史数据的方式，我们叫它快照读 ，而读取数据库当前版本数据的方式，叫当前读。

- 快照读：就是select（快照读的前提是隔离级别不是串行级别，串行级别下的快照读会退化成当前读；）

```sql
select * from table ….;
```

- 当前读：读取的是记录的最新版本，读取时还要保证其他并发事务不能修改当前记录，会对读取的记录进行加锁。

```sql
select * from table where ? lock in share mode;

select * from table where ? for update;

insert;

update ;

delete;
```

事务的隔离级别实际上都是定义了当前读的级别，MySQL为了减少锁处理（包括等待其它锁）的时间，提升并发能力，引入了快照读的概念，使得select不用加锁。而update、insert这些“当前读”，就需要另外的模块来解决了。

### 简略

#### 基本思想

在封锁一节中提到，加锁能解决多个事务同时执行时出现的并发一致性问题。在实际场景中读操作往往多于写操作，因此又引入了读写锁来避免不必要的加锁操作，例如读和读没有互斥关系。读写锁中读和写操作仍然是互斥的，而 MVCC 利用了多版本的思想，写操作更新最新的版本快照，而读操作去读旧版本快照，没有互斥关系，这一点和 CopyOnWrite 类似。

在 MVCC 中事务的修改操作（DELETE、INSERT、UPDATE）会为数据行新增一个版本快照。

脏读和不可重复读最根本的原因是事务读取到其它事务未提交的修改。在事务进行读取操作时，为了解决脏读和不可重复读问题，MVCC 规定只能读取已经提交的快照。当然一个事务可以读取自身未提交的快照，这不算是脏读。

版本号

- 系统版本号 SYS_ID：是一个递增的数字，每开始一个新的事务，系统版本号就会自动递增。
- 事务版本号 TRX_ID ：事务开始时的系统版本号。

#### Undo 日志

MVCC 的多版本指的是多个版本的快照，快照存储在 Undo 日志中，该日志通过回滚指针 ROLL_PTR 把一个数据行的所有快照连接起来。

例如在 MySQL 创建一个表 t，包含主键 id 和一个字段 x。我们先插入一个数据行，然后对该数据行执行两次更新操作。

```sql
INSERT INTO t(id, x) VALUES(1, "a");
UPDATE t SET x="b" WHERE id=1;
UPDATE t SET x="c" WHERE id=1;
```

因为没有使用 `START TRANSACTION` 将上面的操作当成一个事务来执行，根据 MySQL 的 AUTOCOMMIT 机制，每个操作都会被当成一个事务来执行，所以上面的操作总共涉及到三个事务。快照中除了记录事务版本号 TRX_ID 和操作之外，还记录了一个 bit 的 DEL 字段，用于标记是否被删除。

[![img](picture/数据库知识整理.assets/68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f696d6167652d32303139313230383136343830383231372e706e67)](https://camo.githubusercontent.com/648b989516b41055bbd1f39ee4b61a6b0a00e0d0114632dceb1c6b91ec39dca6/68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f696d6167652d32303139313230383136343830383231372e706e67)



INSERT、UPDATE、DELETE 操作会创建一个日志，并将事务版本号 TRX_ID 写入。DELETE 可以看成是一个特殊的 UPDATE，还会额外将 DEL 字段设置为 1。

#### ReadView

MVCC 维护了一个 ReadView 结构，主要包含了[**当前系统未提交的事务**]()列表 TRX_IDs {TRX_ID_1, TRX_ID_2, ...}，还有该列表的最小值 TRX_ID_MIN 和 TRX_ID_MAX。

[![img](picture/数据库知识整理.assets/68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f696d6167652d32303139313230383137313434353637342e706e67-1617004973102)](https://camo.githubusercontent.com/1fe81e9af90e3b9256a607920773eab0c685bda03fbb947a2c1a4d26ff55c21e/68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f696d6167652d32303139313230383137313434353637342e706e67)



在进行 SELECT 操作时，根据数据行快照的 TRX_ID 与 TRX_ID_MIN 和 TRX_ID_MAX 之间的关系，从而判断数据行快照是否可以使用：

- TRX_ID < TRX_ID_MIN，表示该数据行快照时在当前所有未提交事务之前进行更改的，因此可以使用。
- TRX_ID > TRX_ID_MAX，表示该数据行快照是在事务启动之后被更改的，因此不可使用。
- TRX_ID_MIN <= TRX_ID <= TRX_ID_MAX，需要根据隔离级别再进行判断：
  - **提交读**：如果 TRX_ID 在 TRX_IDs 列表中，表示该数据行快照对应的事务还未提交，则该快照不可使用。否则表示已经提交，可以使用。
  - **可重复读**：都不可以使用。因为如果可以使用的话，那么其它事务也可以读到这个数据行快照并进行修改，那么当前事务再去读这个数据行得到的值就会发生改变，也就是出现了不可重复读问题。

在数据行快照不可使用的情况下，需要沿着 Undo Log 的回滚指针 ROLL_PTR 找到下一个快照，再进行上面的判断。

### 补充

#### MVCC的实现原理

MVCC的目的就是多版本并发控制，在数据库中的实现，就是为了解决读写冲突，它的实现原理主要是依赖记录中的 3个隐式字段，undo日志 ，Read View 来实现的。所以我们先来看看这个三个point的概念

#### 隐式字段

每行记录除了我们自定义的字段外，还有数据库隐式定义的DB_TRX_ID,DB_ROLL_PTR,DB_ROW_ID等字段

- DB_TRX_ID （事务版本号）
   6byte，最近修改(修改/插入)事务ID：记录创建这条记录/最后一次修改该记录的事务ID

- DB_ROLL_PTR
   7byte，回滚指针，指向这条记录的上一个版本（存储于rollback segment里）

- DB_ROW_ID
   6byte，隐含的自增ID（隐藏主键），如果数据表没有主键，InnoDB会自动以DB_ROW_ID产生一个聚簇索引

- 实际还有一个删除flag隐藏字段, 既记录被更新或删除并不代表真的删除，而是删除flag变了

  ![img](https:////upload-images.jianshu.io/upload_images/3133209-b45e9ebf0a3d8b14.png?imageMogr2/auto-orient/strip|imageView2/2/w/927/format/webp)

如上图，DB_ROW_ID是数据库默认为该行记录生成的唯一隐式主键，DB_TRX_ID是当前操作该记录的事务ID,而DB_ROLL_PTR是一个回滚指针，用于配合undo日志，指向上一个旧版本

#### UndoLog日志

一、 比如一个有个事务插入persion表插入了一条新记录，记录如下，name为Jerry, age为24岁，隐式主键是1，事务ID和回滚指针，我们假设为NULL

![img](https:////upload-images.jianshu.io/upload_images/3133209-e52ee5ae248c5a08.png?imageMogr2/auto-orient/strip|imageView2/2/w/833/format/webp)

二、 现在来了一个事务1对该记录的name做出了修改，改为Tom

- 在事务1修改该行(记录)数据时，数据库会先对该行加排他锁

- 然后把该行数据拷贝到undo log中，作为旧记录，即在undo log中有当前行的拷贝副本

- 拷贝完毕后，修改该行name为Tom，并且修改隐藏字段的事务ID为当前事务1的ID, 我们默认从1开始，之后递增，回滚指针指向拷贝到undo log的副本记录，即表示我的上一个版本就是它

- 事务提交后，释放锁

  ![img](https:////upload-images.jianshu.io/upload_images/3133209-3b89396902dbf513.png?imageMogr2/auto-orient/strip|imageView2/2/w/843/format/webp)

三、 又来了个事务2修改person表的同一个记录，将age修改为30岁

- 在事务2修改该行数据时，数据库也先为该行加锁

- 然后把该行数据拷贝到undo log中，作为旧记录，发现该行记录已经有undo log了，那么最新的旧数据作为链表的表头，插在该行记录的undo log最前面

- 修改该行age为30岁，并且修改隐藏字段的事务ID为当前事务2的ID, 那就是2，回滚指针指向刚刚拷贝到undo log的副本记录

- 事务提交，释放锁

  ![img](https:////upload-images.jianshu.io/upload_images/3133209-70cdae4621d5543e.png?imageMogr2/auto-orient/strip|imageView2/2/w/838/format/webp)

从上面，我们就可以看出，不同事务或者相同事务的对同一记录的修改，会导致该记录的undo log成为一条记录版本线性表，既链表，undo log的链首就是最新的旧记录

#### Read View(读视图)

###### 什么是Read View?

> 什么是Read View，说白了Read View就是事务进行快照读操作的时候生产的读视图(Read View)，在该事务执行的快照读的那一刻，会生成数据库系统当前的一个快照，记录并维护系统当前活跃事务的ID(当每个事务开启时，都会被分配一个ID, 这个ID是递增的，所以最新的事务，ID值越大)

所以我们知道 Read View主要是用来做可见性判断的, 即当我们某个事务执行快照读的时候，对该记录创建一个Read View读视图，把它比作条件用来判断当前事务能够看到哪个版本的数据，既可能是当前最新的数据，也有可能是该行记录的undo log里面的某个版本的数据。

那么这个判断条件是什么呢？

> trx_list（名字我随便取的）
>  一个数值列表，用来维护Read View生成时刻系统正活跃的事务ID
>  up_limit_id
>  记录trx_list列表中事务ID最小的ID
>  low_limit_id
>  ReadView生成时刻系统尚未分配的下一个事务ID，也就是目前已出现过的事务ID的最大值+1

MVCC 维护了一个 ReadView 结构，**主要包含了当前系统未提交的事务列表** TRX_IDs {TRX_ID_1, TRX_ID_2, ...}，还有该列表的最小值 TRX_ID_MIN 和 TRX_ID_MAX。

[![img](picture/数据库知识整理.assets/68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f696d6167652d32303139313230383137313434353637342e706e67)](https://camo.githubusercontent.com/1fe81e9af90e3b9256a607920773eab0c685bda03fbb947a2c1a4d26ff55c21e/68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f696d6167652d32303139313230383137313434353637342e706e67)



在进行 SELECT 操作时，根据数据行快照的 TRX_ID 与 TRX_ID_MIN 和 TRX_ID_MAX 之间的关系，从而判断数据行快照是否可以使用：

- TRX_ID < TRX_ID_MIN，表示该数据行快照时在当前所有未提交事务之前进行更改的，因此可以使用。
- TRX_ID > TRX_ID_MAX，表示该数据行快照是在事务启动之后被更改的，因此不可使用。
- TRX_ID_MIN <= TRX_ID <= TRX_ID_MAX，需要根据隔离级别再进行判断：
  - 提交读：如果 TRX_ID 在 TRX_IDs 列表中，表示该数据行快照对应的事务还未提交，则该快照不可使用。否则表示已经提交，可以使用。
  - 可重复读：都不可以使用。因为如果可以使用的话，那么其它事务也可以读到这个数据行快照并进行修改，那么当前事务再去读这个数据行得到的值就会发生改变，也就是出现了不可重复读问题。

在数据行快照不可使用的情况下，需要沿着 Undo Log 的回滚指针 ROLL_PTR 找到下一个快照，再进行上面的判断。

#### MVCC相关问题

##### RR是如何在RC级的基础上解决不可重复读的？

当前读和快照读在RR级别下的区别：
 表1:

![img](https:////upload-images.jianshu.io/upload_images/3133209-1d04f4bede14f4b5.png?imageMogr2/auto-orient/strip|imageView2/2/w/1200/format/webp)

表2:

![img](https:////upload-images.jianshu.io/upload_images/3133209-ba10316e166babf6.png?imageMogr2/auto-orient/strip|imageView2/2/w/1200/format/webp)

而在表2这里的顺序中，事务B在事务A提交后的快照读和当前读都是实时的新数据400，这是为什么呢？

- 这里与上表的唯一区别仅仅是表1的事务B在事务A修改金额前快照读过一次金额数据，而表2的事务B在事务A修改金额前没有进行过快照读。

所以我们知道事务中快照读的结果是非常依赖该事务首次出现快照读的地方，即某个事务中首次出现快照读的地方非常关键，它有决定该事务后续快照读结果的能力

我们这里测试的是更新，同时删除和更新也是一样的，如果事务B的快照读是在事务A操作之后进行的，事务B的快照读也是能读取到最新的数据的

##### RC,RR级别下的InnoDB快照读有什么不同？

正是Read View生成时机的不同，从而造成RC,RR级别下快照读的结果的不同

- 在RR级别下的某个事务的对某条记录的第一次快照读会创建一个快照及Read View, 将当前系统活跃的其他事务记录起来，此后在调用快照读的时候，还是使用的是同一个Read View，所以只要当前事务在其他事务提交更新之前使用过快照读，那么之后的快照读使用的都是同一个Read View，所以对之后的修改不可见；
- 即RR级别下，快照读生成Read View时，Read View会记录此时所有其他活动事务的快照，这些事务的修改对于当前事务都是不可见的。而早于Read View创建的事务所做的修改均是可见
- 而在RC级别下的，事务中，每次快照读都会新生成一个快照和Read View, 这就是我们在RC级别下的事务中可以看到别的事务提交的更新的原因

总之在RC隔离级别下，是每个快照读都会生成并获取最新的Read View；而在RR隔离级别下，则是同一个事务中的第一个快照读才会创建Read View, 之后的快照读获取的都是同一个Read View。

### Next-Key锁

Next-Key锁是行锁和GAP（间隙锁）的合并

> 行锁可以防止不同事务版本的数据修改提交时造成数据冲突的情况。但如何避免别的事务插入数据就成了问题。
>
> 我们可以看看RR级别和RC级别的对比

RC级别：

<img src="picture/数据库知识整理.assets/image-20210329160450654.png" alt="image-20210329160450654" style="zoom:80%;" />

RR级别：

<img src="picture/数据库知识整理.assets/image-20210329160554801.png" alt="image-20210329160554801" style="zoom:80%;" />

通过对比我们可以发现，在RC级别中，事务A修改了所有teacher_id=30的数据，但是当事务Binsert进新数据后，事务A发现莫名其妙多了一行teacher_id=30的数据，而且没有被之前的update语句所修改，这就是“当前读”的幻读。

RR级别中，事务A在update后加锁，事务B无法插入新数据，这样事务A在update前后读的数据保持一致，避免了幻读。这个锁，就是Gap锁。

#### MySQL实现

在class_teacher这张表中，teacher_id是个索引，那么它就会维护一套B+树的数据关系，为了简化，我们用链表结构来表达（实际上是个树形结构，但原理相同）

![innodb_lock_2](picture/数据库知识整理.assets/b3b6a55f.png)



如图所示，InnoDB使用的是聚集索引，teacher_id身为二级索引，就要维护一个索引字段和主键id的树状结构（这里用链表形式表现），并保持顺序排列。

Innodb将这段数据分成几个个区间

- (negative infinity, 5],
- (5,30],
- (30,positive infinity)；

update class_teacher set class_name=‘初三四班’ where teacher_id=30;不仅用行锁，锁住了相应的数据行；同时也在两边的区间，（5,30]和（30，positive infinity），都加入了gap锁。这样事务B就无法在这个两个区间insert进新数据。

受限于这种实现方式，Innodb很多时候会锁住不需要锁的区间。如下所示：

![image-20210329160748375](picture/数据库知识整理.assets/image-20210329160748375.png)update的teacher_id=20是在(5，30]区间，即使没有修改任何数据，Innodb也会在这个区间加gap锁，而其它区间不会影响，事务C正常插入。

如果使用的是没有索引的字段，比如update class_teacher set teacher_id=7 where class_name=‘初三八班（即使没有匹配到任何数据）’,那么会给全表加入gap锁。同时，它不能像上文中行锁一样经过MySQL Server过滤自动解除不满足条件的锁，因为没有索引，则这些字段也就没有排序，也就没有区间。除非该事务提交，否则其它事务无法插入任何数据。

[**行锁防止别的事务修改或删除，GAP锁防止别的事务新增，行锁和GAP锁结合形成的的Next-Key锁共同解决了RR级别在写数据时的幻读问题。**]()

#### 锁的选择

1）、如果更新条件没有走索引，此时会进行全表扫描，扫表的时候，要阻止其他任何的更新操作，**所以上升为表锁**。

2）、如果更新条件为索引字段，但是并非唯一索引（包括主键索引），那么此时更新会使用Next-Key Lock。使用Next-Key Lock的原因：

- 首先要保证在符合条件的记录上加上排他锁，**会锁定当前非唯一索引和对应的主键索引的值；**（record）
- 还要保证锁定的区间不能插入新的数据。（gap）
- 如果更新条件为唯一索引，则使用Record Lock（记录锁）。

3)、如果是唯一索引，InnoDB根据唯一索引，找到相应记录，将主键索引值和唯一索引值加上记录锁。但不使用Gap Lock（间隙锁）。

### Serializable

这个级别很简单，读加共享锁，写加排他锁，读写互斥。使用的悲观锁的理论，实现简单，数据更加安全，但是并发能力非常差。如果你的业务并发的特别少或者没有并发，同时又要求数据及时可靠的话，可以使用这种模式。

这里要吐槽一句，不要看到select就说不会加锁了，在Serializable这个级别，还是会加锁的！

## 3.mysql锁的类型

### 1.共享/排它锁(Shared and Exclusive Locks)

共享锁和排他锁是InnoDB引擎实现的标准行级别锁。

拿共享锁是为了让当前事务去读一行数据。

拿排他锁是为了让当前事务去修改或删除某一行数据。

设置共享锁：`select * from user where id = 1 LOCK IN SHARE MODE;`

设置排他锁：`select * from user where id = 1 FOR UPDATE;`

### 2.意向锁(Intention Locks)

意向锁存在的意义在于，使得行锁和表锁能够共存。

[**意向锁是表级别的锁**]()，用来说明事务稍后会对表中的数据行加哪种类型的锁(共享锁或独占锁)。

当一个事务对表加了意向排他锁时，另外一个事务在加锁前就会通过该表的意向排他锁知道前面已经有事务在对该表进行独占操作，从而等待。

> - **IX，IS是表级锁，不会和行级的X，S锁发生冲突。只会和表级的X，S发生冲突**
> - 行级别的X和S按照普通的共享、排他规则即可。

#### 意向锁的作用

①在mysql中有表锁，

```
LOCK TABLE my_tabl_name READ;   用读锁锁表，会阻塞其他事务修改表数据。
```

```
LOCK TABLE my_table_name WRITE; 用写锁锁表，会阻塞其他事务读和写。
```

②Innodb引擎又支持行锁，行锁分为

> 共享锁，一个事务对一行的共享只读锁。
>
> 排它锁，一个事务对一行的排他读写锁。

③这两中类型的锁共存的问题

考虑这个例子：

> 事务A锁住了表中的**一行**，让这一行只能读，不能写。
>
> 之后，事务B申请**整个表**的写锁。
>
> 如果事务B申请成功，那么理论上它就能修改表中的任意一行，这与A持有的行锁是冲突的。
>
> 数据库需要避免这种冲突，就是说要让B的申请被阻塞，直到A释放了行锁。

数据库要怎么判断这个冲突呢？

> step1：判断表是否已被其他事务用表锁锁表
> step2：判断表中的每一行是否已被行锁锁住。

注意step2，这样的判断方法效率实在不高，因为需要遍历整个表。于是就有了意向锁。

在意向锁存在的情况下，事务A必须先申请表的意向共享锁，成功后再申请一行的行锁。

在意向锁存在的情况下，上面的判断可以改成

> step1：不变
> step2：发现表上有意向共享锁，说明表中有些行被共享行锁锁住了，因此，事务B申请表的写锁会被阻塞。

#### 意向锁是如何让表锁和行锁共存的？

下面的X 和S 都是表级的排他锁和共享锁

![img](picture/数据库知识整理.assets/v2-92a413ba897f16c2e515e47fc6a56068_1440w.jpg)

有了意向锁之后，前面例子中的事务A在申请行锁（写锁）之前，数据库会自动先给事务A申请表的意向排他锁。当事务B去申请表的写锁时就会失败，[**因为表上有意向排他锁之后事务B申请表的写锁时会被阻塞。**]()

所以，意向锁的作用就是：

当一个事务在需要获取资源的锁定时，则数据库会自动给该事务申请一个该表的意向锁。如果自己需要一个共享锁定，就申请一个意向共享锁。如果需要的是某行（或者某些行）的排他锁定，则申请一个意向排他锁。

#### 意向锁是表锁还是行锁？

首先可以肯定的是，意向锁是表级别锁。意向锁是表锁是有原因的。

当我们需要给一个加表锁的时候，我们需要根据意向锁去判断表中有没有数据行被锁定，以确定是否能加成功。如果意向锁是行锁，那么我们就得遍历表中所有数据行来判断。如果意向锁是表锁，则我们直接判断一次就知道表中是否有数据行被锁定了。

### 3.记录锁(Record Locks)

记录锁是索引记录上的锁，例如：`SELECT c1 FROM t WHERE c1 = 10 FOR UPDATE`;会阻止其他事务对c1=10的数据行进行插入、更新、删除等操作。

> 记录锁总是[**锁定索引记录**]()。如果一个表没有定义索引，那么就会去锁定隐式的“聚集索引”。
>

### 4.间隙锁(Gap Locks)

间隙锁是一个在索引记录之间的间隙上的锁。

[**一个间隙可能跨越单个索引值、多个索引值，甚至为空。**]()

对于使用唯一索引来搜索唯一行的语句，只加记录锁不加间隙锁(这并不包括组合唯一索引）。

### 5.Next-key Locks

Next-Key Locks是行锁与间隙锁的组合。当InnoDB扫描索引记录的时候，会首先对选中的索引记录加上记录锁（Record Lock），然后再对索引记录两边的间隙加上间隙锁（Gap Lock）。

### 6.插入意向锁(Insert Intention Locks)

> 插入意向锁是在数据行插入之前通过插入操作设置的间隙锁定类型。
>

如果多个事务插入到相同的索引间隙中，如果它们不在间隙中的相同位置插入，则无需等待其他事务。例如：在4和7的索引间隙之间两个事务分别插入5和6，则两个事务不会发冲突阻塞。 

### 7.自增锁(Auto-inc Locks)

自增锁是事务插入到有自增列的表中而获得的一种特殊的表级锁。

> 如果一个事务正在向表中插入值，那么任何其他事务都必须等待，保证第一个事务插入的行是连续的自增值。

## 4.行锁的实现方式

[**InnoDB行锁是通过给索引加锁来实现的，如果没有索引，InnoDB会通过隐藏的聚簇索引来对记录进行加锁（全表扫描，也就是表锁）。**]()

但是，[**为了效率考量，MySQL做了优化，对于不满足条件的记录，会放锁，最终持有的，是满足条件的记录上的锁。**]()但是不满足条件的记录上的加锁/放锁动作是不会省略的。所以在没有索引时，不满足条件的数据行会有加锁又放锁的耗时过程。

索引分为主键索引和非主键索引两种。如果一条sql语句操作了主键索引，MySQL就会锁定对应主键索引；如果一条语句操作了非主键索引，MySQL会先锁定非主键索引，再锁定对应的主键索引。


## 5.mysql锁在4种事务隔离级别里的应用

事务的四种隔离级别有：

### 1.读未提交(Read Uncommitted)

> 此时select语句不加任何锁。此时并发最高，但会产生脏读。
>

### 2.读提交(Read Committed, RC)

> 普通select语句是快照读
>
> update语句、delete语句、显示加锁的select语句（select … in share mode 或者 select … for update） 等，除了在外键约束检查和重复键检查时会封锁区间，其他情况都只使用记录锁；
>

### 3.可重复读(Repeated Read, RR)

> 普通select语句也是快照读
>
> update语句、delete语句、显示加锁的select语句（select … in share mode 或者 select … for update）则要分情况：
>
> 在唯一索引上使用唯一的查询条件，则使用记录锁。如: select * from user where id = 1;其中id建立了唯一索引。
>
> 在唯一索引上使用范围查询条件，则使用[**next-key锁**]()。如: select * from user where id >20;
>

###  4.串行化(Serializable)

> 此时所有select语句都会被隐式加锁：select … in share mode.

## 5.优化锁

### （1）合理利用InnoDB的行级锁定

1. 尽可能让所有的数据检索都通过[**索引**]()来完成，从而避免InnoDB因为无法通过索引键加锁而升级为表级锁定； 

2. 合理设计索引，让InnoDB在索引键上面加锁的时候尽可能准确，尽可能的缩小锁定范围，避免造成不必要的锁定而影响其他Query的执行； 

3. 尽可能[**减少基于范围的数据检索过滤条件**]()，避免因为间隙锁带来的负面影响而锁定了不该锁定的记录； 

4. 尽量[**控制事务的大小**]()，减少锁定的资源量和锁定时间长度； 

5. 在业务环境允许的情况下，尽量使用[**较低级别的事务隔离**]()，以减少MySQL因为实现事务隔离级别所带来的附加成本。


### （2）比较常用的减少死锁产生概率的建议

1. 类似业务模块中，尽可能按照相同的访问[**顺序**]()来访问，防止产生死锁； （不要A union B+B union A）

2. 在同一个事务中，尽可能做到一次锁定所需要的所有资源，减少死锁产生概率； 

3. 对于非常容易产生死锁的业务部分，可以尝试使用升级锁定颗粒度，通过[**表级锁定**]()来减少死锁产生的概率。

# 优化

## 1.一个 SQL 查询很慢,从哪些地方进行优化?

### 如何判断一条sql的性能有问题？

> 1.   通过用户反馈获取存在性能问题的SQL
> 2.   通过慢查询日志获取存在性能问题的SQL
> 3.   实时获取存在性能问题的SQL（explain）

> MySQL服务器处理查询请求的整个过程
>
> ​        . 客户端发送SQL请求给服务器
>
> ​        . 服务器检查是否可以在查询缓存中命中该SQL
>
> ​        . 服务器端进行SQL解析，预处理，再由优化器生成对应的执行计划
>
> ​        . 根据执行计划，调用存储引擎API来查询数据
>
> ​        . 将结果返回给客户端

### 优化

#### 索引类

```sql
1.对查询进行优化，应尽量避免全表扫描，首先应考虑在 where 及 order by 涉及的列上建立索引。    
    
2.应尽量避免在 where 子句中对字段进行 null 值判断，否则将导致引擎放弃使用索引而进行全表扫描，如：    
select id from t where num is null    
可以在num上设置默认值0，确保表中num列没有null值，然后这样查询：    
select id from t where num=0    
    
3.应尽量避免在 where子句中使用!=或<>操作符，否则将引擎放弃使用索引而进行全表扫描。    
    
4.应尽量避免在 where 子句中使用 or 来连接条件，否则将导致引擎放弃使用索引而进行全表扫描，如：    
select id from t where num=10 or num=20    
可以这样查询：    
select id from t where num=10    
union all    
select id from t where num=20    
    
5.in 和 not in 也要慎用，否则会导致全表扫描，如：    
select id from t where num in(1,2,3)    
对于连续的数值，能用 between 就不要用 in 了：    
select id from t where num between 1 and 3    
    
6.下面的查询也将导致全表扫描：    
select id from t where name like '%abc%'    
    
7.应尽量避免在 where 子句中对字段进行表达式操作，这将导致引擎放弃使用索引而进行全表扫描。如：    
select id from t where num/2=100    
应改为:    
select id from t where num=100*2    
    
8.应尽量避免在where子句中对字段进行函数操作，这将导致引擎放弃使用索引而进行全表扫描。如：    
select id from t where substring(name,1,3)='abc'--name以abc开头的id    
应改为:    
select id from t where name like 'abc%'    
    
9.不要在 where 子句中的“=”左边进行函数、算术运算或其他表达式运算，否则系统将可能无法正确使用索引。    
    
10.在使用索引字段作为条件时，如果该索引是复合索引，那么必须使用到该索引中的第一个字段作为条件时才能保证系统使用该索引，否则该索引将不会被使用，并且应尽可能的让字段顺序与索引顺序相一致。    
    
12.很多时候用 exists 代替 in 是一个好的选择：    
select num from a where num in(select num from b)    
用下面的语句替换：    
select num from a where exists(select 1 from b where num=a.num)    
    
13.并不是所有索引对查询都有效，SQL是根据表中数据来进行查询优化的，当索引列有大量数据重复时，SQL查询可能不会去利用索引，如一表中有字段sex，male、female几乎各一半，那么即使在sex上建了索引也对查询效率起不了作用。    
    
14.索引并不是越多越好，索引固然可以提高相应的 select 的效率，但同时也降低了 insert 及 update 的效率，   
因为 insert 或 update 时有可能会重建索引，所以怎样建索引需要慎重考虑，视具体情况而定。    
一个表的索引数最好不要超过6个，若太多则应考虑一些不常使用到的列上建的索引是否有必要。 
```

#### 其他类

```sql
   
1.尽量使用数字型字段，若只含数值信息的字段尽量不要设计为字符型，这会降低查询和连接的性能，并会增加存储开销。 
这是因为引擎在处理查询和连接时会逐个比较字符串中每一个字符，而对于数字型而言只需要比较一次就够了。    
    
2.尽可能的使用 varchar 代替 char ，因为首先变长字段存储空间小，可以节省存储空间，    
其次对于查询来说，在一个相对较小的字段内搜索效率显然要高些。    
    
3.任何地方都不要使用 select * from t ，用具体的字段列表代替“*”，不要返回用不到的任何字段。 
(1)程序变更问题，出现不可以预知隐患；
假设某一天修改了表结构，如果用select *，返回的数据必然会会变化，客户端是否对数据库变化作适配，是否所有地方都做了适配，这都是问题。
(2)性能问题
a. 使用了select ，必然导致数据库需要先解析代表哪写字段，从数据字段中将*转化为具体的字段含义，存在性能开销;
b. 不可能对所有字段建索引，在索引优化必然会有局限性，导致查询时性能差；
c. 可能会存在不需要的列，传输过程中有不必要的性能损耗；  

4.尽量避免大事务操作，提高系统并发能力。

5.尽量避免向客户端返回大数据量，若数据量过大，应该考虑相应需求是否合理。
```

## 2.慢查询日志

> [慢查询日志是将mysql服务器中影响数据库性能的相关SQL语句记录到日志文件，通过对这些特殊的SQL语句分析，改进以达到提高数据库性能的目的。]()

> 通过使用--slow_query_log[={0|1}]选项来启用慢查询日志。所有执行时间超过long_query_time秒的SQL语句都会被记录到慢查询日志。

> 缺省情况下hostname-slow.log为慢查询日志文件安名，存放到数据目录，同时缺省情况下未开启慢查询日志。
>
> 缺省情况下数据库相关管理型SQL(比如OPTIMIZE TABLE、ANALYZE TABLE和ALTER TABLE)不会被记录到日志。
>
> 对于管理型SQL可以通过--log-slow-admin-statements开启记录管理型慢SQL。
>
> mysqld在语句执行完并且所有锁释放后记入慢查询日志。记录顺序可以与执行顺序不相同。      
>
> 获得初始表锁定的时间不算作执行时间。
>
> 用查询缓存处理的查询不加到慢查询日志中。
>
> 表有零行或一行而不能从索引中受益的查询也不写入慢查询日志(默认)。

```sql
long_query_time     ：  设定慢查询的阀值，超出次设定值的SQL即被记录到慢查询日志，缺省值为10s
slow_query_log      ：  指定是否开启慢查询日志
slow_query_log_file ：  指定慢日志文件存放位置，可以为空，系统会给一个缺省的文件host_name-slow.log
min_examined_row_limit：查询检查返回少于该参数指定行的SQL不被记录到慢查询日志
log_queries_not_using_indexes: 不使用索引的慢查询日志是否记录到索引
    
--当前版本
root@localhost[(none)]> show variables like 'version';
+---------------+------------+
| Variable_name | Value      |
+---------------+------------+
| version       | 5.5.39-log |
+---------------+------------+
 
root@localhost[(none)]> show variables like '%slow%';
+---------------------+---------------------------------+
| Variable_name       | Value                           |
+---------------------+---------------------------------+
| log_slow_queries    | OFF                             |
| slow_launch_time    | 2                               |
| slow_query_log      | OFF                             |
| slow_query_log_file | /var/lib/mysql/suse11b-slow.log |
+---------------------+---------------------------------+
 
root@localhost[tempdb]> set global slow_query_log=1;
Query OK, 0 rows affected, 1 warning (0.00 sec)
 
--从下面的查询中可知，2个系统变量log_slow_queries，slow_query_log同时被置为on
root@localhost[(none)]> show variables like '%slow%';
+---------------------+---------------------------------+
| Variable_name       | Value                           |
+---------------------+---------------------------------+
| log_slow_queries    | ON                              |
| slow_launch_time    | 2                               |
| slow_query_log      | ON                              |
| slow_query_log_file | /var/lib/mysql/suse11b-slow.log |
+---------------------+---------------------------------+
 
root@localhost[tempdb]> show variables like '%long_query_time%';
+-----------------+-----------+
| Variable_name   | Value     |
+-----------------+-----------+
| long_query_time | 10.000000 |
+-----------------+-----------+
 
--为便于演示，我们将全局和session级别long_query_time设置为1
root@localhost[tempdb]> set global long_query_time=1;
Query OK, 0 rows affected (0.00 sec)
 
root@localhost[tempdb]> set session long_query_time=1;
Query OK, 0 rows affected (0.00 sec)

 
root@localhost[tempdb]> create table tb_slow as select * from information_schema.columns;
Query OK, 829 rows affected (0.10 sec)
Records: 829  Duplicates: 0  Warnings: 0
 
root@localhost[tempdb]> insert into tb_slow select * from tb_slow;
Query OK, 829 rows affected (0.05 sec)
Records: 829  Duplicates: 0  Warnings: 0
       .....为便于演示，我们插入一些数据，中间重复过程省略
root@localhost[tempdb]> insert into tb_slow select * from tb_slow;
Query OK, 26528 rows affected (4.40 sec)
Records: 26528  Duplicates: 0  Warnings: 0
 
root@localhost[tempdb]> system tail  /var/lib/mysql/suse11b-slow.log
/usr/sbin/mysqld, Version: 5.5.39-log (MySQL Community Server (GPL)). started with:
Tcp port: 3306  Unix socket: /var/lib/mysql/mysql.sock
Time                 Id Command    Argument
# Time: 141004 22:05:48
# User@Host: root[root] @ localhost []
# Query_time: 4.396858  Lock_time: 0.000140 Rows_sent: 0  Rows_examined: 53056
use tempdb;
SET timestamp=1412431548;
insert into tb_slow select * from tb_slow;
 
    ....再次插入一些记录....
root@localhost[tempdb]> insert into tb_slow select * from tb_slow;
Query OK, 212224 rows affected (37.51 sec)
Records: 212224  Duplicates: 0  Warnings: 0
 
root@localhost[tempdb]> select table_schema,table_name,count(*) from tb_slow
    -> group by table_schema,table_name order by 3,2;
+--------------------+----------------------------------------------+----------+
| table_schema       | table_name                                   | count(*) |
+--------------------+----------------------------------------------+----------+
| information_schema | COLLATION_CHARACTER_SET_APPLICABILITY        |     1024 |
| performance_schema | cond_instances                               |     1024 |
                  ...........
| mysql              | user                                         |    21504 |
+--------------------+----------------------------------------------+----------+
83 rows in set (1.58 sec)                  
 
root@localhost[tempdb]> system tail  /var/lib/mysql/suse11b-slow.log
# User@Host: root[root] @ localhost []
# Query_time: 37.514172  Lock_time: 0.000123 Rows_sent: 0  Rows_examined: 424448
SET timestamp=1412431806;
insert into tb_slow select * from tb_slow;
# Time: 141004 22:10:47
# User@Host: root[root] @ localhost []
# Query_time: 1.573293  Lock_time: 0.000183 Rows_sent: 83  Rows_examined: 424614
SET timestamp=1412431847;
select table_schema,table_name,count(*) from tb_slow  --这条SQL被记录下来了，其查询时间为1.573293s
group by table_schema,table_name order by 3,2;
 
root@localhost[tempdb]> show variables like '%log_queries_not_using_indexes';
+-------------------------------+-------+
| Variable_name                 | Value |
+-------------------------------+-------+
| log_queries_not_using_indexes | OFF   |
+-------------------------------+-------+
 
root@localhost[tempdb]> set global log_queries_not_using_indexes=1;
Query OK, 0 rows affected (0.00 sec)
 
--查看表tb_slow索引信息，表tb_slow无任何索引
root@localhost[tempdb]> show index from tb_slow;
Empty set (0.00 sec)
 
root@localhost[tempdb]> select count(*) from tb_slow;
+----------+
| count(*) |
+----------+
|   424448 |
+----------+
1 row in set (0.20 sec)
 
root@localhost[tempdb]> system tail -n3 /var/lib/mysql/suse11b-slow.log
# Query_time: 0.199840  Lock_time: 0.000152 Rows_sent: 1  Rows_examined: 424448
SET timestamp=1412432188;
select count(*) from tb_slow;   --此次查询时间为0.199840，被记录的原因是因为没有走索引，因为表本身没有索引
```

**格式化**

```sql
结构化慢查询日志就是把慢查询日志中的重要信息按照便于阅读以及按照特定的排序方式来提取SQL。
这种方式有点类似于Oracle中有个tkprof来格式化oracle的trace文件。
对于前面的慢查询日志我们使用mysqldumpslow来提取如下：
 
suse11b:~ # mysqldumpslow -s at,al /var/lib/mysql/suse11b-slow.log
Reading mysql slow query log from /var/lib/mysql/suse11b-slow.log
Count: 4  Time=16.87s (67s)  Lock=0.00s (0s)  Rows=0.0 (0), root[root]@localhost
  insert into tb_slow select * from tb_slow
 
Count: 1  Time=0.20s (0s)  Lock=0.00s (0s)  Rows=1.0 (1), root[root]@localhost
  select count(*) from tb_slow
 
Count: 1  Time=1.57s (1s)  Lock=0.00s (0s)  Rows=83.0 (83), root[root]@localhost
  select table_schema,table_name,count(*) from tb_slow
  group by table_schema,table_name order by N,N
 
#以下是按照最大耗用时间排最后，只显示2条的方式格式化日志文件
suse11b:~ # mysqldumpslow -r -t 2 /var/lib/mysql/suse11b-slow.log
Reading mysql slow query log from /var/lib/mysql/suse11b-slow.log
Count: 1  Time=1.57s (1s)  Lock=0.00s (0s)  Rows=83.0 (83), root[root]@localhost
  select table_schema,table_name,count(*) from tb_slow
  group by table_schema,table_name order by N,N
 
Count: 4  Time=16.87s (67s)  Lock=0.00s (0s)  Rows=0.0 (0), root[root]@localhost
  insert into tb_slow select * from tb_slow
  
#获取mysqldumpslow的帮助信息
suse11b:~ # mysqldumpslow --help
Usage: mysqldumpslow [ OPTS... ] [ LOGS... ]
 
Parse and summarize the MySQL slow query log. Options are
 
  --verbose    verbose
  --debug      debug
  --help       write this text to standard output
 
  -v           verbose
  -d           debug
  -s ORDER     what to sort by (al, at, ar, c, l, r, t), 'at' is default
                al: average lock time
                ar: average rows sent
                at: average query time
                 c: count        #query的次数
                 l: lock time
                 r: rows sent    #返回的记录数
                 t: query time  
  -r           reverse the sort order (largest last instead of first)
  -t NUM       just show the top n queries
  -a           don't abstract all numbers to N and strings to 'S'
  -n NUM       abstract numbers with at least n digits within names
  -g PATTERN   grep: only consider stmts that include this string
  -h HOSTNAME  hostname of db server for *-slow.log filename (can be wildcard),
               default is '*', i.e. match all
  -i NAME      name of server instance (if using mysql.server startup script)
  -l           don't subtract lock time from total time
```

## 3.Mysql的慢查询优化方式

### 分析慢查询日志         

​       直接分析mysql慢查询日志 ,利用explain关键字可以模拟优化器执行SQL查询语句，来分析sql慢查询语句

```sql
例如：执行EXPLAIN SELECT * FROM res_user ORDER BYmodifiedtime LIMIT 0,1000

得到如下结果： 显示结果分析：  

table |  type | possible_keys | key |key_len  | ref | rows | Extra  EXPLAIN列的解释：         table                 显示这一行的数据是关于哪张表的           
type                  这是重要的列，显示连接使用了何种类型。从最好到最差的连接类型为const、eq_reg、ref、range、indexhe和ALL 
rows                	显示需要扫描行数
key                   使用的索引
```

### 常见的慢查询优化

####  （1）索引没起作用的情况

    1. 使用LIKE关键字的查询语句
    在使用LIKE关键字进行查询的查询语句中，如果匹配字符串的第一个字符为“%”，索引不会起作用。
    
    2. 使用多列索引的查询语句
    MySQL可以为多个字段创建索引。一个索引最多可以包括16个字段。对于多列索引，只有查询条件使用了这些字段中的第一个字段时，索引才会被使用。

####  （2）优化数据库结构

合理的数据库结构不仅可以使数据库占用更小的磁盘空间，而且能够使查询速度更快。数据库结构的设计，需要考虑`数据冗余、查询和更新的速度、字段的数据类型是否合理`等多方面的内容。

1. 将字段很多的表分解成多个表 

        对于字段比较多的表，如果有些字段的使用频率很低，可以将这些字段分离出来形成新表。因为当一个表的数据量很大时，会由于使用频率低的字段的存在而变慢。

2. 增加中间表

        对于需要经常联合查询的表，可以建立中间表以提高查询效率。通过建立中间表，把需要经常联合查询的数据插入到中间表中，然后将原来的联合查询改为对中间表的查询，以此来提高查询效率。

#### （3）分解关联查询

将一个大的查询分解为多个小查询是很有必要的。

很多高性能的应用都会对关联查询进行分解，就是可以对每一个表进行一次单表查询，然后将查询结果在应用程序中进行关联，很多场景下这样会更高效，例如：       

```sql
 SELECT * FROM tag 
        JOIN tag_post ON tag_id = tag.id
        JOIN post ON tag_post.post_id = post.id
        WHERE tag.tag = 'mysql';
        分解为：
        SELECT * FROM tag WHERE tag = 'mysql';
        SELECT * FROM tag_post WHERE tag_id = 1234;
        SELECT * FROM post WHERE post.id in (123,456,567);
```

#### （4）优化LIMIT分页

​      在系统中需要分页的操作通常会使用limit加上偏移量的方法实现，同时加上合适的order by 子句。如果有对应的索引，通常效率会不错，否则MySQL需要做大量的文件排序操作。

一个非常令人头疼问题就是当偏移量非常大的时候，例如可能是limit 10000,20这样的查询，这是mysql需要查询10020条然后只返回最后20条，前面的10000条记录都将被舍弃，这样的代价很高。

优化此类查询的**一个最简单的方法是尽可能的使用索引覆盖扫描**，而不是查询所有的列。然后根据需要做一次关联操作再返回所需的列。对于偏移量很大的时候这样做的效率会得到很大提升。

  对于下面的查询：

```sql
select id,title from collect limit 90000,10;
```

​		该语句存在的最大问题在于limit M,N中偏移量M太大（我们暂不考虑筛选字段上要不要添加索引的影响），导致每次查询都要先从整个表中找到满足条件的前M条记录，之后舍弃这M条记录并从第M+1条记录开始再依次找到N条满足条件的记录。如果表非常大，且筛选字段没有合适的索引，且M特别大那么这样的代价是非常高的。 

​		试想，如我们下一次的查询能从前一次查询结束后标记的位置开始查找，找到满足条件的100条记录，并记下下一次查询应该开始的位置，以便于下一次查询能直接从该位置 开始，这样就不必每次查询都先从整个表中先找到满足条件的前M条记录，舍弃，在从M+1开始再找到100条满足条件的记录了。

##### 方法一：虑筛选字段（title）上加索引

```
  title字段加索引  （此效率如何未加验证）
```

##### 方法二：先查询出主键id值

```sql
select id,title from collect where id>=(select id from collect order by id limit 90000,1) limit 10;

原理：先查询出90000条数据对应的主键id的值，然后直接通过该id的值直接查询该id后面的数据。
```

##### 方法三：“关延迟联”

如果这个表非常大，那么这个查询可以改写成如下的方式：

```sql
  Select news.id, news.description from news inner join (select id from news order by title limit 50000,5) as myNew using(id);

    这里的“关延迟联”将大大提升查询的效率，它让MySQL扫描尽可能少的页面，获取需要的记录后再根据关联列回原表查询需要的所有列。这个技术也可以用在优化关联查询中的limit。
```

##### 方法四：建立复合索引 acct_id和create_time

```sql
select * from acct_trans_log WHERE  acct_id = 3095  order by create_time desc limit 0,10

注意sql查询慢的原因都是:引起filesort
```

#### （5）分析具体的SQL语句

 1.两个表选哪个为驱动表，表面是可以以数据量的大小作为依据，但是实际经验最好交给mysql查询优化器自己去判断。

```sql
例如：select * from a where id in (select id from b );  
    对于这条sql语句它的执行计划其实并不是先查询出b表的所有id,然后再与a表的id进行比较。
mysql会把in子查询转换成exists相关子查询，所以它实际等同于这条sql语句：select * from a where exists(select * from b where b.id=a.id );
    而exists相关子查询的执行原理是: 循环取出a表的每一条记录与b表进行比较，比较的条件是a.id=b.id . 看a表的每条记录的id是否在b表存在，如果存在就行返回a表的这条记录。
```

2.exists查询有什么弊端？

```
  由exists执行原理可知，a表(外表)使用不了索引，必须全表扫描，因为是拿a表的数据到b表查。而且必须得使用a表的数据到b表中查（外表到里表中），顺序是固定死的。
```

3.如何优化？

```
  建索引。但是由上面分析可知，要建索引只能在b表的id字段建，不能在a表的id上，mysql利用不上。
```

4.这样优化够了吗？还差一些。

```
   由于exists查询它的执行计划只能拿着a表的数据到b表查（外表到里表中），虽然可以在b表的id字段建索引来提高查询效率。但是并不能反过来拿着b表的数据到a表查，exists子查询的查询顺序是固定死的。
```

5.为什么要反过来？

```
因为首先可以肯定的是反过来的结果也是一样的。这样就又引出了一个更细致的疑问：在双方两个表的id字段上都建有索引时，到底是a表查b表的效率高，还是b表查a表的效率高？
```

6.该如何进一步优化？

```
把查询修改成inner join连接查询：select * from a inner join b on a.id=b.id; （但是仅此还不够，接着往下看）
```

7.为什么不用left join 和 right join？

```
这时候表之间的连接的顺序就被固定住了，比如左连接就是必须先查左表全表扫描，然后一条一条的到另外表去查询，右连接同理。仍然不是最好的选择。
```

8.为什么使用inner join就可以？

```
inner join中的两张表，如： a inner join b，但实际执行的顺序是跟写法的顺序没有半毛钱关系的，最终执行也可能会是b连接a，顺序不是固定死的。如果on条件字段有索引的情况下，同样可以使用上索引。
```

9.那我们又怎么能知道a和b什么样的执行顺序效率更高？

    		不知道。谁知道？mysql自己知道。让mysql自己去判断（查询优化器）。具体表的连接顺序和使用索引情况，mysql查询优化器会对每种情况做出成本评估，最终选择最优的那个做为执行计划。
        在inner join的连接中,mysql会自己评估使用a表查b表的效率高还是b表查a表高，如果两个表都建有索引的情况下，mysql同样会评估使用a表条件字段上的索引效率高还是b表的。

10.利用explain字段查看执行时运用到的key（索引）

```
而我们要做的就是：把两个表的连接条件的两个字段都各自建立上索引，然后explain 一下，查看执行计划，看mysql到底利用了哪个索引，最后再把没有使用索引的表的字段索引给去掉就行了。
```

## 4.explain 里面有哪些字段?

### explain的用途

```
1. 表的读取顺序如何(id字段)
2. 数据读取操作有哪些操作类型
3. 哪些索引可以使用
4. 哪些索引被实际使用
5. 表之间是如何引用
6. 每张表有多少行被优化器查询
......
```

### explain的执行效果

```mysql
mysql> explain select * from subject where id = 1 \G
******************************************************
           id: 1
  select_type: SIMPLE
        table: subject
   partitions: NULL
         type: const
possible_keys: PRIMARY
          key: PRIMARY
      key_len: 4
          ref: const
         rows: 1
     filtered: 100.00
        Extra: NULL
******************************************************
```

### explain包含的字段

```
1. id //select查询的序列号，包含一组数字，表示查询中执行select子句或操作表的顺序
2. select_type //查询类型(是简单查询还是包含了子查询)
3. table //正在访问哪个表
4. partitions //匹配的分区
5. type //访问的类型  （最重要）
6. possible_keys //显示可能应用在这张表中的索引，一个或多个，但不一定实际使用到
7. key //实际使用到的索引，如果为NULL，则没有使用索引
8. key_len //表示索引中使用的字节数，可通过该列计算查询中使用的索引的长度
9. ref //显示索引的哪一列被使用了，如果可能的话，是一个常数，哪些列或常量被用于查找索引列上的值
10. rows //根据表统计信息及索引选用情况，大致估算出找到所需的记录所需读取的行数
11. filtered //查询的表行占表的百分比
12. Extra //包含不适合在其它列中显示但十分重要的额外信息（Using index表示使用了覆盖索引）
```

![preview](picture/数据库知识整理.assets/view)



### id字段

##### 1. id相同

```sql
执行顺序从上至下
例子：
explain select subject.* from subject,student_score,teacher where subject.id = student_id and subject.teacher_id = teacher.id;
读取顺序：subject > teacher > student_score
```

![image-20210310132434489](picture/数据库知识整理.assets/image-20210310132434489.png)

##### 2. id不同

```sql
如果是子查询，id的序号会递增，id的值越大优先级越高，越先被执行
例子：
explain select score.* from student_score as score where subject_id = (select id from subject where teacher_id = (select id from teacher where id = 2));
读取顺序：teacher > subject > student_score
```

![image-20210310132606190](picture/数据库知识整理.assets/image-20210310132606190.png)

##### 3. id相同又不同

```sql
id如果相同，可以认为是一组，从上往下顺序执行
在所有组中，id值越大，优先级越高，越先执行
例子：
explain select subject.* from subject left join teacher on subject.teacher_id = teacher.id
 -> union 
 -> select subject.* from subject right join teacher on subject.teacher_id = teacher.id;
 读取顺序：2.teacher > 2.subject > 1.subject > 1.teacher
```

![image-20210310132700987](picture/数据库知识整理.assets/image-20210310132700987.png)

### select_type字段

##### 1. SIMPLE

```sql
简单查询，不包含子查询或Union查询
例子：
explain select subject.* from subject,student_score,teacher where subject.id = student_id and subject.teacher_id = teacher.id;
```

![image-20210310132844460](picture/数据库知识整理.assets/image-20210310132844460.png)

##### 2. PRIMARY

```sql
查询中若包含任何复杂的子部分，最外层查询则被标记为主查询
例子：
explain select score.* from student_score as score where subject_id = (select id from subject where teacher_id = (select id from teacher where id = 2));
```

![image-20210310132748752](picture/数据库知识整理.assets/image-20210310132748752.png)

##### 3. SUBQUERY

```sql
在select或where中包含子查询
例子：
explain select score.* from student_score as score where subject_id = (select id from subject where teacher_id = (select id from teacher where id = 2));
```

![image-20210310132939149](picture/数据库知识整理.assets/image-20210310132939149.png)

##### 4. DERIVED

```
在FROM列表中包含的子查询被标记为DERIVED（衍生），MySQL会递归执行这些子查询，把结果放在临时表中
备注：
MySQL5.7+ 进行优化了，增加了derived_merge（派生合并），默认开启，可加快查询效率
```

##### 5. UNION

```sql
若第二个select出现在union之后，则被标记为UNION
例子：
explain select subject.* from subject left join teacher on subject.teacher_id = teacher.id
 -> union 
 -> select subject.* from subject right join teacher on subject.teacher_id = teacher.id;
```

![image-20210310133103860](picture/数据库知识整理.assets/image-20210310133103860.png)

##### 6. UNION RESULT

```sql
从UNION表获取结果的select
例子：
explain select subject.* from subject left join teacher on subject.teacher_id = teacher.id
 -> union 
 -> select subject.* from subject right join teacher on subject.teacher_id = teacher.id;
```

![image-20210310133118220](picture/数据库知识整理.assets/image-20210310133118220.png)

### type字段

```
NULL>system>const>eq_ref>ref>fulltext>ref_or_null>index_merge>unique_subquery>index_subquery>range>index>ALL //最好到最差
备注：掌握以下10种常见的即可
NULL>system>const>eq_ref>ref>ref_or_null>index_merge>range>index>ALL
```

##### 1. NULL

```sql
MySQL能够在优化阶段分解查询语句，在执行阶段用不着再访问表或索引
例子：
explain select min(id) from subject;
```

![image-20210310133317722](picture/数据库知识整理.assets/image-20210310133317722.png)

##### 2. system

```
表只有一行记录（等于系统表），这是const类型的特列，平时不大会出现，可以忽略
```

##### 3. const（主键查询）

```sql
表示通过索引一次就找到了，const用于比较primary key或unique索引，因为只匹配一行数据，所以很快，如主键置于where列表中，MySQL就能将该查询转换为一个常量
例子：
explain select * from teacher where teacher_no = 'T2010001';
```

![image-20210310133506804](picture/数据库知识整理.assets/image-20210310133506804.png)

##### 4. eq_ref（与const的区别可能在于联表）

```sql
唯一性索引扫描，对于每个索引键，表中只有一条记录与之匹配，常见于主键或唯一索引扫描
例子：
explain select subject.* from subject left join teacher on subject.teacher_id = teacher.id;
```

![image-20210310133614612](picture/数据库知识整理.assets/image-20210310133614612.png)

##### 5. ref（非唯一索引扫描）

```sql
非唯一性索引扫描，返回匹配某个单独值的所有行
本质上也是一种索引访问，返回所有匹配某个单独值的行
然而可能会找到多个符合条件的行，应该属于查找和扫描的混合体
例子：
explain select subject.* from subject,student_score,teacher where subject.id = student_id and subject.teacher_id = teacher.id;
```

![image-20210310133631051](picture/数据库知识整理.assets/image-20210310133631051.png)

##### 6. ref_or_null（非唯一索引扫描-可null）

```sql
类似ref，但是可以搜索值为NULL的行
例子：
explain select * from teacher where name = 'wangsi' or name is null;
```

![image-20210310133927769](picture/数据库知识整理.assets/image-20210310133927769.png)

##### 7. index_merge

```sql
表示使用了索引合并的优化方法
例子：
explain select * from teacher where id = 1 or teacher_no = 'T2010001' .
```

![一张图搞定 explain](picture/数据库知识整理.assets/1460000021458129)

##### 8. range

```sql
只检索给定范围的行，使用一个索引来选择行，key列显示使用了哪个索引
一般就是在你的where语句中出现between、<>、in等的查询。
例子：
explain select * from subject where id between 1 and 3;
```

![image-20210310134040000](picture/数据库知识整理.assets/image-20210310134040000.png)

##### 9. index（全表查询-索引中）

```sql
Full index Scan，Index与All区别：index只遍历索引树，通常比All快
因为索引文件通常比数据文件小，也就是虽然all和index都是读全表，但index是从索引中读取的，而all是从硬盘读的。
例子：
explain select id from subject;
```

![image-20210310134053442](picture/数据库知识整理.assets/image-20210310134053442.png)

##### 10. ALL（全表查询-磁盘）

```
Full Table Scan，将遍历全表以找到匹配行
例子：
explain select * from subject;
```

![image-20210310134442282](picture/数据库知识整理.assets/image-20210310134442282.png)

### table字段

```
数据来自哪张表
```

### possible_keys字段

```
显示可能应用在这张表中的索引，一个或多个
查询涉及到的字段若存在索引，则该索引将被列出，但不一定被实际使用
```

### key字段

```
 实际使用到的索引，如果为NULL，则没有使用索引
查询中若使用了覆盖索引（查询的列刚好是索引），则该索引仅出现在key列表
```

### key_len字段

```
 表示索引中使用的字节数，可通过该列计算查询中使用的索引的长度
在不损失精确度的情况下，长度越短越好
key_len显示的值为索引字段最大的可能长度，并非实际使用长度
即key_len是根据定义计算而得，不是通过表内检索出的
```

### ref字段

```
显示索引的哪一列被使用了，如果可能的话，是一个常数，哪些列或常量被用于查找索引列上的值
```

### rows字段

```
根据表统计信息及索引选用情况，大致估算出找到所需的记录所需读取的行数
```

### partitions字段

```
匹配的分区
```

### filtered字段

```
查询的表行占表的百分比
```

### Extra字段

```
包含不适合在其它列中显示但十分重要的额外信息
```

##### 1. Using filesort

```sql
说明MySQL会对数据使用一个外部的索引排序，而不是按照表内的索引顺序进行读取
MySQL中无法利用索引完成的排序操作称为“文件排序”
例子：
explain select * from subject order by name;
```

![image-20210310135105289](picture/数据库知识整理.assets/image-20210310135105289.png)

##### 2. Using temporary

```sql
使用了临时表保存中间结果，MySQL在对结果排序时使用临时表，常见于排序order by 和分组查询group by
例子：
explain select subject.* from subject left join teacher on subject.teacher_id = teacher.id
 -> union 
 -> select subject.* from subject right join teacher on subject.teacher_id = teacher.id;
```

![image-20210310135157070](picture/数据库知识整理.assets/image-20210310135157070.png)

##### 3. Using index

```sql
表示相应的select操作中使用了覆盖索引（Covering Index）,避免访问了表的数据行，效率不错！
如果同时出现using where，表明索引被用来执行索引键值的查找
如果没有同时出现using where，表明索引用来读取数据而非执行查找动作
例子：
explain select subject.* from subject,student_score,teacher where subject.id = student_id and subject.teacher_id = teacher.id;
备注：
覆盖索引：select的数据列只用从索引中就能够取得，不必读取数据行，MySQL可以利用索引返回select列表中的字段，而不必根据索引再次读取数据文件，即查询列要被所建的索引覆盖
```

![image-20210310135354967](picture/数据库知识整理.assets/image-20210310135354967.png)

##### 4. Using where

```sql
使用了where条件
例子：
explain select subject.* from subject,student_score,teacher where subject.id = student_id and subject.teacher_id = teacher.id;
```

![image-20210310135446485](picture/数据库知识整理.assets/image-20210310135446485.png)

##### 5. Using join buffer

```sql
使用了连接缓存
例子：
explain select student.*,teacher.*,subject.* from student,teacher,subject;
```

![image-20210310135533048](picture/数据库知识整理.assets/image-20210310135533048.png)

##### 6. impossible where

```sql
where子句的值总是false，不能用来获取任何元组
例子：
explain select * from teacher where name = 'wangsi' and name = 'lisi';
```

![image-20210310135607919](picture/数据库知识整理.assets/image-20210310135607919.png)

##### 7. distinct

```sql
一旦mysql找到了与行相联合匹配的行，就不再搜索了
例子：
explain select distinct teacher.name from teacher left join subject on teacher.id = subject.teacher_id;
```

![image-20210310135622870](picture/数据库知识整理.assets/image-20210310135622870.png)

##### 8. Select tables optimized away

```sql
SELECT操作已经优化到不能再优化了（MySQL根本没有遍历表或索引就返回数据了）
例子：
explain select min(id) from subject;
```

![image-20210310135638517](picture/数据库知识整理.assets/image-20210310135638517.png)

### 使用的数据表

```sql
create table subject(
 -> id int(10) auto_increment,
 -> name varchar(20),
 -> teacher_id int(10),
 -> primary key (id),
 -> index idx_teacher_id (teacher_id));//学科表
 
create table teacher(
 -> id int(10) auto_increment,
 -> name varchar(20),
 -> teacher_no varchar(20),
 -> primary key (id),
 -> unique index unx_teacher_no (teacher_no(20)));//教师表
 
 create table student(
 -> id int(10) auto_increment,
 -> name varchar(20),
 -> student_no varchar(20),
 -> primary key (id),
 -> unique index unx_student_no (student_no(20)));//学生表
 
 create table student_score(
 -> id int(10) auto_increment,
 -> student_id int(10),
 -> subject_id int(10),
 -> score int(10),
 -> primary key (id),
 -> index idx_student_id (student_id),
 -> index idx_subject_id (subject_id));//学生成绩表
 
 alter table teacher add index idx_name(name(20));//教师表增加名字普通索引
 
 数据填充：
 insert into student(name,student_no) values ('zhangsan','20200001'),('lisi','20200002'),('yan','20200003'),('dede','20200004');
 
 insert into teacher(name,teacher_no) values('wangsi','T2010001'),('sunsi','T2010002'),('jiangsi','T2010003'),('zhousi','T2010004');
 
 insert into subject(name,teacher_id) values('math',1),('Chinese',2),('English',3),('history',4);
 
insert into student_score(student_id,subject_id,score) values(1,1,90),(1,2,60),(1,3,80),(1,4,100),(2,4,60),(2,3,50),(2,2,80),(2,1,90),(3,1,90),(3,4,100),(4,1,40),(4,2,80),(4,3,80),(4,5,100);
```

## 5.慢查询优化示例

### MySQL索引原理

#### 索引原理

数据库实现比较复杂，数据保存在磁盘上，而为了提高性能，每次又可以把部分数据读入内存来计算，因为我们知道访问磁盘的成本大概是访问内存的十万倍左右，所以简单的搜索树难以满足复杂的应用场景。

##### 磁盘IO与预读

前面提到了访问磁盘，那么这里先简单介绍一下磁盘IO和预读，磁盘读取数据靠的是机械运动，每次读取数据花费的时间可以分为寻道时间、旋转延迟、传输时间三个部分，寻道时间指的是磁臂移动到指定磁道所需要的时间，主流磁盘一般在5ms以下；旋转延迟就是我们经常听说的磁盘转速，比如一个磁盘7200转，表示每分钟能转7200次，也就是说1秒钟能转120次，旋转延迟就是1/120/2 = 4.17ms；传输时间指的是从磁盘读出或将数据写入磁盘的时间，一般在零点几毫秒，相对于前两个时间可以忽略不计。那么访问一次磁盘的时间，即一次磁盘IO的时间约等于5+4.17 = 9ms左右，听起来还挺不错的，但要知道一台500 -MIPS的机器每秒可以执行5亿条指令，因为指令依靠的是电的性质，换句话说执行一次IO的时间可以执行40万条指令，数据库动辄十万百万乃至千万级数据，每次9毫秒的时间，显然是个灾难。下图是计算机硬件延迟的对比图，供大家参考：

![various-system-software-hardware-latencies](picture/数据库知识整理.assets/7f46a0a4.png)

考虑到磁盘IO是非常高昂的操作，计算机操作系统做了一些优化，当一次IO时，不光把当前磁盘地址的数据，而是把相邻的数据也都读取到内存缓冲区内，因为局部预读性原理告诉我们，当计算机访问一个地址的数据的时候，与其相邻的数据也会很快被访问到。每一次IO读取的数据我们称之为一页(page)。具体一页有多大数据跟操作系统有关，一般为4k或8k，也就是我们读取一页内的数据时候，实际上才发生了一次IO，这个理论对于索引的数据结构设计非常有帮助。

##### 索引的数据结构

每次查找数据时把磁盘IO次数控制在一个很小的数量级，最好是常数数量级。那么我们就想到如果一个高度可控的多路搜索树是否能满足需求呢？就这样，b+树应运而生。

##### 详解b+树

![b+树](picture/数据库知识整理.assets/7af22798.jpg)

浅蓝色的块我们称之为一个磁盘块，可以看到每个磁盘块包含几个数据项（深蓝色所示）和指针（黄色所示），如磁盘块1包含数据项17和35，包含指针P1、P2、P3，P1表示小于17的磁盘块，P2表示在17和35之间的磁盘块，P3表示大于35的磁盘块。真实的数据存在于叶子节点即3、5、9、10、13、15、28、29、36、60、75、79、90、99。非叶子节点只不存储真实的数据，只存储指引搜索方向的数据项，如17、35并不真实存在于数据表中。

##### b+树的查找过程

如图所示，如果要查找数据项29，那么首先会把磁盘块1由磁盘加载到内存，此时发生一次IO，在内存中用二分查找确定29在17和35之间，锁定磁盘块1的P2指针，内存时间因为非常短（相比磁盘的IO）可以忽略不计，通过磁盘块1的P2指针的磁盘地址把磁盘块3由磁盘加载到内存，发生第二次IO，29在26和30之间，锁定磁盘块3的P2指针，通过指针加载磁盘块8到内存，发生第三次IO，同时内存中做二分查找找到29，结束查询，总计三次IO。真实的情况是，3层的b+树可以表示上百万的数据，如果上百万的数据查找只需要三次IO，性能提高将是巨大的，如果没有索引，每个数据项都要发生一次IO，那么总共需要百万次的IO，显然成本非常非常高。

##### b+树性质

1.通过上面的分析，我们知道IO次数取决于b+数的高度h，假设当前数据表的数据为N，每个磁盘块的数据项的数量是m，则有h=㏒(m+1)N，当数据量N一定的情况下，m越大，h越小；而m = 磁盘块的大小 / 数据项的大小，磁盘块的大小也就是一个数据页的大小，是固定的，如果数据项占的空间越小，数据项的数量越多，树的高度越低。

这就是为什么每个数据项，即索引字段要尽量的小，比如int占4字节，要比bigint8字节少一半。这也是为什么b+树要求把真实的数据放到叶子节点而不是内层节点，一旦放到内层节点，磁盘块的数据项会大幅度下降，导致树增高。当数据项等于1时将会退化成线性表。

2.当b+树的数据项是复合的数据结构，比如(name,age,sex)的时候，b+数是按照从左到右的顺序来建立搜索树的，比如当(张三,20,F)这样的数据来检索的时候，b+树会优先比较name来确定下一步的所搜方向，如果name相同再依次比较age和sex，最后得到检索的数据；但当(20,F)这样的没有name的数据来的时候，b+树就不知道下一步该查哪个节点，因为建立搜索树的时候name就是第一个比较因子，必须要先根据name来搜索才能知道下一步去哪里查询。比如当(张三,F)这样的数据来检索时，b+树可以用name来指定搜索方向，但下一个字段age的缺失，所以只能把名字等于张三的数据都找到，然后再匹配性别是F的数据了， 这个是非常重要的性质，即索引的最左匹配特性。

### 慢查询优化

先总结一下索引的几大基本原则：

#### 建索引的几大原则

1.[**最左前缀匹配原则**]()，非常重要的原则，mysql会一直向右匹配直到遇到范围查询(>、<、between、like)就停止匹配，比如a = 1 and b = 2 and c > 3 and d = 4 如果建立(a,b,c,d)顺序的索引，d是用不到索引的，如果建立(a,b,d,c)的索引则都可以用到，a,b,d的顺序可以任意调整。

2.[**=和in可以乱序**]()，比如a = 1 and b = 2 and c = 3 建立(a,b,c)索引可以任意顺序，mysql的查询优化器会帮你优化成索引可以识别的形式。

3.[**尽量选择区分度高的列作为索引**]()，区分度的公式是count(distinct col)/count(*)，表示字段不重复的比例，比例越大我们扫描的记录数越少，唯一键的区分度是1，而一些状态、性别字段可能在大数据面前区分度就是0，那可能有人会问，这个比例有什么经验值吗？使用场景不同，这个值也很难确定，一般需要join的字段我们都要求是0.1以上，即平均1条扫描10条记录。

4.[**索引列不能参与计算**]()，保持列“干净”，比如from_unixtime(create_time) = ’2014-05-29’就不能使用到索引，原因很简单，b+树中存的都是数据表中的字段值，但进行检索时，需要把所有元素都应用函数才能比较，显然成本太大。所以语句应该写成create_time = unix_timestamp(’2014-05-29’)。

5.[**尽量的扩展索引，不要新建索引**]()。比如表中已经有a的索引，现在要加(a,b)的索引，那么只需要修改原来的索引即可。

#### 查询优化神器 - explain命令

这里需要强调rows是核心指标，绝大部分rows小的语句执行一定很快（有例外，下面会讲到）。所以优化语句基本上都是在优化rows。

#### 慢查询优化基本步骤

0.先运行看看是否真的很慢，注意设置SQL_NO_CACHE

1.where条件单表查，锁定最小返回记录表。这句话的意思是把查询语句的where都应用到表中返回的记录数最小的表开始查起，单表每个字段分别查询，看哪个字段的区分度最高

2.explain查看执行计划，是否与1预期一致（从锁定记录较少的表开始查询）

3.order by limit 形式的sql语句让排序的表优先查

4.了解业务方使用场景

5.加索引时参照建索引的几大原则

6.观察结果，不符合预期继续从0分析

### 几个慢查询案例

#### 复杂语句写法

很多情况下，我们写SQL只是为了实现功能，这只是第一步，不同的语句书写方式对于效率往往有本质的差别，这要求我们对mysql的执行计划和索引原则有非常清楚的认识，请看下面的语句：

```sql
select
   distinct cert.emp_id 
from
   cm_log cl 
inner join
   (
      select
         emp.id as emp_id,
         emp_cert.id as cert_id 
      from
         employee emp 
      left join
         emp_certificate emp_cert 
            on emp.id = emp_cert.emp_id 
      where
         emp.is_deleted=0
   ) cert 
      on (
         cl.ref_table='Employee' 
         and cl.ref_oid= cert.emp_id
      ) 
      or (
         cl.ref_table='EmpCertificate' 
         and cl.ref_oid= cert.cert_id
      ) 
where
   cl.last_upd_date >='2013-11-07 15:03:00' 
   and cl.last_upd_date<='2013-11-08 16:00:00';
```

0.先运行一下，53条记录 1.87秒，又没有用聚合语句，比较慢

```sql
53 rows in set (1.87 sec)
```

1.explain

![image-20210418172321553](picture/数据库知识整理.assets/image-20210418172321553.png)

简述一下执行计划，首先mysql根据idx_last_upd_date索引扫描cm_log表获得379条记录；

然后查表扫描了63727条记录，分为两部分，derived表示构造表，也就是不存在的表，可以简单理解成是一个语句形成的结果集，后面的数字表示语句的ID。derived2表示的是ID = 2的查询构造了虚拟表，并且返回了63727条记录。

我们再来看看ID = 2的语句究竟做了写什么返回了这么大量的数据，首先全表扫描employee表13317条记录，然后根据索引emp_certificate_empid关联emp_certificate表，rows = 1表示，每个关联都只锁定了一条记录，效率比较高。

获得后，再和cm_log的379条记录根据规则关联。从执行过程上可以看出返回了太多的数据，返回的数据绝大部分cm_log都用不到，因为cm_log只锁定了379条记录。

如何优化呢？可以看到我们在运行完后还是要和cm_log做join,那么我们能不能之前和cm_log做join呢？仔细分析语句不难发现，其基本思想是如果cm_log的ref_table是EmpCertificate就关联emp_certificate表，如果ref_table是Employee就关联employee表，我们完全可以拆成两部分，并用union连接起来，注意这里用union，而不用union all是因为原语句有“distinct”来得到唯一的记录，而union恰好具备了这种功能。如果原语句中没有distinct不需要去重，我们就可以直接使用union all了，[**因为使用union需要去重的动作，会影响SQL性能。**]()

优化过的语句如下：

```sql
select
   emp.id 
from
   cm_log cl 
inner join
   employee emp 
      on cl.ref_table = 'Employee' 
      and cl.ref_oid = emp.id  
where
   cl.last_upd_date >='2013-11-07 15:03:00' 
   and cl.last_upd_date<='2013-11-08 16:00:00' 
   and emp.is_deleted = 0  
union
select
   emp.id 
from
   cm_log cl 
inner join
   emp_certificate ec 
      on cl.ref_table = 'EmpCertificate' 
      and cl.ref_oid = ec.id  
inner join
   employee emp 
      on emp.id = ec.emp_id  
where
   cl.last_upd_date >='2013-11-07 15:03:00' 
   and cl.last_upd_date<='2013-11-08 16:00:00' 
   and emp.is_deleted = 0
```

4.不需要了解业务场景，只需要改造的语句和改造之前的语句保持结果一致

5.现有索引可以满足，不需要建索引

6.用改造后的语句实验一下，只需要10ms 降低了近200倍！

![image-20210418173101659](picture/数据库知识整理.assets/image-20210418173101659.png)

#### 明确应用场景

举这个例子的目的在于颠覆我们对列的区分度的认知，一般上我们认为区分度越高的列，越容易锁定更少的记录，但在一些特殊的情况下，这种理论是有局限性的。

```sql
select
   * 
from
   stage_poi sp 
where
   sp.accurate_result=1 
   and (
      sp.sync_status=0 
      or sp.sync_status=2 
      or sp.sync_status=4
   );
```

0.先看看运行多长时间,951条数据6.22秒，真的很慢。

```sql
951 rows in set (6.22 sec)
```

1.先explain，rows达到了361万，type = ALL表明是全表扫描。

![image-20210418173438123](picture/数据库知识整理.assets/image-20210418173438123.png)

2.所有字段都应用查询返回记录数，因为是单表查询 0已经做过了951条。

3.让explain的rows 尽量逼近951。

看一下accurate_result = 1的记录数：

```sql
select count(*),accurate_result from stage_poi  group by accurate_result;
+----------+-----------------+
| count(*) | accurate_result |
+----------+-----------------+
|     1023 |              -1 |
|  2114655 |               0 |
|   972815 |               1 |
+----------+-----------------+
```

我们看到accurate_result这个字段的区分度非常低，整个表只有-1,0,1三个值，加上索引也无法锁定特别少量的数据。

再看一下sync_status字段的情况：

```sql
select count(*),sync_status from stage_poi  group by sync_status;
+----------+-------------+
| count(*) | sync_status |
+----------+-------------+
|     3080 |           0 |
|  3085413 |           3 |
+----------+-------------+
```

同样的区分度也很低，根据理论，也不适合建立索引。

问题分析到这，好像得出了这个表无法优化的结论，两个列的区分度都很低，即便加上索引也只能适应这种情况，很难做普遍性的优化，比如当sync_status 0、3分布的很平均，那么锁定记录也是百万级别的。

4.找业务方去沟通，看看使用场景。业务方是这么来使用这个SQL语句的，每隔五分钟会扫描符合条件的数据，处理完成后把sync_status这个字段变成1,五分钟符合条件的记录数并不会太多，1000个左右。**了解了业务方的使用场景后，优化这个SQL就变得简单了，因为业务方保证了数据的不平衡，如果加上索引可以过滤掉绝大部分不需要的数据。**

5.根据建立索引规则，使用如下语句建立索引

```sql
alter table stage_poi add index idx_acc_status(accurate_result,sync_status);
```

6.观察预期结果,发现只需要200ms，快了30多倍。

```sql
952 rows in set (0.20 sec)
```

我们再来回顾一下分析问题的过程，单表查询相对来说比较好优化，大部分时候只需要把where条件里面的字段依照规则加上索引就好，如果只是这种“无脑”优化的话，显然一些区分度非常低的列，不应该加索引的列也会被加上索引，这样会对插入、更新性能造成严重的影响，同时也有可能影响其它的查询语句。

所以我们第4步调差SQL的使用场景非常关键，我们只有知道这个业务场景，才能更好地辅助我们更好的分析和优化查询语句。[**（也就是说虽然区分度很低，但是可以帮我们过滤大量的数据）**]()

#### 无法优化的语句

```sql
select
   c.id,
   c.name,
   c.position,
   c.sex,
   c.phone,
   c.office_phone,
   c.feature_info,
   c.birthday,
   c.creator_id,
   c.is_keyperson,
   c.giveup_reason,
   c.status,
   c.data_source,
   from_unixtime(c.created_time) as created_time,
   from_unixtime(c.last_modified) as last_modified,
   c.last_modified_user_id  
from
   contact c  
inner join
   contact_branch cb 
      on  c.id = cb.contact_id  
inner join
   branch_user bu 
      on  cb.branch_id = bu.branch_id 
      and bu.status in (
         1,
      2)  
   inner join
      org_emp_info oei 
         on  oei.data_id = bu.user_id 
         and oei.node_left >= 2875 
         and oei.node_right <= 10802 
         and oei.org_category = - 1  
   order by
      c.created_time desc  limit 0 ,
      10;
```

还是几个步骤。

0.先看语句运行多长时间，10条记录用了13秒，已经不可忍受。

```sql
10 rows in set (13.06 sec)
```

1.explain

![image-20210418174231132](picture/数据库知识整理.assets/image-20210418174231132.png)

从执行计划上看，mysql先查org_emp_info表扫描8849记录，再用索引idx_userid_status关联branch_user表，再用索引idx_branch_id关联contact_branch表，最后主键关联contact表。

rows返回的都非常少，看不到有什么异常情况。我们在看一下语句，发现后面有order by + limit组合，会不会是排序量太大搞的？于是我们简化SQL，去掉后面的order by 和 limit，看看到底用了多少记录来排序。

```sql
select
  count(*)
from
   contact c  
inner join
   contact_branch cb 
      on  c.id = cb.contact_id  
inner join
   branch_user bu 
      on  cb.branch_id = bu.branch_id 
      and bu.status in (
         1,
      2)  
   inner join
      org_emp_info oei 
         on  oei.data_id = bu.user_id 
         and oei.node_left >= 2875 
         and oei.node_right <= 10802 
         and oei.org_category = - 1  
+----------+
| count(*) |
+----------+
|   778878 |
+----------+
1 row in set (5.19 sec)
```

发现排序之前居然锁定了778878条记录，如果针对70万的结果集排序，将是灾难性的，怪不得这么慢，那我们能不能换个思路，先根据contact的created_time排序，再来join会不会比较快呢？

于是改造成下面的语句，也可以用straight_join来优化：

```sql
select
   c.id,
   c.name,
   c.position,
   c.sex,
   c.phone,
   c.office_phone,
   c.feature_info,
   c.birthday,
   c.creator_id,
   c.is_keyperson,
   c.giveup_reason,
   c.status,
   c.data_source,
   from_unixtime(c.created_time) as created_time,
   from_unixtime(c.last_modified) as last_modified,
   c.last_modified_user_id   
from
   contact c  
where
   exists (
      select
         1 
      from
         contact_branch cb  
      inner join
         branch_user bu        
            on  cb.branch_id = bu.branch_id        
            and bu.status in (
               1,
            2)      
         inner join
            org_emp_info oei           
               on  oei.data_id = bu.user_id           
               and oei.node_left >= 2875           
               and oei.node_right <= 10802           
               and oei.org_category = - 1      
         where
            c.id = cb.contact_id    
      )    
   order by
      c.created_time desc  limit 0 ,
      10;
```

验证一下效果 预计在1ms内，提升了13000多倍！

```sql
10 rows in set (0.00 sec)
```

本以为至此大工告成，但我们在前面的分析中漏了一个细节，先排序再join和先join再排序理论上开销是一样的，为何提升这么多是因为有一个limit！

大致执行过程是：mysql先按索引排序得到前10条记录，然后再去join过滤，当发现不够10条的时候，再次去10条，再次join，这显然在内层join过滤的数据非常多的时候，将是灾难的，极端情况，内层一条数据都找不到，mysql还傻乎乎的每次取10条，几乎遍历了这个数据表！

用不同参数的SQL试验下：

```sql
select
   sql_no_cache   c.id,
   c.name,
   c.position,
   c.sex,
   c.phone,
   c.office_phone,
   c.feature_info,
   c.birthday,
   c.creator_id,
   c.is_keyperson,
   c.giveup_reason,
   c.status,
   c.data_source,
   from_unixtime(c.created_time) as created_time,
   from_unixtime(c.last_modified) as last_modified,
   c.last_modified_user_id    
from
   contact c   
where
   exists (
      select
         1        
      from
         contact_branch cb         
      inner join
         branch_user bu                     
            on  cb.branch_id = bu.branch_id                     
            and bu.status in (
               1,
            2)                
         inner join
            org_emp_info oei                           
               on  oei.data_id = bu.user_id                           
               and oei.node_left >= 2875                           
               and oei.node_right <= 2875                           
               and oei.org_category = - 1                
         where
            c.id = cb.contact_id           
      )        
   order by
      c.created_time desc  limit 0 ,
      10;
Empty set (2 min 18.99 sec)
```

2 min 18.99 sec！比之前的情况还糟糕很多。由于mysql的nested loop机制，遇到这种情况，基本是无法优化的。这条语句最终也只能交给应用系统去优化自己的逻辑了。

通过这个例子我们可以看到，并不是所有语句都能优化，而往往我们优化时，由于SQL用例回归时落掉一些极端情况，会造成比原来还严重的后果。所以，第一：不要指望所有语句都能通过SQL优化，第二：不要过于自信，只针对具体case来优化，而忽略了更复杂的情况。

慢查询的案例就分析到这儿，以上只是一些比较典型的案例。我们在优化过程中遇到过超过1000行，涉及到16个表join的“垃圾SQL”，也遇到过线上线下数据库差异导致应用直接被慢查询拖死，也遇到过varchar等值比较没有写单引号，还遇到过笛卡尔积查询直接把从库搞死。再多的案例其实也只是一些经验的积累，如果我们熟悉查询优化器、索引的内部原理，那么分析这些案例就变得特别简单了。

# 分布式

## 1.分库分表

### 分库分表是什么

关系型数据库本身比较容易成为系统瓶颈，单机存储容量、连接数、处理能力都有限。当单表的数据量达到1000W或100G以后，由于查询维度较多，即使添加从库、优化索引，做很多操作时性能仍下降严重。

### 垂直分表

用户在浏览商品列表时，只有对某商品感兴趣时才会查看该商品的详细描述。因此，商品信息中商品描述字段访问频次较低，且该字段存储占用空间较大，访问单个数据IO时间较长；商品信息中商品名称、商品图片、商品价格等其他字段数据访问频次较高。

由于这两种数据的特性不一样，因此他考虑将商品信息表拆分如下：

> 将访问频次低的商品描述信息单独存放在一张表中，访问频次较高的商品基本信息单独放在一张表中。
>

**垂直分表定义：将一个表按照字段分成多表，每个表存储其中一部分字段。**
它带来的提升是：

> 1.为了[**避免IO争抢并减少锁表的几率**]()，查看详情的用户与商品信息浏览互不影响
>
> 2.充分发挥热门数据的操作效率，**商品信息的操作的高效率不会被商品描述的低效率所拖累**。

为什么大字段IO效率低：

> 第一是由于数据量本身大，需要[**更长的读取时间**]()；
>
> 第二是跨页，页是数据库存储单位，很多查找及定位操作都是以页为单位，单页内的数据行越多数据库整体性能越好，而[**大字段占用空间大，单页内存储行数少**]()，因此IO效率较低。
>
> 第三，数据库以行为单位将数据加载到内存中，这样表中字段长度较短且访问频率较高，[**内存能加载更多的数据，命中率更高**]()，减少了磁盘IO，从而提升了数据库性能。

所以，当表数据量很大时，可以将表按字段切开，将热门字段、冷门字段分开放置在不同库中，这些库可以放在不同的存储设备上，避免IO争抢。垂直切分带来的性能提升主要集中在热门数据的操作效率上，而且磁盘争用情况减少。

通常我们按以下原则进行垂直拆分:

> 把不常用的字段单独放在一张表;
>
> 经常组合查询的列放在一张表中;
>
> 把text，blob等大字段拆分出来放在附表中;

### 垂直分库

通过垂直分表性能得到了一定程度的提升，但是还没有达到要求，并且磁盘空间也快不够了，因为数据还是始终限制在一台服务器，库内垂直分表只解决了单一表数据量过大的问题，但没有将表分布到不同的服务器上，因此每个表还是竞争同一个物理机的CPU、内存、网络IO、磁盘。

经过思考，他把原有的SELLER_DB(卖家库)，分为了PRODUCT_DB(商品库)和STORE_DB(店铺库)，并把这两个库分散到不同服务器，如下图：

由于商品信息与商品描述业务耦合度较高，因此一起被存放在PRODUCT_DB(商品库)；而店铺信息相对独立，因此单独被存放在STORE_DB(店铺库)。

**[垂直分库是指按照业务将表进行分类]()，分布到不同的数据库上面，每个库可以放在不同的服务器上，它的核心理念是专库专用。**

它带来的提升是：

> 解决业务层面的耦合，[业务清晰]()
>
> 能对不同业务的数据进行分级管理、维护、监控、扩展等
>
> 高并发场景下，垂直分库一定程度的提升[IO、数据库连接数]()、降低单机硬件资源的瓶颈
>

垂直分库通过将表按业务分类，然后分布在不同数据库，并且可以将这些数据库部署在不同服务器上，从而达到多个服务器共同分摊压力的效果，但是依然没有解决单表数据量过大的问题。

### 水平分库

经过垂直分库后，数据库性能问题得到一定程度的解决，但是随着业务量的增长，PRODUCT_DB(商品库)单库存储数据已经超出预估。粗略估计，目前有8w店铺，每个店铺平均150个不同规格的商品，再算上增长，那商品数量得往1500w+上预估，并且PRODUCT_DB(商品库)属于访问非常频繁的资源，单台服务器已经无法支撑。此时该如何优化？

再次分库？但是从业务角度分析，目前情况已经无法再次垂直分库。

尝试水平分库，将店铺ID为单数的和店铺ID为双数的商品信息分别放在两个库中。


也就是说，要操作某条数据，先分析这条数据所属的店铺ID。如果店铺ID为双数，将此操作映射至RRODUCT_DB1(商品库1)；如果店铺ID为单数，将操作映射至RRODUCT_DB2(商品库2)。此操作要访问数据库名称的表达式为RRODUCT_DB[店铺ID%2 + 1] 。

> **水平分库是把同一个表的数据按一定规则拆到不同的数据库中，每个库可以放在不同的服务器上。**

它带来的提升是：

> 解决了单库大数据，高并发的性能瓶颈。
>
> 提高了系统的稳定性及可用性。
>
> 稳定性体现在IO冲突减少，锁定减少，可用性指某个库出问题，部分可用

当一个应用难以再细粒度的垂直切分，或切分后数据量行数巨大，[**存在单库读写、存储性能瓶颈，这时候就需要进行水平分库**]()了，经过水平切分的优化，往往能解决单库存储量及性能瓶颈。但由于同一个表被分配在不同的数据库，需要额外进行数据操作的路由工作，因此大大提升了系统复杂度。

### 水平分表

其目的也是为解决单表数据量大的问题。

如果商品ID为双数，将此操作映射至商品信息1表；如果商品ID为单数，将操作映射至商品信息2表。此操作要访问表名称的表达式为商品信息[商品ID%2 + 1] 。

> **水平分表是在同一个数据库内，把同一个表的数据按一定规则拆到多个表中。**

它带来的提升是：

> 优化[**单一表数据量过大**]()而产生的性能问题
>
> 避免IO争抢并减少锁表的几率
>
> 库内的水平分表，解决了单一表数据量过大的问题，分出来的小表中只包含一部分数据，从而使得单个表的数据量变小，提高检索性能。
>


垂直分表：可以把一个宽表的字段按访问频次、是否是大字段的原则拆分为多个表，这样既能使业务清晰，还能提升部分性能。拆分后，尽量从业务角度避免联查，否则性能方面将得不偿失。

垂直分库：可以把多个表按业务耦合松紧归类，分别存放在不同的库，这些库可以分布在不同服务器，从而使访问压力被多服务器负载，大大提升性能，同时能提高整体架构的业务清晰度，不同的业务库可根据自身情况定制优化方案。但是它需要解决跨库带来的所有复杂问题。

水平分库：可以把一个表的数据(按数据行)分到多个不同的库，每个库只有这个表的部分数据，这些库可以分布在不同服务器，从而使访问压力被多服务器负载，大大提升性能。它不仅需要解决跨库带来的所有复杂问题，还要解决数据路由的问题(数据路由问题后边介绍)。

水平分表：可以把一个表的数据(按数据行)分到多个同一个数据库的多张表中，每个表只有这个表的部分数据，这样做能小幅提升性能，它仅仅作为水平分库的一个补充优化。

### 总结

一般来说，[在系统设计阶段就应该根据业务耦合松紧来确定垂直分库，垂直分表方案。]()

在数据量及访问压力不是特别大的情况，首先考虑[缓存、读写分离、索引技术等方案]()。若数据量极大，且持续增长，再考虑水平分库水平分表方案。

## 2.分表的时候,怎么设计主键?

### 数据库自增 id

这个就是说你的系统里每次得到一个 id，都是往一个库的一个表里插入一条没什么业务含义的数据，然后获取一个数据库自增的一个 id。拿到这个 id 之后再往对应的分库分表里去写入。

> 这个方案的好处就是方便简单，谁都会用；

> **缺点就是单库生成**自增 id，要是高并发的话，就会有瓶颈的；

如果你硬是要改进一下，那么就专门开一个服务出来，这个服务每次就拿到当前 id 最大值，然后自己递增几个 id，一次性返回一批 id，然后再把当前最大 id 值修改成递增几个 id 之后的一个值；但是**无论如何都是基于单个数据库**。

**适合的场景**：你分库分表就俩原因，要不就是单库并发太高，要不就是单库数据量太大；除非是你**并发不高，但是数据量太大**导致的分库分表扩容，你可以用这个方案，因为可能每秒最高并发最多就几百，那么就走单独的一个库和表生成自增主键即可。

### 设置数据库 sequence 或者表自增字段步长

可以通过设置数据库 sequence 或者表的自增字段步长来进行水平伸缩。

比如说，现在有 8 个服务节点，每个服务节点使用一个 sequence 功能来产生 ID，每个 sequence 的起始 ID 不同，并且依次递增，步长都是 8。



<img src="picture/面试手册.assets/v2-fe6dcc59232112ba10156d35eb97c30a_1440w.jpg" alt="img" style="zoom: 33%;" />



**适合的场景**：在用户防止产生的 ID 重复时，这种方案实现起来比较简单，也能达到性能目标。但是服务节点固定，步长也固定，将来如果还要增加服务节点，就不好搞了。

### UUID

> 好处就是本地生成，不要基于数据库来了；

> 不好之处就是，UUID 太长了、占用空间大，**作为主键性能太差**了；
>
> 更重要的是，UUID 不具有有序性，会导致 B+ 树索引在写的时候有过多的随机写操作（连续的 ID 可以产生部分顺序写）。
>
> 还有，由于在写的时候不能产生有顺序的 append 操作，而需要进行 insert 操作，将会读取整个 B+ 树节点的内存，在插入这条记录后会将整个节点写回磁盘，这种操作在记录占用空间比较大的情况下，性能下降明显。

适合的场景：如果你是要随机生成个什么文件名、编号之类的，你可以用 UUID，但是作为主键是不能用 UUID 的。

```java
UUID.randomUUID().toString().replace(“-”, “”) -> sfsdf23423rr234sfdaf
```

### 获取系统当前时间

这个就是获取当前时间即可，但是问题是，**并发很高的时候**，比如一秒并发几千，**会有重复的情况**，这个是肯定不合适的。基本就不用考虑了。

适合的场景：一般如果用这个方案，是将当前时间跟很多其他的业务字段拼接起来，作为一个 id，如果业务上你觉得可以接受，那么也是可以的。你可以将别的业务字段值跟当前时间拼接起来，组成一个全局唯一的编号。

### snowflake 算法

snowflake 算法是 twitter 开源的分布式 id 生成算法，采用 Scala 语言实现，是把一个 64 位的 long 型的 id，1 个 bit 是不用的，用其中的 41 bit 作为毫秒数，用 10 bit 作为工作机器 id，12 bit 作为序列号。

- 1 bit：不用，为啥呢？因为二进制里第一个 bit 为如果是 1，那么都是负数，但是我们生成的 id 都是正数，所以第一个 bit 统一都是 0。
- 41 bit：表示的是时间戳，单位是毫秒。41 bit 可以表示的数字多达 `2^41 - 1`，也就是可以标识 `2^41 - 1` 个毫秒值，换算成年就是表示69年的时间。
- 10 bit：记录工作机器 id，代表的是这个服务最多可以部署在 2^10台机器上哪，也就是1024台机器。但是 10 bit 里 5 个 bit 代表机房 id，5 个 bit 代表机器 id。意思就是最多代表 `2^5`个机房（32个机房），每个机房里可以代表 `2^5` 个机器（32台机器）。
- 12 bit：这个是用来记录同一个毫秒内产生的不同 id，12 bit 可以代表的最大正整数是 `2^12 - 1 = 4096`，也就是说可以用这个 12 bit 代表的数字来区分**同一个毫秒内**的 4096 个不同的 id。

```text
0 | 0001100 10100010 10111110 10001001 01011100 00 | 10001 | 1 1001 | 0000 00000000
```



```java
public class IdWorker {

    private long workerId;
    private long datacenterId;
    private long sequence;

    public IdWorker(long workerId, long datacenterId, long sequence) {
        // sanity check for workerId
        // 这儿不就检查了一下，要求就是你传递进来的机房id和机器id不能超过32，不能小于0
        if (workerId > maxWorkerId || workerId < 0) {
            throw new IllegalArgumentException(
                    String.format("worker Id can't be greater than %d or less than 0", maxWorkerId));
        }
        if (datacenterId > maxDatacenterId || datacenterId < 0) {
            throw new IllegalArgumentException(
                    String.format("datacenter Id can't be greater than %d or less than 0", maxDatacenterId));
        }
        System.out.printf(
                "worker starting. timestamp left shift %d, datacenter id bits %d, worker id bits %d, sequence bits %d, workerid %d",
                timestampLeftShift, datacenterIdBits, workerIdBits, sequenceBits, workerId);

        this.workerId = workerId;
        this.datacenterId = datacenterId;
        this.sequence = sequence;
    }

    private long twepoch = 1288834974657L;

    private long workerIdBits = 5L;
    private long datacenterIdBits = 5L;

    // 这个是二进制运算，就是 5 bit最多只能有31个数字，也就是说机器id最多只能是32以内
    private long maxWorkerId = -1L ^ (-1L << workerIdBits);

    // 这个是一个意思，就是 5 bit最多只能有31个数字，机房id最多只能是32以内
    private long maxDatacenterId = -1L ^ (-1L << datacenterIdBits);
    private long sequenceBits = 12L;

    private long workerIdShift = sequenceBits;
    private long datacenterIdShift = sequenceBits + workerIdBits;
    private long timestampLeftShift = sequenceBits + workerIdBits + datacenterIdBits;
    private long sequenceMask = -1L ^ (-1L << sequenceBits);

    private long lastTimestamp = -1L;

    public long getWorkerId() {
        return workerId;
    }

    public long getDatacenterId() {
        return datacenterId;
    }

    public long getTimestamp() {
        return System.currentTimeMillis();
    }

    public synchronized long nextId() {
        // 这儿就是获取当前时间戳，单位是毫秒
        long timestamp = timeGen();

        if (timestamp < lastTimestamp) {
            System.err.printf("clock is moving backwards.  Rejecting requests until %d.", lastTimestamp);
            throw new RuntimeException(String.format(
                    "Clock moved backwards.  Refusing to generate id for %d milliseconds", lastTimestamp - timestamp));
        }

        if (lastTimestamp == timestamp) {
            // 这个意思是说一个毫秒内最多只能有4096个数字
            // 无论你传递多少进来，这个位运算保证始终就是在4096这个范围内，避免你自己传递个sequence超过了4096这个范围
            sequence = (sequence + 1) & sequenceMask;
            if (sequence == 0) {
                timestamp = tilNextMillis(lastTimestamp);
            }
        } else {
            sequence = 0;
        }

        // 这儿记录一下最近一次生成id的时间戳，单位是毫秒
        lastTimestamp = timestamp;

        // 这儿就是将时间戳左移，放到 41 bit那儿；
        // 将机房 id左移放到 5 bit那儿；
        // 将机器id左移放到5 bit那儿；将序号放最后12 bit；
        // 最后拼接起来成一个 64 bit的二进制数字，转换成 10 进制就是个 long 型
        return ((timestamp - twepoch) << timestampLeftShift) | (datacenterId << datacenterIdShift)
                | (workerId << workerIdShift) | sequence;
    }

    private long tilNextMillis(long lastTimestamp) {
        long timestamp = timeGen();
        while (timestamp <= lastTimestamp) {
            timestamp = timeGen();
        }
        return timestamp;
    }

    private long timeGen() {
        return System.currentTimeMillis();
    }

    // ---------------测试---------------
    public static void main(String[] args) {
        IdWorker worker = new IdWorker(1, 1, 1);
        for (int i = 0; i < 30; i++) {
            System.out.println(worker.nextId());
        }
    }

}
```

怎么说呢，大概这个意思吧，就是说 41 bit 是当前毫秒单位的一个时间戳，就这意思；然后 5 bit 是你传递进来的一个**机房** id（但是最大只能是 32 以内），另外 5 bit 是你传递进来的**机器** id（但是最大只能是 32 以内），剩下的那个 12 bit序列号，就是如果跟你上次生成 id 的时间还在一个毫秒内，那么会把顺序给你累加，最多在 4096 个序号以内。

所以你自己利用这个工具类，自己搞一个服务，然后对每个机房的每个机器都初始化这么一个东西，刚开始这个机房的这个机器的序号就是 0。然后每次接收到一个请求，说这个机房的这个机器要生成一个 id，你就找到对应的 Worker 生成。

利用这个 snowflake 算法，你可以开发自己公司的服务，甚至对于机房 id 和机器 id，反正给你预留了 5 bit + 5 bit，你换成别的有业务含义的东西也可以的。

这个 snowflake 算法相对来说还是比较靠谱的，所以你要真是搞分布式 id 生成，如果是高并发啥的，那么用这个应该性能比较好，一般每秒几万并发的场景，也足够你用了。

## 3.自增 ID 申请完了会发生什么事情?

首先，创建一个最简单的表，只包含一个自增id，并插入一条数据。

```
create table t0(id int unsigned auto_increment primary key) ;insert into t0 values(null);
```

通过show命令 `show create table t0;` 查看表情况

```
CREATE TABLE `t0` (  `id` int(10) unsigned NOT NULL AUTO_INCREMENT,  PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8
```

可以发现 AUTO_INCREMENT 已经自动变成2，这离用完还有很远，我们可以算下最大当前声明的自增ID最大是多少，由于这里定义的是 `intunsigned`，所以最大可以达到2的32幂次方 - 1 = 4294967295

这里有个小技巧，可以在创建表的时候，直接声明AUTO_INCREMENT的初始值

```
create table t1(id int unsigned auto_increment primary key)  auto_increment = 4294967295;insert into t1 values(null);
```

同样，通过show命令，查看t1的表结构

```
CREATE TABLE `t1` (  `id` int(10) unsigned NOT NULL AUTO_INCREMENT,  PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=4294967295 DEFAULT CHARSET=utf8
```

可以发现，AUTO_INCREMENT已经变成4294967295了，当想再尝试插入一条数据时，得到了下面的异常结果

```
17:28:03    insert into t1 values(null) Error Code: 1062. Duplicate entry '4294967295' for key 'PRIMARY'    0.00054 sec
```

> **说明，当再次插入时，使用的自增ID还是 `4294967295`，报主键冲突的错误。**

4294967295，这个数字已经可以应付大部分的场景了，如果你的服务会经常性的插入和删除数据的话，还是存在用完的风险，**建议采用bigint unsigned**，这个数字就大了。

**不过，还存在另一种情况，如果在创建表没有显示申明主键，会怎么办？**

如果是这种情况，InnoDB会自动帮你创建一个不可见的、长度为6字节的row_id，而且InnoDB 维护了一个全局的 dictsys.row_id，所以未定义主键的表都共享该row_id，每次插入一条数据，都把全局row_id当成主键id，然后全局row_id加1

该全局row_id在代码实现上使用的是bigint unsigned类型，但实际上只给row_id留了6字节，这种设计就会存在一个问题：**如果全局row_id一直涨，一直涨，直到2的48幂次-1时，这个时候再+1，row_id的低48位都为0，结果在插入新一行数据时，拿到的row_id就为0，存在主键冲突的可能性。**

所以，为了避免这种隐患，每个表都需要定一个主键。

## 4.主从复制和读写分离

### 主从复制

主要涉及三个线程：binlog 线程、I/O 线程和 SQL 线程。

- **binlog 线程** ：负责将主服务器上的数据更改写入二进制日志（Binary log）中。
- **I/O 线程** ：负责从主服务器上读取二进制日志，并写入从服务器的中继日志（Relay log）。
- **SQL 线程** ：负责读取中继日志，解析出主服务器已经执行的数据更改并在从服务器中重放（Replay）。

![img](picture/数据库知识整理.assets/68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f6d61737465722d736c6176652e706e67)

### 读写分离

主服务器处理写操作以及实时性要求比较高的读操作，而从服务器处理读操作。

读写分离能提高性能的原因在于：

- 主从服务器负责各自的读和写，极大程度缓解了锁的争用；
- 从服务器可以使用 MyISAM，提升查询性能以及节约系统开销；
- 增加冗余，提高可用性。

读写分离常用代理方式来实现，代理服务器接收应用层传来的读写请求，然后决定转发到哪个服务器。

![img](picture/数据库知识整理.assets/68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f6d61737465722d736c6176652d70726f78792e706e67)

### 主从复制的几个问题

> **主从复制的几种方式:**
>
> **异步复制:**[**MySQL的默认设置**]()
>
> master只需要完成自己的数据库操作即可。至于slaves是否收到二进制日志，是否完成操作，不用关心。
>
> **半同步复制:**
>
> master只保证slaves中的一个操作成功，就返回，其他slave不管。 这个功能，是由google为MySQL引入的。

**问题1：**master的写操作，slaves被动的进行一样的操作，保持数据一致性，那么slave是否可以主动的进行写操作？

> 假设slave可以主动的进行写操作，slave又无法通知master，这样就导致了master和slave数据不一致了。
>
> **因此slave不应该进行写操作，至少是slave上涉及到复制的数据库不可以写。**

**问题2：**主从复制中，可以有N个slave,可是这些slave又不能进行写操作，要他们干嘛？

> 1.**实现数据备份:**
> 类似于高可用的功能，一旦master挂了，可以让slave顶上去，同时slave提升为master。
>
> **2.异地容灾:**比如master在北京，地震挂了，那么在上海的slave还可以继续。
>
> **3.读写分离：**主要用于实现scale out,分担负载,可以将读的任务分散到slaves上。

**问题3：**主从复制中有master,slave1,slave2,...等等这么多MySQL数据库，那比如一个JAVA WEB应用到底应该连接哪个数据库?

> 找一个组件，application program只需要与它打交道，用它来完成MySQL的，实现SQL语句的路由。
> MySQL proxy并不负责，怎么从众多的slaves挑一个？可以交给另一个组件(比如haproxy)来完成。

**问题4：**如果MySQL proxy , direct , master他们中的某些挂了怎么办？

> ***一般都会弄个副***，以防不测。同样的，可以给这些关键的节点来个备份。

**问题5：**当master的二进制日志每产生一个事件，都需要发往slave，如果我们有N个slave,那是发N次，还是只发一次？如果只发一次，发给了slave-1，那slave-2,slave-3,...它们怎么办？

> 显然，应该发N次。实际上，在MySQL master内部，维护N个线程，每一个线程负责将二进制日志文件发往对应的slave。master既要负责写操作，还的维护N个线程，负担会很重。**可以这样，slave-1是master的从，slave-1又是slave-2,slave-3,...的主，同时slave-1不再负责select。 slave-1将master的复制线程的负担，转移到自己的身上。这就是所谓的多级复制的概念。**

**问题6：**当一个select发往MySQL proxy，可能这次由slave-2响应，下次由slave-3响应，这样的话，就无法利用查询缓存了。

> 应该找一个共享式的缓存，比如mem***来解决。将slave-2,slave-3,...这些查询的结果都缓存至mem***中。

### Mysql 高并发环境解决方案?

> **MySQL 高并发环境解决方案：** 分库 分表 分布式 增加二级缓存
>
> **现有解决方式：**水平分库分表，由单点分布到多点数据库中，从而降低单点数据库压力。
>
> **集群方案：**解决DB宕机带来的单点DB不能访问问题。
>
> **读写分离策略：**极大限度提高了应用中Read数据的速度和并发量。无法解决高写入压力。

