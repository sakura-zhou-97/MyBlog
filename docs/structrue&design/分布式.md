# 分布式

## 1.什么是RPC

> RPC是指远程过程调用，也就是说两台服务器A，B，一个应用部署在A服务器上，想要调用B服务器上应用提供的函数/方法，由于不在一个内存空间，不能直接调用，需要通过网络来表达调用的语义和传达调用的数据。

> 为什么要用RPC呢？就是无法在一个进程内，甚至一个计算机内通过本地调用的方式完成的需求，比如不同的系统间的通讯，甚至不同的组织间的通讯，由于计算能力需要横向扩展，需要在多台机器组成的集群上部署应用。

RPC就是要像调用本地的函数一样去调远程函数。在研究RPC前，我们先看看本地调用是怎么调的。假设我们要调用函数Multiply来计算lvalue * rvalue的结果:

```cpp
1 int Multiply(int l, int r) {
2    int y = l * r;
3    return y;
4 }
5 
6 int lvalue = 10;
7 int rvalue = 20;
8 int l_times_r = Multiply(lvalue, rvalue);
```

那么在第8行时，我们实际上执行了以下操作：

```
 将 lvalue 和 rvalue 的值压栈
 进入Multiply函数，取出栈中的值10 和 20，将其赋予 l 和 r
 执行第2行代码，计算 l * r ，并将结果存在 y
 将 y 的值压栈，然后从Multiply返回
 第8行，从栈中取出返回值 200 ，并赋值给 l_times_r
 以上5步就是执行本地调用的过程。
```

在远程调用时，我们需要执行的函数体是在远程的机器上的，也就是说，Multiply是在另一个进程中执行的。这就带来了几个新问题：

1.Call ID映射。我们怎么告诉远程机器我们要调用Multiply，而不是Add或者FooBar呢？

在本地调用中，函数体是直接通过函数指针来指定的，我们调用Multiply，编译器就自动帮我们调用它相应的函数指针。但是在远程调用中，函数指针是不行的，因为两个进程的地址空间是完全不一样的。所以，在RPC中，所有的函数都必须有自己的一个ID。这个ID在所有进程中都是唯一确定的。客户端在做远程过程调用时，必须附上这个ID。然后我们还需要在客户端和服务端分别维护一个 {函数 <--> Call ID} 的对应表。两者的表不一定需要完全相同，但相同的函数对应的Call ID必须相同。当客户端需要进行远程调用时，它就查一下这个表，找出相应的Call ID，然后把它传给服务端，服务端也通过查表，来确定客户端需要调用的函数，然后执行相应函数的代码。

2.序列化和反序列化。客户端怎么把参数值传给远程的函数呢？

在本地调用中，我们只需要把参数压到栈里，然后让函数自己去栈里读就行。但是在远程过程调用时，客户端跟服务端是不同的进程，不能通过内存来传递参数。甚至有时候客户端和服务端使用的都不是同一种语言（比如服务端用C++，客户端用Java或者Python）。这时候就需要客户端把参数先转成一个字节流，传给服务端后，再把字节流转成自己能读取的格式。这个过程叫序列化和反序列化。同理，从服务端返回的值也需要序列化反序列化的过程。

3.网络传输。

远程调用往往用在网络上，客户端和服务端是通过网络连接的。所有的数据都需要通过网络传输，因此就需要有一个网络传输层。网络传输层需要把Call ID和序列化后的参数字节流传给服务端，然后再把序列化后的调用结果传回客户端。只要能完成这两者的，都可以作为传输层使用。因此，它所使用的协议其实是不限的，能完成传输就行。尽管大部分RPC框架都使用TCP协议，但其实UDP也可以，而gRPC干脆就用了HTTP2。Java的Netty也属于这层的东西。

所以，要实现一个 RPC 框架，只需要把以下三点实现了就基本完成了：

- Call ID 映射：可以直接使用函数字符串，也可以使用整数 ID。映射表一般就是一个哈希表。
- 序列化反序列化：可以自己写，也可以使用 Protobuf 或者 FlatBuffers 之类的。
- 网络传输库：可以自己写 Socket，或者用 Asio，ZeroMQ，Netty 之类。

## 2.为什么RPC

![image-20210311101919903](picture/分布式.assets/image-20210311101919903.png)

## 3.关于Netty

而Netty框架不局限于RPC，**更多的是作为一种网络协议的实现框架**，比如HTTP，**由于RPC需要高效的网络通信，就可能选择以Netty作为基础。**

大体上来说，**Netty就是提供一种事件驱动的，责任链式（也可以说是流水线）的网络协议实现方式**。网络协议包含很多层次，很多部分组成，如传输层协议，编码解码，压缩解压，身份认证，加密解密，请求的处理逻辑，怎么能够更好的复用，扩展，业界通用的方法就是责任链，

一个请求应答网络交互通常包含两条链，一条链（Upstream）是从传输层，经过一系列步骤，如身份认证，解密，日志，流控，最后到达业务层，一条链（DownStream）是业务层返回后，又经过一系列步骤，如加密等，又回到传输层。 

![image-20210311102038593](picture/分布式.assets/image-20210311102038593.png)

这样每一层都有一个处理接口，都可以进行不同的操作，比如身份认证，加解密，日志，流控，将不同的处理实现像拼积木那样插接起来就可以实现一个网络协议了（快速开发）。每一层都有自己的实现，上层不需要关注面向网络的操作（可维护）。Netty已经提供了很多实现。

当然Netty还有许多好处，比如对非阻塞IO（NIO）的支持，比如在链上传递时最大程度的减少buffer的copy（高性能）。

## 4.通信协议 RPC vs HTTP

![image-20210311103802576](picture/分布式.assets/image-20210311103802576.png)

在 RPC 中可选的网络传输方式有多种，可以选择 TCP 协议、UDP 协议、HTTP 协议。

每一种协议对整体的性能和效率都有不同的影响，如何选择一个正确的网络传输协议呢?首先要搞明白各种传输协议在 RPC 中的工作方式。

**基于 TCP 协议的 RPC 调用**

由服务的调用方与服务的提供方建立 Socket 连接，并由服务的调用方通过 Socket 将需要调用的接口名称、方法名称和参数序列化后传递给服务的提供方，服务的提供方反序列化后再利用反射调用相关的方法。

将结果返回给服务的调用方，整个基于 TCP 协议的 RPC 调用大致如此。

但是在实例应用中则会进行一系列的封装，如 RMI 便是在 TCP 协议上传递可序列化的 Java 对象。

**基于 HTTP 协议的 RPC 调用**

该方法更像是访问网页一样，只是它的返回结果更加单一简单。

其大致流程为：由服务的调用者向服务的提供者发送请求，这种请求的方式可能是 GET、POST、PUT、DELETE 等中的一种，服务的提供者可能会根据不同的请求方式做出不同的处理，或者某个方法只允许某种请求方式。

而调用的具体方法则是根据 URL 进行方法调用，而方法所需要的参数可能是对服务调用方传输过去的 XML 数据或者 JSON 数据解析后的结果，返回 JOSN 或者 XML 的数据结果。

由于目前有很多开源的 Web 服务器，如 Tomcat，所以其实现起来更加容易，就像做 Web 项目一样。

**两种方式对比**

基于 TCP 的协议实现的 RPC 调用：

优点：**由于 TCP 协议处于协议栈的下层，能够更加灵活地对协议字段进行定制，减少网络开销，提高性能，实现更大的吞吐量和并发数。**

缺点：但是需要更多关注底层复杂的细节，实现的代价更高。同时对不同平台，如安卓，iOS 等，需要重新开发出不同的工具包来进行请求发送和相应解析，工作量大，难以快速响应和满足用户需求。

基于 HTTP 协议实现的 RPC ：

优点：可以使用 JSON 和 XML 格式的请求或响应数据，而 JSON 和 XML 作为通用的格式标准(使用 HTTP 协议也需要序列化和反序列化，不过这不是该协议下关心的内容，成熟的 Web 程序已经做好了序列化内容)，开源的解析工具已经相当成熟，在其上进行二次开发会非常便捷和简单。

缺点：但是由于 HTTP 协议是上层协议，发送包含同等内容的信息，使用 HTTP 协议传输所占用的字节数会比使用 TCP 协议传输所占用的字节数更高。

因此在同等网络下，通过 HTTP 协议传输相同内容，效率会比基于 TCP 协议的数据效率要低，信息传输所占用的时间也会更长，当然压缩数据，能够缩小这一差距。

## 5.RPC和HTTP的应用场景

RPC 主要用于公司内部的服务调用，性能消耗低，传输效率高，实现复杂。

HTTP 主要用于对外的异构环境，浏览器接口调用，App 接口调用，第三方接口调用等。

RPC 使用场景(大型的网站，内部子系统较多、接口非常多的情况下适合使用 RPC)：

- 长链接。不必每次通信都要像 HTTP 一样去 3 次握手，减少了网络开销。
- 注册发布机制。RPC 框架一般都有注册中心，有丰富的监控管理;发布、下线接口、动态扩展等，对调用方来说是无感知、统一化的操作。
- 安全性，没有暴露资源操作。
- 微服务支持。就是最近流行的服务化架构、服务化治理，RPC 框架是一个强力的支撑。

## 6.RabbitMq实现的RPC

基本流程如下图：

![image-20210311110534253](picture/分布式.assets/image-20210311110534253.png)

RabbitMQ中实现RPC的机制是：

(1) 客户端发送消息请求时，在消息的属性设置两个值reply_to和correlation_id

```
 reply_to和correlation_id，这两个属性是AMQP中消息的属性，reply_to是我们上面提到的服务端处理完成之后将结果发送给客户端指定的队列；而correlation_id是将RPC的请求和响应关联起来，此次RPC请求的标识号，服务器处理消息完成后将此属性返回，客户端根据这个id理解哪条请求被成功执行或执行失败。

reply_to（回复目标）：通常用来命名回调队列。

correlation_id（关联标识）：用来将RPC的响应和请求关联起来。
```

(2) 服务器端接收到消息并处理

(3) 服务器端处理完消息后，将生成的应答消息到reply_to指定的队列中，同时带上correlation_id属性

(4) 客户端之前已经订阅replyto指定的queue，从中接收到服务器的应答消息后，根据其中的属性correlation_id分析哪条请求被执行了，根据执行结果进行后续业务处理。


## 7.使用mq实现RPC的意义

通过 MQ 实现 RPC 看起来比客户端和服务器直接通讯要复杂一些，那我们为什么要这样做呢？或者说这样做有什么好处：

- **将客户端和服务器解耦：**客户端只是发布一个请求到 MQ 并消费这个请求的响应。并不关心具体由谁来处理这个请求。
- **减轻服务器的压力：**传统的 RPC 模式中如果客户端和请求过多，服务器的压力会过大。由 MQ 作为中间件的话，过多的请求而是被 MQ 消化掉，服务器可以控制消费请求的频次，并不会影响到服务器。
- **服务器的横向扩展更加容易：**如果服务器的处理能力不能满足请求的频次，只需要增加服务器来消费 MQ 的消息即可，MQ会帮我们实现消息消费的负载均衡。

## 8.服务注册中心 好处

在传统应用组件间调用，通过接口规范约束来实现的，从而实现不同模块间良好协作；但是被拆分成微服务后，每个微服务实例的数量和网络地址都可能动态变化，使得原来硬编码的地址极不方便，故需要一个中心化的组件来进行服务的登记和管理。

**服务注册中心**：实现服务治理，管理所有的服务信息和状态。

**注册中心好处**：不用关心服务提供方数量、地址等细节。

**注册中心技术栈**：Eureka、Nacos、Consul、Zookeeper等。

**服务注册与发现包括两部分**：一个是服务器端，另一个是客户端

Server是一个公共服务，为Client提供服务注册和发现的功能，维护注册到自身的Client的相关信息，同时提供接口给Client获取注册表中其他服务的信息，使得动态变化的Client能够进行服务间的相互调用。

Client将自己的服务信息通过一定的方式登记到Server上，并在正常范围内维护自己信息一致性，方便其他服务发现自己，同时可以通过Server获取到自己依赖的其他服务信息，完成服务调用，还内置了负载均衡器，用来进行基本的负载均衡。

Spring Cloud以Eureka作为服务注册中心，是一个RESTful风格服务，是服务注册和发现的基础组件，它屏蔽了Server和Client的交互细节，使得开发者将精力放在业务上。

## 一个分布式部署的项目?至少需要哪些模块?

## 任何一个流量打过来都会打到注册中心么?

## 一个注册中心,至少需要具备哪些条件?

## 有一大批流量总是被打到一个实例上面,这个实例的兄弟实例分到的流量很少,怎么办?

## 有一个实例挂了怎么办?

## 注册中心集群的时候,其中一个注册中心挂了怎么办?

## 垃圾回收算法？各有什么优缺点

## 

# 分布式事务

## 2PC

### 1.何谓分布式事务

#### 1.1 单体应用

首先，来看下传统的单体应用。下图是一个单体应用的 3 个 模块，在同一个数据源上更新数据来完成一项业务，整个过程的数据一致性可以由数据库的本地事务来保证，如下图：

<img src="picture/分布式.assets/20200409164719500.png" alt="img" style="zoom:67%;" />

#### 1.2 分布式应用

随着业务需求和架构的变化，单体应用进行了服务化拆分：原来的 3 个 模块被拆分为 3 个独立的服务，每个服务使用独立的数据源（Pattern: Database per service）。整个业务过程将由 3 个服务的调用来完成，如下图：

<img src="picture/分布式.assets/20200409164719909.png" alt="img" style="zoom:67%;" />

此时，每个服务自身的数据一致性仍有本地事务来保证，但是整个业务层面的全局数据一致性要如何保障呢？比如订单服务和账户服务，都有各自的数据库，必须保证操作的一致性，不能出现下单成功但是没记账的情况。这就是分布式系统所面临的典型分布式事务需求：

分布式系统需要一个解决方案来保障对所有节点操作的数据一致性，这些操作组成一个分布式事务，要么全部执行，要么全部不执行。

### 2.二阶段协议详解

二阶段提交（2PC， two-phase commit protocol），顾名思义，就是通过二阶段的协商来完成一个提交操作。

2PC 最早是用来实现数据库的分布式事务的，不过现在最常用的协议是 XA 协议。这个协议是 X/Open 国际联盟基于二阶段提交协议提出的，也叫作 X/Open Distributed Transaction Processing（DTP）模型，比如 MySQL 就是通过 MySQL XA 实现了分布式事务。

2PC分为两个阶段：投票阶段和提交阶段，我们来详细看下。

#### 2.1 事务过程

二阶段提交协议，包含两类节点：

- 一个中心化协调者节点（coordinator），一般也叫做[事务协调者]()
- 多个参与者节点（participant、cohort），一般也叫做[事务参与者]()

二阶段提交协议的每一次事务提交过程如下：

##### 投票阶段（commit-request phase / voting phase）

1. 事务协调者请所有事务参与者进行投票：是否可以提交事务，然后等待所有参与者的投票结果；
2. 参与者如果投票表示可以提交事务，那么就必须预留本地资源（执行本地事务），然后响应YES，后续也不再允许放弃事务；如果不能，就返回NO响应；
3. 如果协调者接受某个参与者的响应超时，它会认为该参与者投票为NO，即预留资源失败。

<img src="picture/分布式.assets/20200409164720487.png" alt="img" style="zoom: 67%;" />

##### 提交阶段（commit phase）

在该阶段，事务协调者将基于投票阶段的投票结果进行决策：提交或取消各参与者的本地事务

1. 仅当所有参与者都返回 YES 响应时，协调者才向所有参与者发出提交请求，此时所有参与者必须保证提交事务成功；
2. 如果投票阶段中任意一个参与者返回 No 响应，则协调者向所有参与者发出回滚请求，所有参与者进行回滚操作。

两阶段提交协议成功场景示意图：

![img](picture/分布式.assets/20200409164720938.png)

> 2PC假设所有节点都采用预写式日志（Write-Ahead Logging）来写数据，且日志写入后不会丢失。[**WAL 的核心思想就是先写日志，再写数据。**(binlog是提交任务的时候才会去写)]()

#### 2.2 优缺点

优点：

1. 强一致性，因为一阶段预留了资源，所以只要节点或者网络最终恢复正常，协议就能保证二阶段执行成功；
2. 业界标准支持，二阶段协议在业界有标准规范——XA 规范，许多数据库和框架都有针对XA规范的分布式事务实现。

缺点：

1. 在提交请求阶段，需要预留资源，在资源预留期间，[**其他人不能操作（比如，XA 在第一阶段会将相关资源锁定） ，会造成分布式系统吞吐量大幅下降；**]()
2. 容错能力较差，比如在节点宕机或者超时的情况下，无法确定流程的状态，只能不断重试，同时这也会导致事务在访问共享资源时发生冲突和死锁的概率增高，随着数据库节点的增多，这种趋势会越来越严重，从而成为系统在数据库层面上水平伸缩的"枷锁"；

2PC分布式事务方案，比较适合单体应用跨多库的场景，一般用spring + JTA就可以实现。但是因为[**严重依赖于数据库层面来搞定复杂的事务，效率很低，所以绝对不适合高并发的场景。**]()

> 注意：一般来说，如果某个服务内部出现跨多库的直接操作，其实是不合规的。 按照分布式服务治理的规范，一个分布式系统，拆成几十个服务，每个服务只能操作自己对应的一个数据库，如果需要操作别的服务对应的库，不允许直连库，必须通过调用别的服务的接口来实现。

### 3.总结

二阶段提交协议，虽然是目前分布式事务的事实规范，但实际应用并不多。不过2PC是一种非常经典的思想，Paxos、Raft 等强一致性算法，都采用了二阶段提交操作。所以，读者应当理解该协议背后的二阶段提交的思想，当后续需要时，能灵活地根据二阶段提交思想，设计新的事务或一致性协议。

## 3PC

### 1.简介

在二阶段协议中，事务参与者在投票阶段，如果同意提交事务，则会锁定资源，此时任何其他访问该资源的请求将处于阻塞状态。

正因为这个原因，三阶段协议（Three-phase commit protocol, 3PC）对二阶段协议进行了改进：

- 一方面引入超时机制，解决资源阻塞问题；
- 另一方面[**新增一个询问阶段**]()（CanCommit），提前确认下各个参与者的状态是否正常。


![img](picture/分布式.assets/20200409164814824.png)

### 2.协议详解

我们先来看下三阶段提交协议的成功场景：

![img](picture/分布式.assets/20200409164815393.png)

#### 2.1 询问阶段（CanCommit）

询问阶段，事务协调者向事务参与者发送 CanCommit 请求，参与者如果可以提交就返回 Yes 响应，否则返回 No 响应。这样的话，[**询问阶段就可以确保尽早的发现无法执行操作的参与者节点，提升效率**]()。该阶段参与者也不会取锁定资源。

1. 事务协调者发送事务询问指令（canCommit），询问事务参与者是否可以提交事务；
2. 参与者如果可以提交就返回 Yes 响应，否则返回 No 响应，不需要做真正的操作。

> 对于事务协调者，如果询问阶段有任一参与者返回NO或超时，则协调者向所有参与者发送abort指令。
> 对于返回NO的参与者，如果在指定时间内无法收到协调者的abort指令，则自动中止事务。

#### 2.2 准备阶段（PreCommit）

事务协调者根据事务参与者在询问阶段的响应，判断是执行事务还是中断事务：

1. 如果询问阶段所有参与者都返回YES，则协调者向参与者们发送预执行指令（preCommit），参与者接受到preCommit指令后，写redo和undo日志，执行事务操作，占用资源，但是不会提交事务；
2. 参与者响应事务操作结果，并等待最终指令：提交（doCommit）或中止（abort）。

#### 2.3 提交阶段（DoCommit）

1. 如果每个参与者在准备阶段都返回ACK确认（即事务执行成功），则协调者向参与者发起提交指令（doCommit），参与者收到指令后提交事务，并释放锁定的资源，最后响应ACK；
2. 如果任意一个参与者在准备阶段返回NO（即执行事务操作失败），或者协调者在指定时间没收到全部的ACK响应，就会发起中止（abort）指令，参与者取消已经变更的事务，执行undo日志，释放锁定的资源。

> 当参与者响应ACK后，即使在指定时间内没收到doCommit指令，也会进行事务的最终提交；
> 一旦进入提交阶段，即使因为网络原因导致参与者无法收到协调者的doCommit或Abort请求，超时时间一过，参与者也会自动完成事务的提交。

### 3.优缺点

优点：

1. 增加了一个询问阶段，询问阶段可以确保尽早的发现无法执行操作的参与者节点，提升效率；
2. 在准备阶段成功以后，协调者和参与者执行的任务中都增加了超时，一旦超时，参与者都会继续提交事务，默认为成功，降低了阻塞范围。

缺点：

1. [**如果准备阶段执行事务后，某些参与者反馈执行事务失败，但是由于出现网络分区，导致这些参与者无法收到协调者的中止请求，那么由于超时机制，这些参与者仍会提交事务，导致出现不一致；**]()
2. 性能瓶颈，不适合高并发场景。

所以无论是 2PC 还是 3PC，当出现网络分区且不能及时恢复时， 都不能保证分布式系统中的数据 100% 一致。

### 4.总结

三阶段提交协议，虽然针对二阶段提交协议的“协调者故障，参与者长期锁定资源”的痛点，通过[**引入了询问阶段和超时机制**]()，来减少资源被长时间锁定的情况，但这也会导致集群各节点在正常运行的情况下，使用更多的消息进行协商，增加系统负载和响应延迟。也正是因为这些问题，三阶段提交协议很少被使用。

## TCC

### 1.简介

2007年，Pat Helland发表了一篇名为[《Life beyond Distributed Transactions: an Apostate’s Opinion》](http://adrianmarriott.net/logosroot/papers/LifeBeyondTxns.pdf)的论文，提出了TCC（Try-Confirm-Cancel） 的概念。

[**两阶段提交（2PC）和三阶段提交（3PC）并不适用于并发量大的业务场景。**]()TCC事务机制相比于2PC、3PC，不会锁定整个资源，而是通过引入补偿机制，将资源转换为业务逻辑形式，锁的粒度变小。

TCC的核心思想是：[**针对每个操作，都要注册一个与其对应的确认和补偿（撤销）操作**]()，分为三个阶段：

- Try：这个阶段对各个服务的资源做检测以及对资源进行锁定或者预留；
- Confirm ：执行真正的业务操作，不作任何业务检查，只使用Try阶段预留的业务资源，Confirm操作要求具备幂等设计，Confirm失败后需要进行重试；
- Cancel：如果任何一个服务的业务方法执行出错，那么这里就需要进行补偿，即执行回滚操作，释放Try阶段预留的业务资源 ，Cancel操作要求具备幂等设计，Cancel失败后需要进行重试。

![img](picture/分布式.assets/20200409164914188.png)

举个例子，电商系统中有两个服务：订单服务A、库存服务B：
对外提供服务时，必须接受一些不确定性，即对服务A/B的一次调用仅是一个临时性操作，服务消费方保留了后续的取消权。
如果消费方认为全局事务应该rollback，它会要求取消之前的临时性操作；如果消费方认为全局事务应该commit时，它会进行的一个确认操作。

### 2.TCC的执行

TCC将一次事务操作分为三个阶段：Try、Confirm、Cancel，我们通过一个订单/库存的示例来理解。假设我们的分布式系统一共包含4个服务：订单服务、库存服务、积分服务、仓储服务，每个服务有自己的数据库，如下图：

<img src="picture/分布式.assets/20200409164914805.png" alt="img" style="zoom:50%;" />

#### 2.1 Try

Try阶段一般用于锁定某个资源，设置一个预备状态或冻结部分数据。对于示例中的每一个服务，Try阶段所做的工作如下：**[（也就是创建这些具体的数据列，并且设置中间状态）]()**

- 订单服务：先置一个中间状态“UPDATING”，而不是直接设置“支付成功”状态；
- 库存服务：先用一个冻结库存字段保存冻结库存数，而不是直接扣掉库存；
- 积分服务：预增加会员积分；
- 仓储服务：创建销售出库单，但状态是UNKONWN。


![img](picture/分布式.assets/20200409164915615.png)

#### 2.2 Confirm

根据Try阶段的执行情况，Confirm分为两种情况：

1. 理想情况下，所有Try全部执行成功，则执行各个服务的Confirm逻辑；
2. 部分服务Try执行失败，则执行第三阶段——Cancel。

Confirm阶段一般需要各个服务自己实现Confirm逻辑：

- 订单服务：confirm逻辑可以是将订单的[**中间状态**]()变更为PAYED-支付成功；
- 库存服务：将冻结库存数清零，同时扣减掉真正的库存；
- 积分服务：将预增加积分清零，同时增加真实会员积分；
- 仓储服务：修改销售出库单的状态为已创建-CREATED。


![img](picture/分布式.assets/20200409164916872.png)

> Confirm阶段的各个服务本身可能出现问题，这时候一般就需要TCC框架了（比如ByteTCC，tcc-transaction，himly），[**TCC事务框架一般会记录一些分布式事务的活动日志，保存事务运行的各个阶段和状态，从而保证整个分布式事务的最终一致性。**]()

#### 2.3 Cancel

如果Try阶段执行异常，就会执行Cancel阶段。比如：对于订单服务，可以实现的一种Cancel逻辑就是：

将订单的状态设置为“CANCELED”；

对于库存服务，Cancel逻辑就是：将冻结库存扣减掉，加回到可销售库存里去。


![img](picture/分布式.assets/20200409164918882.png)

> 许多公司为了简化TCC的使用，通常会将一个服务的某个核心接口拆成两个，比如库存服务的扣减库存接口，拆成两个子接口：①扣减接口 ②回滚扣减库存接口，由TCC框架来保证当某个接口执行失败后去执行对应的rollback接口。

### 3.总结

从正常的流程上讲，TCC仍然是一个两阶段提交协议。但是，在执行出现问题的时候，有一定的自我修复能力，如果任何一个事务参与者出现了问题，[**协调者可以通过执行逆操作来取消之前的操作，达到最终的一致状态**]()（比如冲正交易、查询交易）。

从TCC的执行流程也可以看出，服务提供方需要提供额外的补偿逻辑，那么原来一个服务接口，引入TCC后可能要改造成3种逻辑：

- Try：先是服务调用链路依次执行Try逻辑；
- Confirm：如果都正常的话，TCC分布式事务框架推进执行Confirm逻辑，完成整个事务；
- Cancel：如果某个服务的Try逻辑有问题，TCC分布式事务框架感知到之后就会推进执行各个服务的Cancel逻辑，撤销之前执行的各种操作。

> 注意：在设计TCC事务时，[**接口的Cancel和Confirm操作都必须满足幂等设计。**]()

#### 3.1 框架选型

TCC框架的可供选择余地比较少，目前相对比较成熟的是阿里开源的[分布式事务框架seata](https://github.com/seata/seata)(Seata并不完全是一个TCC事务框架)，这个框架是经历过阿里生产环境的大量考验，同时也支持dubbo、spring cloud。

#### 3.2 优点

跟2PC比起来，实现以及流程相对简单了一些，但数据的一致性比2PC也要差一些，当然性能也可以得到提升。

#### 3.3 缺点

[**TCC模型对业务的侵入性太强**]()，事务回滚实际上就是自己写业务代码来进行回滚和补偿，改造的难度大。一般来说支付、交易等核心业务场景，可能会用TCC来严格保证分布式事务的一致性，要么全部成功，要么全部自动回滚。这些业务场景都是整个公司的核心业务有，比如银行核心主机的账务系统，不容半点差池。

但是，在一般的业务场景下，尽量别没事就用TCC作为分布式事务的解决方案，因为自己手写回滚/补偿逻辑，会造成业务代码臃肿且很难维护。

## 可靠消息最终一致性方案

### 1.简介

本章，我们将要介绍一种生产上最常用的分布式事务解决方案——可靠消息最终一致性方案。所谓可靠消息最终一致性方案，其实就是在分布式系统当中，把一个业务操作转换成一个消息，然后利用消息来实现事务的最终一致性。

> 比如从A账户向B账户转账的操作，当服务A从A账户扣除完金额后，通过消息中间件向服务B发一个消息，服务B收到这条消息后，进行B账户的金额增加操作。

可靠消息最终一致性方案一般有两种实现方式，原理其实是一样的：

- 基于本地消息表
- 基于支持分布式事务的消息中间件，如RocketMQ等

### 2.本地消息表

基于本地消息表的分布式事务，是最简便的实现方式，其核心思想是将分布式事务拆分成本地事务进行处理，这种思路是来源于eBay。

我们来看下面这张图，基于本地消息服务的分布式事务分为三大部分：

- 可靠消息服务：存储消息，因为通常通过数据库存储，所以也叫本地消息表
- 生产者（上游服务）：生产者是接口的调用方，生产消息
- 消费者（下游服务）：消费者是接口的服务方，消费消息


![img](picture/分布式.assets/20200409165036648.png)

#### 2.1 可靠消息服务

可靠消息服务就是一个单独的服务，有自己的数据库，[**其主要作用就是存储消息（包含接口调用信息，全局唯一的消息编号）**]()，消息通常包含以下状态：

- 待确认：上游服务发送待确认消息
- 已发送：上游服务发送确认消息
- 已取消（终态）：上游服务发送取消消息
- 已完成（终态）：下游服务确认接口执行完成

#### 2.2 生产者

服务调用方（消息生产者）需要调用下游接口时，不直接通过RPC之类的方式调用，而是先生成一条消息，其主要步骤如下：

1. 生产者调用接口前，先发送一条待确认消息（一般称为half-msg，包含接口调用信息）给可靠消息服务，可靠消息服务会将这条记录存储到自己的数据库（或本地磁盘），状态为【待确认】；
2. 生产者执行本地事务，本地事务执行成功并提交后，向可靠消息服务发送一条确认消息；如果本地执行失败，则向消息服务发送一条取消消息；
3. 可靠消息服务如果收到消息后，修改本地数据库中的那条消息记录的状态改为【已发送】或【已取消】。如果是确认消息，则将消息投递到MQ消息队列；（[**修改消息状态和投递MQ必须在一个事务里，保证要么都成功要么都失败**]()）

> 为了防止出现：生产者的本地事务执行成功，但是发送确认/取消消息超时的情况。可靠消息服务里一般会提供一个后台定时任务，不停的检查消息表中那些【待确认】的消息，然后回调生产者（上游服务）的一个接口，由生产者确认到底是取消这条消息，还是确认并发送这条消息。


![img](picture/分布式.assets/20200409165037610.png)

通过上面这套机制，可以保证生产者对消息的100%可靠投递。

#### 2.3 消费者

服务提供方（消息消费者），从MQ消费消息，然后执行本地事务。执行成功后，反过来通知可靠消息服务，说自己处理成功了，然后可靠消息服务就会把本地消息表中的消息状态置为最终状态【已完成】 。

这里要注意两种情况：

1. 消费者消费消息失败，或者消费成功但执行本地事务失败。
   针对这种情况，可靠消息服务可以提供一个后台定时任务，不停的检查消息表中那些【已发送】但始终没有变成【已完成】的消息，然后再次投递到MQ，让下游服务来再次处理。也可以引入zookeeper，由消费者通知zookeeper，生产者监听到zookeeper上节点变化后，进行消息的重新投递。
2. 如果消息重复投递，消息者的接口逻辑需要实现幂等性，保证多次处理一个消息不会插入重复数据或造成业务数据混乱。
   针对这种情况，消费者可以准备一张消息表，用于判重。消费者消费消息后，需要去本地消息表查看这条消息有没处理成功，如果处理成功直接返回成功。


![img](picture/分布式.assets/20200409165039757.png)

#### 2.4 总结

这个方案的优点是简单，但最大的问题在于可靠消息服务是严重依赖于数据库的，即通过数据库的消息表来管理事务，不太适合并发量很高的场景。

### 3.分布式消息中间件

许多开源的消息中间件都支持分布式事务，比如RocketMQ、Kafka。其思想几乎是和本地消息表/服务实一样的，只不过是将可靠消息服务和MQ功能封装在一起，屏蔽了底层细节，从而更方便用户的使用。这种方案有时也叫做可靠消息最终一致性方案。

以RocketMQ为例，消息的发送分成2个阶段：Prepare阶段和确认阶段。


![img](picture/分布式.assets/20200409165041821.png)

#### 3.1 prepare阶段

1. 生产者发送一个不完整的事务消息——HalfMsg到消息中间件，消息中间件会为这个HalfMsg生成一个全局唯一标识，生产者可以持有标识，以便下一阶段找到这个HalfMsg；
2. 生产者执行本地事务。

> 注意：消费者无法立刻消费HalfMsg，生产者可以对HalfMsg进行Commit或者Rollback来终结事务。只有当Commit了HalfMsg后，消费者才能消费到这条消息。

#### 3.2 确认阶段

1. 如果生产者执行本地事务成功，就向消息中间件发送一个Commit消息（包含之前HalfMsg的唯一标识），中间件修改HalfMsg的状态为【已提交】，然后通知消费者执行事务；
2. 如果生产者执行本地事务失败，就向消息中间件发送一个Rollback消息（包含之前HalfMsg的唯一标识），中间件修改HalfMsg的状态为【已取消】。

> 消息中间件会定期去向生产者询问，是否可以Commit或者Rollback那些由于错误没有被终结的HalfMsg，以此来结束它们的生命周期，以达成事务最终的一致。之所以需要这个询问机制，是因为生产者可能提交完本地事务，还没来得及对HalfMsg进行Commit或者Rollback，就挂掉了，这样就会处于一种不一致状态。

#### 3.3 ACK机制

消费者消费完消息后，可能因为自身异常，导致业务执行失败，此时就必须要能够重复消费消息。RocketMQ提供了ACK机制，即RocketMQ只有收到服务消费者的ack message后才认为消费成功。

所以，服务消费者可以在自身业务员逻辑执行成功后，向RocketMQ发送ack message，保证消费逻辑执行成功。

### 4.示例

我们最后以一个电子商务支付系统的核心交易链路为示例，来更好的理解下可靠消息最终一致性方案。

#### 4.1 交易链路

假设我们的系统的核心交易链路如下图。用户支付订单时，首先调用订单服务的对外接口服务，然后开始核心交易链路的调用，依次经过订单业务服务、库存服务、积分服务，全部成功后再通过MQ异步调用仓储服务：


![img](picture/分布式.assets/20200409165043203.png)

上图中，订单业务服务、库存服务、积分服务都是同步调用的，由于是核心链路，我们可以通过上一章中讲解的[TCC分布式事务](https://www.tpvlog.com/article/69)来保证分布式事务的一致性。而调用仓储服务可以异步执行，所以我们依赖RocketMQ来实现分布式事务。

#### 4.2 事务执行

接着，我们来看下引入RocketMQ来实现分布式事务后，整个系统的业务执行流程发生了哪些变化，整个流程如下图：


![img](picture/分布式.assets/20200409165044829.png)

1. 当用户针对订单发起支付时，首先订单接口服务先发送一个half-msg消息给RocketMQ，收到RocketMQ的成功响应（注意，此时仓储服务还不能消费消息，因为half-msg还没有确认）。
2. 然后，订单接口服务调用核心交易链路，如果其中任一服务执行失败，则先执行内部的TCC事务回滚；
3. 如果订单接口服务收到链路失败的响应，则向MQ投递一个rollback消息，取消之前的half-msg；
4. 如果订单接口服务收到链路成功的响应，则向MQ投递一个commit消息，确认之前的half-msg，那仓库服务就可以消费消息；
5. 仓储服务消费消息成功并执行完自身的逻辑后，会向RocketMQ投递一个ack message，以确保消费成功。

> 注意，如果因为网络原因，导致RocketMQ始终没有收到订单接口服务对half-msg的commit或rollback消息，RocketMQ就会回调订单接口服务的某个接口，以查询该half-msg究竟是进行commit还是rollback。

### 5.总结

[**可靠消息最终一致性方案，一般适用于异步的服务调用**]()，比如支付成功后，调用积分服务进行积分累加、调用库存服务进行发货等等。总结一下，可靠消息最终一致性方案其实最基本的思想就两点：

1. [**通过引入消息中间件，保证生产者对消息的100%可靠投递；**]()
2. 通过引入Zookeeper，保证消费者能够对未成功消费的消息进行重新消费（消费者要保证自身接口的幂等性）。

#### 5.1 优缺点

可靠消息最终一致性方案是目前业务主流的分布式事务落地方案，其优缺点主要如下：

优点：

> 消息数据独立存储，降低业务系统与消息系统间的耦合。

缺点：

> 一次消息发送需要两次请求，业务服务需要提供消息状态查询的回调接口。

一般来讲，99%的分布式接口调用不需要做分布式事务，通过监控（邮件、短信告警）、记录日志，就可以事后快速定位问题，然后就是排查、出解决方案、修复数据。

因为用分布式事务一定是有成本的，而且这个成本会比较高，特别是对于一些中小型公司。同时，引入分布式事务后，代码复杂度、开发周期会大幅上升，系统性能和吞吐量会大幅下跌，这就导致系统更加更加脆弱，更容易出bug。当然，如果有资源能够持续投入，分布式事务做好了的话，好处就是可以100%保证数据一致性不会出错。

# 可扩展：服务化拆分

集中式应用的分布式改造必然伴随着拆分，拆分的方式有很多种，从最早的N层架构到SOA，再到现在流行的微服务。拆分的重要目的之一就是增强系统的可扩展性，本文将段对最常见的三种应用拆分方式进行介绍。

## 一、分层架构

应用为了实现[**“高内聚、低耦合”**]()的设计目标，往往会进行拆分，一种最常见的拆分方式就是“分层”。“分层架构”也叫做“N层架构”，通常情况下，N至少是2层。基于划分维度的不同，“分层架构”又可以区分为：B/S和C/S架构、MVC和MVP架构、逻辑分层架构等。比较典型的是逻辑分层架构，如下图是J2EE应用的常见分层架构：


![img](picture/分布式.assets/20200409165610161.png)

逻辑分层架构最核心的一点是需要保证各层之间的差异足够清晰，边界足够明显，[**其本质是隔离关注点（separation of concerns），即每一层的组件只处理本层的逻辑。**]()通常来说，分层架构的主要关注点在于系统内部的拆分，而对于分布式的应用，我们更多关注的是系统间的拆分，与此对应的是两种主要的可扩展架构模式：[**SOA和微服务**]()。

## 二、SOA（关注点在适配）

SOA的全称是Service Oriented Architecture——面向服务的架构，诞生于20世纪90年代。SOA在很多传统企业，特别是金融业、通信业落地和推广得比较成功，在互联网行业并没有大规模地实践和推广。这与传统企业的特点有关：

- 企业各部门有独立的IT系统，每个IT系统可能采购于不同的供应商，实现技术不同，采购方也不太可能完全对这些系统进行重构；
- 随着业务的发展，业务复杂越来越高，许多流程和业务需要多个IT系统共同配合完成。而原本这些相互独立的IT系统并没有统一的对外接口，每次开发新的流程和业务，都需要协调大量的IT系统，同时定制开发，效率很低。

为了应对上述问题，SOA提出了3个关键概念：服务、ESB、松耦合。

### 2.1 服务

[**SOA认为，所有业务功能都是一项服务，对外提供开放接口，当其他系统需要使用这项功能时，无须定制化开发，比如SOA服务化改造后的应用一般都会对外提供web service接口。**]()

### 2.2 ESB

ESB的全称是 Enterprise Service Bus，即企业服务总线。ESB将企业中各个不同的服务连接在一起。因为各个独立的服务往往是异构的，如果没有统一的标准，则各个异构系统对外提供的接口是各式各样的。而ESB的作用就是屏蔽异构系统对外提供各种不同的接口方式，以此来达到服务间高效的互联调用。


![img](picture/分布式.assets/20200409165610640.jpg)

### 2.3 松耦合

松耦合的目的是减少服务间的依赖和影响。因为采用SOA架构后，各个服务是独立部署运行的，甚至不清楚某个服务到底有多少对其他服务的依赖。如果做不到松耦合，某个服务一升级，依赖它的服务就可能全部故障。但实际上要真正做到松耦合并没有那么容易，是一项复杂的工作。

### 2.4 优缺点

[**SOA解决的是资源的重复利用问题，它的拆分粒度通常比较大。**]()ESB需要实现各种系统间的协议转换、数据转换、透明的动态路由等功能，工作量和复杂度都很大，而且这种转换很消耗计算资源，当ESB承载的消息很多时，ESB本身会成为整个系统的性能瓶颈。

> SOA是在各种异构系统存在多年的背景下产生的，其主要目的是去适配已经存在的各种异构系统。

## 三、微服务（Microservice）

微服务是近几年非常火的架构设计理念，Martin Fowler对微服务进行过[系统性的阐述](https://martinfowler.com/articles/microservices.html)，将这一架构设计理念推向了高潮，其本质就是将原来单体应用内的各个模块拆分成独立的服务（子系统），如下图：


![img](picture/分布式.assets/20200409165611084.jpg)

由于微服务中也包含了“服务”的概念，而SOA中也有“服务“的概念，我们自然而然就会想：微服务与SOA有什么关系？为了有了SOA还要提出微服务？

### 3.1 微服务与SOA的关系

#### 服务粒度

整体上来说，SOA的服务粒度要更粗一些，而微服务的服务粒度更细一些。例如，对一个大型企业来说，“客户关系管理系统（CBMS）”就是SOA架构中的一个服务；而如果采用微服务架构，则CBMS会被拆分为更多的服务。

#### 服务通信

SOA采用了ESB作为服务间通信的关键组件，负责服务定义、服务路由、消息转换、消息传递，总体上是重量级的实现。而[**微服务推荐统一的协议和格式，例如，RESTful协议、RPC协议**]()。

#### 服务交付

SOA对服务的交付没有特殊要求，因为[**SOA更多考虑的是兼容已有的系统**]()；微服务提倡“快速交付“，相应地要求[**采取自动化测试、持续集成、自动化部署**]()等敏捷开发相关的最佳实践。如果没有这些基础能力的支持，微服务规模一旦变大，部署运维成本呈成指数上升。

#### 应用场景

SOA更加适合庞大、复杂、异构的企业级系统，这也是SOA诞生的背景。这类系统典型的特征是很多系统已经发展多年，采用不同的企业级开发技术，有的自研究，有的采购，很难完全推倒重来或进行大规模的优化和重构。因为成本和影响太大，只能采用兼容的方式进行处理，而承担兼容任务的就是ESB。

微服务更加适合于快速、轻量级的互联网应用，因为互联网企业大多没有历史包袱，能够接受快速尝试。Martin Fowler对微服务架构的特点作了比较精准的描述：

> In short, the microservice architectural style is an approach to developing a single application as a suite of small services, each running in its own process and communicating with lightweight mechanisms, often an HTTP resource API. These services are built around business capabilities and independently deployable by fully automated deployment machinery. There is a bare minimum of centralized management of these services, which may be written in different programming languages and use different data storage technologies.


![img](picture/分布式.assets/20200409165611688.jpg)

### 3.2 微服务的陷阱

单纯从上面的对比来看，似乎微服务大大优于SOA，这也导致了很多团队不假思索地就开始使用微服务架构。我们来看下微服务具体有哪些坑：

#### 服务划分过细，服务间关系复杂

[**服务划分过细，单个服务的复杂度确实下降了，但整个系统的复杂度却上升了，因为微服务将系统内的复杂度转移为系统间的复杂度了。**]()从理论的角度来计算，n个服务的复杂度是n*(n-1)/2，整体系统的复杂度是随着微服务数量的增加呈指数级增加的。

#### 服务数量太多，团队效率急剧下降

- 开发工程师要设计多个接口，打开多个工程，调试时要部署多个程序，提测时要打多个包；
- 测试工程师要部署多个环境，准备多个微服务的数据，测试多个接口；
- 运维工程师每次上线都要操作多个微服务，并且微服务之间可能还有依赖关系。

#### 调用链太长，[**性能下降**]()

由于微服务之间通过HTTP或RPC调用，每次调用必须经过网络，假设一个接口正常平均相应时间50ms，经过6次调用性能消耗将达到300ms。

#### 调用链太长，[**问题定位困难**]()

系统拆分为微服务后，一次用户请求需要多个微服务协同处理，任意服务的故障都可能导致整个业务的失败。由于微服务数量众多，且故障存在扩散现象，快速定位到底是哪个服务故障是一件困难的事情。

#### 没有自动化支撑，无法快速交付

如果没有相应的自动化系统进行支撑，都靠人工去操作，那么微服务不但达不到快速交付的目的甚至还不如一个大而全的系统效率高，没有自动化支撑将出现以下问题：

- 没有自动化测试支撑，每次测试都需要测试大量接口
- 没有自动化部署支撑，每次部署6-7个服务，几十台机器，只能通过shell人肉运维
- 没有自动化监控支撑，每次故障定位需要人工查几十台服务器甚至几百个微服务的各种日志文件

#### 没有服务治理，难以统一管理大量微服务

随着微服务种类和数量越来越多，如果没有服务治理系统进行支撑，将会产生以下问题：

- 服务路由：假设某个微服务有60个节点，分散在20台机器上，那么其它依赖该服务的微服务如何知道其部署情况？
- 服务故障隔离：假设上述例子中的60个节点有5个节点发生了故障，依赖的微服务如何处理这种情况？
- 服务注册和发现：假设某个微服务从60个节点扩容到80个节点，或缩减到40个节点，新增或减少的节点如何让依赖的服务知道？

如果以上这些场景都依赖人工去解决，那么微服务架构反而会成为分布式系统的瓶颈，所以微服务架构要落地，最终的解决方案必须依赖自动化的服务管理系统。

### 四、总结

分布式系统要求具备可扩展性，通常的实现方式就是进行应用的拆分，而拆分往往伴随服务化改造。SOA和微服务是两种主流的可扩展架构模式。SOA是在各种异构系统存在多年的背景下产生的，其主要目的是去适配已经存在的各种异构系统。微服务本质是SOA思想的延伸或变形，微服务架构的落地依赖于各类自动化基础设施的构建，当完成整个微服务的基础设施后，你会发现其复杂度与ESB所差无几。

# 可扩展：全局流水号

## 一、简介

在数据分片中，不管是普通hash、一致性hash还是range based，都要基于某个key进行hash运算，然后根据计算值进行分片。

Key一般采用基于记录的特征值，这个特征值在不同的框架中有不同的叫法，比如MongoDB中的[sharding key](https://docs.mongodb.com/manual/core/sharding-shard-key/) ，Oracle中的[Partition Key](https://docs.oracle.com/cd/B28359_01/server.111/b32024/partition.htm)。一般来讲，特征值的选取应当具有区分度。比如，在分布式调度系统中，通常会为每个子任务生成一个全局唯一的流水号，由任务控制者通过对流水号进行hash运算，将其分配给Job Executor（负责子任务的执行）。

对于流水号的生成，读者肯定不会陌生，比如数据库的自增主键、uuid等都是生成流水号的方式。但是数据库自增主键只能保证单个DB实例内的单表唯一，uuid则利用了mac地址，缺少规律、可读性及安全性。我们希望的流水号应该满足以下特性：

- 分布式系统内全局唯一
- 趋势递增

## 二、数据库ID表

基于数据库ID表，生成全局流水号，是一种比较常见的方式，以Mysql为例，可以建立一张ID表，表一共两列：id为自增主键，type则表示流水的类型，可以自定义取值，但要保证唯一：

```sql
CREATE TABLE `ID_TABLE` (
     `id` BIGINT(64) UNSIGNED NOT NULL AUTO_INCREMENT,
     `type` VARCHAR(16) NOT NULL DEFAULT '',
     PRIMARY KEY (`id`),
     UNIQUE KEY `idx_type` (`type`)
) ENGINE=MYISAM
```

每次客户端需要生成唯一流水号时，可以通过以下语句获取：

```sql
REPLACE INTO ID_TABLE('type') VALUES('ORDER_KEY');
SELECT LAST_INSERT_ID();
```

> 这里解释下REPLACE INTO：假如表中有一条记录与用PRIMARY KEY或UNIQUE KEY索引的新记录具有相同的值，则先删除旧记录，再插入新记录。

这种方式对于同一个type值，比如“ORDER_KEY”，生成的流水号都是唯一的。在分布式系统中，可以通过数据库中间件请求到不同的分库，每个分库设置不同的初始值和自增步长，以避免出现重复流水号，如下图：

![img](picture/分布式.assets/20200409165447912.png)

上述，DB1生成的ID是1,4,7,10,13....，DB2生成的ID是2,5,8,11,14.....

缺点：
显然，基于数据库表生成流水号的方式效率太低，很难满足高并发的业务场景，而且与数据库紧耦合。所以，通常只适合并发量低，但是数据量大的场景。

## 三、Snow Flake算法

### 3.1 原理

Snow Flake是Twitter开源的分布式ID生成算法，其结果是一个long型的ID，Snow Flake的核心思想是：

> 将一个long类型整数按位划分，41bit作为毫秒数，10bit作为机器的ID（5个bit是数据中心，5个bit的机器ID），12bit作为毫秒内的流水号，最后还有一个符号位，永远是0。


![img](picture/分布式.assets/20200409165448302.jpg)

算法实现基本就是二进制操作，单机每秒内理论上最多可以生成1024*(2^12)，即409.6万个ID(1024x4096=4194304)

- 1bit：未使用，因为二进制里第一个bit为如果是1，那么都是负数，但是我们生成的id都是正数，所以第一个bit统一都是0；
- 41bit：时间截，存储的是时间截的差值（当前时间截 - 开始时间截) ，开始时间截一般是我们的id生成器开始使用的时间，由我们程序来指定的。41位的时间截，可以使用69年，年T = (1L << 41) / (1000x60x60x24x365) = 69；
- 10bit：记录工作机器ID，代表的是这个服务最多可以部署在2^10（1024）台机器上，包括5位datacenterId和5位workerId。注意，10-bit可以完全表示1024台机器，如果对IDC划分有需求，可以将10-bit分5-bit给IDC，分5-bit给工作机器，这样就可以表示32个IDC，每个IDC下可以有32台机器，可以根据自身需求定义；
- 12bit：序列号，记录同一个毫秒内产生的不同id，12位的计数顺序号支持每个节点每毫秒(同一机器，同一时间截)产生4096个ID序号。

> 理论上Snow Flake方案的QPS约为409.6w/s，这种分配方式可以保证在任何一个IDC的任何一台机器在任意毫秒内生成的ID都是不同的。

### 3.2 优缺点

优点：

- 整体上按照时间自增排序，并且整个分布式系统内不会产生ID碰撞(由数据中心ID和机器ID作区分)，效率较高；
- [**不依赖数据库等第三方系统，以服务的方式部署，稳定性更高；**]()

缺点：

- 由于41位的时间戳代表的是时间差值，所以SnowFlake强依赖于机器时钟，如果机器上时钟回拨，会导致发号重复或者服务会处于不可用状态。

> 关于时钟回拨，美团开源了一个[Leaf框架](https://github.com/Meituan-Dianping/Leaf)，其中的Leaf-segment方案可以比较好的解决时钟回拨问题，但是该方案又引入了外部依赖——Zookeeper，感兴趣的读者可以自行参考。

### 3.3 代码示例

```java
public class SnowFlake {

    private long workerId;
    private long datacenterId;
    private long sequence;

    public IdWorker(long workerId, long datacenterId, long sequence) {
        // sanity check for workerId
        // 这儿不就检查了一下，要求就是你传递进来的机房id和机器id不能超过32，不能小于0
        if (workerId > maxWorkerId || workerId < 0) {
            throw new IllegalArgumentException(String.format("worker Id can't be greater than %d or less than 0", maxWorkerId));
        }
        if (datacenterId > maxDatacenterId || datacenterId < 0) {
            throw new IllegalArgumentException(String.format("datacenter Id can't be greater than %d or less than 0", maxDatacenterId));
        }
        System.out.printf("worker starting. timestamp left shift %d, datacenter id bits %d, worker id bits %d, sequence bits %d, workerid %d",
                timestampLeftShift, datacenterIdBits, workerIdBits, sequenceBits, workerId);

        this.workerId = workerId;
        this.datacenterId = datacenterId;
        this.sequence = sequence;
    }

    private long twepoch = 1288834974657L;

    private long workerIdBits = 5L;
    private long datacenterIdBits = 5L;
    private long maxWorkerId = -1L ^ (-1L << workerIdBits); // 这个是二进制运算，就是5 bit最多只能有31个数字，也就是说机器id最多只能是32以内
    private long maxDatacenterId = -1L ^ (-1L << datacenterIdBits); // 这个是一个意思，就是5 bit最多只能有31个数字，机房id最多只能是32以内
    private long sequenceBits = 12L;

    private long workerIdShift = sequenceBits;
    private long datacenterIdShift = sequenceBits + workerIdBits;
    private long timestampLeftShift = sequenceBits + workerIdBits + datacenterIdBits;
    private long sequenceMask = -1L ^ (-1L << sequenceBits);

    private long lastTimestamp = -1L;

    public long getWorkerId() {
        return workerId;
    }

    public long getDatacenterId() {
        return datacenterId;
    }

    public long getTimestamp() {
        return System.currentTimeMillis();
    }

    public synchronized long nextId() {
        // 这儿就是获取当前时间戳，单位是毫秒
        long timestamp = timeGen();

        if (timestamp < lastTimestamp) {
            System.err.printf("clock is moving backwards.  Rejecting requests until %d.", lastTimestamp);
            throw new RuntimeException(String.format("Clock moved backwards.  Refusing to generate id for %d milliseconds",
                    lastTimestamp - timestamp));
        }


        // 在同一个毫秒内，又发送了一个请求生成一个id，0 -> 1
        if (lastTimestamp == timestamp) {
            sequence = (sequence + 1) & sequenceMask; // 这个意思是说一个毫秒内最多只能有4096个数字，无论你传递多少进来，这个位运算保证始终就是在4096这个范围内，避免你自己传递个sequence超过了4096这个范围
            if (sequence == 0) {
                timestamp = tilNextMillis(lastTimestamp);
            }
        } else {
            sequence = 0;
        }

        // 这儿记录一下最近一次生成id的时间戳，单位是毫秒
        lastTimestamp = timestamp;

        // 这儿就是将时间戳左移，放到41 bit那儿；将机房id左移放到5 bit那儿；将机器id左移放到5 bit那儿；将序号放最后10 bit；
        // 最后拼接起来成一个64 bit的二进制数字，转换成10进制就是个long型
        return ((timestamp - twepoch) << timestampLeftShift) |
                (datacenterId << datacenterIdShift) |
                (workerId << workerIdShift) |
                sequence;
    }


    private long tilNextMillis(long lastTimestamp) {
        long timestamp = timeGen();
        while (timestamp <= lastTimestamp) {
            timestamp = timeGen();
        }
        return timestamp;
    }

    private long timeGen() {
        return System.currentTimeMillis();
    }

    //---------------测试---------------
    public static void main(String[] args) {
        IdWorker worker = new IdWorker(1, 1, 1);
        for (int i = 0; i < 30; i++) {
            System.out.println(worker.nextId());
        }
    }

}
```

# 高性能：读写分离

## 一、简介

对于大多数业务来说，读相关业务的操作频次要远远高于写相关的。再者，从操作系统的角度来讲，读磁盘的I/O速度也要远快于写磁盘的I/O速度。

[**所以，针对读写操作的优化也由来已久，从最常见的读写锁，再到CopyOnWrite等等。**]()

对于分布式应用，读写分离就是针对一种读/写操作的优化，其基本原理是将读/写操作分散到不同的节点上，下面是读写分离的基本架构图：

![img](picture/分布式.assets/20200409170649039.png)

### 1.1 示例

我们来通过一个示例，更好的理解下为什么要进行读写分离。

假设有一个系统A，每秒平均交易量为6000笔，其中写请求1000笔/s，读请求5000笔/s，分布式缓存承载了4000笔/s的读请求，所以最终数据库承担的读写请求总共2000笔/s：

![img](picture/分布式.assets/20200409170649681.png)

对于MySQL来说，每秒2000笔/s读写请求基本是上限了，此时响应速度会变得非常慢。所以，我们对数据库进行主从架构部署，如下图，进行一主三从的部署，读请求全部访问从库，写请求全部访问主库，数据库的压力就降下来了：

![img](picture/分布式.assets/20200409170650556.png)

## 二、实现方式

读写分离的基本实现是：

- 数据库服务器搭建主从集群（一主一从或一主多从）；
- 数据库主机负责读写操作，从机只负责读操作；
- 数据库主机通过复制将数据同步到从机（每个节点都存储了所有业务数据）；
- [**业务服务器将写操作发给数据库主机，将读操作发给数据库从机。**]()

### 2.1 复制原理

以MySQL为例，其Replication 是一个异步的复制过程，复制通过Binary Log 实现，[**整个复制过程实际上就是 Slave节点 从Master节点获取binlog日志**]()，然后再顺序执行日志中所记录的各种操作。

整个复制过程主要由三个线程来完成：

① Master端的I/O线程；

② Slave端的I/O线程；

③ Slave端的SQL线程。

整个复制步骤如下：

1. Slave的I/O线程连接上Master，并请求获取binlog日志内容；
2. Master 接收到请求后，自身负责复制的I/O 线程根据请求信息读取指定位置的日志信息，返回给Slave 端。返回信息中除了日志信息外，[**还包括Binary Log文件名称及从哪里开始复制**]()；
3. Slave 的I/O 线程接收到回复后，将接收到的日志内容[**依次写入到Relay Log 文件的最末端**]()，并将Master端的**binlog文件名和位置**记录到[**master-info 文件**]()中，这样在下一次读取的时候能够告诉Master需要哪个bin-log 的哪个位置之后的日志内容，实现[**增量复制**]()；
4. Slave 的SQL线程检测到Relay Log 中新增了内容后，会立即解析该Log文件为可执行的Query 语句，然后执行。这样就保持了Slave 端和Master 端的数据一致。

![img](picture/分布式.assets/20200409170651461.png)

### 2.2 复制延迟

**从库同步数据的过程是串行化的，也就是说主库上并行的操作，在从库上会串行执行。**所以在高并发场景下，从库的数据一定会比主库慢一些，是有延时的，经常可能出现刚写入主库的数据从库是读不到的情况，要过几十毫秒，甚至几百毫秒才能读取到，写并发越高，延迟越严重。

MySQL自5.6.x版本后，引入了一个并行复制的机制，用来解决主从同步延时问题 。所谓[**并行复制，就是从库开启多个线程，每个线程从Relay Log中读取一个库的日志进行重放，也就是说是库级别的并行，但是问题不是库级别的并行有时候作用不是很大。**]()

所以，MySQL读写分离，一般建议使用在读远多于写的场景，且对读的数据时效性要求没那么高，对于时效性要求高的数据（写了之后立马就要保证可以查到），则应该采用强制读主库的方式。

可以在从库上执行show slave status命令，查看到Seconds_Behind_Master，该字段表示slave落后master的秒数。

> 复制延迟很容易导致生产问题，17年的时候，笔者曾经负责的一个P2P系统就出现过这个问题，当时在做资金清算的时候，插入库中的一个记录在后续的一个查询中没有找到记录，导致某个融资还款项目清算中止，一开始怎么都排查不到问题所在，每隔两周左右就会出现一次这种情况，只能通过临时数据维护和重新发起清算解决。

所以，针对复制延迟问题，一般的综合解决方案如下：

1. 首先是分库，比如原来写并发2000/s，那么[**水平拆成4个主库**]()，每个主库的写并发就500/s，此时主从延迟可以忽略不计；
2. 打开MySQL支持的[**并行复制**]()功能，多个库并行复制（但是如果某个库的写入并发就是特别高，单库写并发达到了2000/s，这种情况下其实并行复制没啥作用）；
3. 对于一些关键业务，如果确实需要插入后立即查询，可以[**将读写操作全部指向主库**]()（一般通过数据库中间件做），非关键业务采用读写分离，读从库失败后再读一次主库。

### 2.3 数据丢失

所谓数据丢失，就是如果主库突然宕机，然后恰好数据还没同步到从库，那么有些数据可能在从库上是没有的，这些数据可能就丢失了。

MySQL有一个[**半同步复制（semi-sync）机制**](http://www.mstacks.com/135/1436.html#content1436)，用来解决主库数据丢失问题。

即主库写入binlog日志之后，[**会立即强制将数据同步到从库**]()，从库将日志写入自己本地的relay log之后，接着会返回一个ack给主库，[**主库接收到至少一个从库的ack之后才会认为写操作完成了**]()。

当然，缺点就是降低了吞吐量。

## 三、请求路由

在进行读写分离，还有一个关键考虑点——请求路由，就是要将读写操作区分开来，读操作路由到从库，写操作路由到主库，一般有两种方式：程序代码封装和中间件封装。

#### 程序代码封装

程序代码封装指在代码中抽象一个数据访问层，实现读写操作分离和数据库服务器连接的管理。例如，基于 Hibernate 进行简单封装，就可以实现读写分离，基本架构是：


![img](picture/分布式.assets/20200409170652092.png)

程序代码封装的方式具有以下特点：

- 实现简单，而且可以根据业务做较多定制化的功能；
- 故障情况下，如果主从发生切换，则可能需要所有系统都修改配置并重启。

目前开源的实现方案中，Sharding-JDBC 就是一个通用数据访问层，它在Java的JDBC层提供的额外服务，使用客户端直连数据库，以jar包形式提供服务，无需额外部署和依赖，可理解为增强版的JDBC驱动。Sharding-JDBC的基本架构是：


![img](picture/分布式.assets/20200409170652818.jpeg)

#### 中间件封装

中间件封装，指独立出一套系统，用于专门实现读写操作分离和数据库服务器连接的管理。中间件对业务服务器提供 SQL 兼容的协议，业务服务器无须自己实现读写分离。对于业务服务器来说，访问中间件和访问数据库没有区别，事实上在业务服务器看来，中间件就是一个数据库服务器。其基本架构是：


![img](picture/分布式.assets/20200409170653834.png)

中间件封装的方式具有以下特点：

- 支持多种编程语言，对业务服务器提供标准 SQL 接口；
- 支持完整的 SQL 语法和数据库服务器协议，实现复杂；
- 数据库中间件不执行真正的读写操作，但所有的数据库操作请求都要经过中间件，所以对中间件的性能要求也很高；
- 数据库主从切换对业务服务器无感知，数据库中间件可以探测数据库服务器的主从状态。例如，向某个测试表写入一条数据，成功的就是主机，失败的就是从机。

目前的开源数据库中间件方案中，主要有MySQL Router、Atlas、Mycat。

[**MySQL Router**]() 是MySQL 官方推荐的数据库中间件，主要功能有读写分离、故障自动切换、负载均衡、连接池等，其基本架构如下：


![img](picture/分布式.assets/20200409170654341.png)

Atlas 是奇虎 360 公司开源的数据库中间件，基于 MySQL Proxy 实现的：


![img](picture/分布式.assets/20200409170655610.png)

Mycat是目前最主流的开源数据库中间件，其基本架构如下：


![img](picture/分布式.assets/20200409170656617.png)



## 四、总结

本文以数据库为例，介绍了存储系统读写分离的基本思想。目前最主流的开源数据库中间件就是Mycat，Mycat不仅可以用作读写分离，还支持数据分片、分库分表、自动故障切换等功能，在后续章节中，我们会以Mycat为例，介绍其基本使用和原理。

# **高性能：分库分表**

## 一、简介

上一篇高性能：读写分离中，我们介绍了读写分离，其主要目的是为了分散了服务器读写操作的压力。但“读写分离”并不能分散存储压力，当数据量达到千万甚至上亿条的时候，单台服务器的存储能力会成为系统的瓶颈，主要体现在这几个方面：

- 数据量太大，读写的性能会下降，即使有索引，索引也会变得很大，性能同样会下降；
- 数据文件会变得很大，数据库备份和恢复需要耗费很长时间；
- 数据文件越大，极端情况下丢失数据的风险越高（例如，机房火灾导致数据库主备机都发生故障）。

基于上述原因，单台数据库服务器存储的数据量不能太大，需要控制在一定的范围内。为了满足业务数据存储的需求，就需要将存储分散到多台数据库服务器上。“分库分表”就是一种常见的分散存储方法，其中包括“分库”和“分表”两大类。

> 我们曾在 高可用：集群 中，从高可用的角度出发介绍过数据分片，本文则是高性能角度出发，介绍另一种针对数据库的数据分片方案。

### 1.1 示例

我们先通过一个示例来看下分库分表。假如我们现在有个系统，用户数达到1亿，每天活跃用户数上千万，每日单表数据量新增50万条，最后单库单表的数据量已经达到了600万。一般来说，Mysql单表数据量达到几百万的时候，性能就会变差，就得考虑分表了。

接下来，我们来看下如何通过水平分库+分表来解决这个问题。假设当前单库单表有600万条记录，我们先水平拆分成3个库（每个库的结构完全相同），那每个库总共包含200W条记录。
我们可以根据记录的特征值（比如订单ID）与库数进行hash取余——orderId % DB_N，那每条记录就映射到了指定的库中，比如对于orderId=29的记录，29%3=2，那就映射到数据库2：


![img](picture/分布式.assets/20200409171124015.png)

然后，在每个库内进行水平分表，拆成4个表（每个表的结构完全相同），那每张表总共包含50W条记录。
我们再根据记录的特征值与表数进行hash取余——orderId % TB_N，那每条记录就映射到了指定的表中，比如对于orderId=29的记录，29%4=1，那就映射到表1：


![img](picture/分布式.assets/20200409171124825.png)

最终，每个表的数据量从原来的单表600W缩小到50W，SQL的执行效率可以增加好几倍。同时，能够承载的QPS从原来的单库2000QPS扩展到了6000QPS。



## 二、分库

分库一般是从业务模块的角度出发，将数据分散到不同的数据库服务器。例如，一个简单的商户收单系统，包括商户、订单、渠道方三个业务模块，我们可以将商户数据、订单数据、渠道方数据分开存放到三台不同的数据库服务器上，而不是将所有数据都放在一台数据库服务器上。下图是分库的基本示意图：


![img](picture/分布式.assets/20200409171125795.png)

从上图可以看到，分库分为垂直分库和水平分库：

- 垂直分库：以表为维度，按照业务归属将不同的表拆分到不同的数据库中，所有库的并集是全量数据；一般来说，垂直拆分是在数据库模型设计的时候就做好了，并不常用。
- 水平分库：以字段为维度，按照一定策略（hash、range等），将一个库中的数据拆分到多个库中，每个库的结构都一样但数据没有交集，比如我们示例中的按照orderId进行水平分库就是这种方式。

> 水平分库的意义，就是将数据均匀放更多的库里，然后用多个库来抗更高的并发，还有就是用多个库的存储容量来进行扩容。

虽然业务分库能够分散存储和访问压力，但同时也带来了新的问题。

### 2.1 join问题

业务分库后，原本在同一个数据库中的表分散到不同数据库中，导致无法使用 SQL 的 join 查询，即“跨库join问题”。由于分库后数据分散在多个不同的数据库中，无法做跨库 join 查询，只能分多次进行多表查询，这样实现就比简单的 join 查询要复杂一些。

> 虽然很多数据库中间件，比如sharding-jdbc、Mycat都提供了跨库聚合join的功能，但考虑到性能，一般不建议使用，因为跨库join不仅会增加CPU负担并且将不同的库/表耦合在一起。需要关联数据，应当在业务层分别获取主表和扩展表的数据，然后用关联字段遍历得到全部数据。

### 2.2 事务问题

业务分库后，表分散到不同的数据库中，无法通过单个DB的事务统一修改。虽然数据库厂商提供了一些分布式事务的解决方案（例如，MySQL 的 XA），但性能实在太低，与高性能存储的目标相违背。

例如，用户下订单时需要扣减商品库存，如果订单数据和商品数据在同一个数据库中，可以简单的使用数据库事务来保证扣减商品库存和生成订单的操作要么都成功要么都失败；但分库后就无法使用单数据库的事务了，需要业务程序自己来模拟实现事务的功能。常见的分布式事务实现方案可以参考本系列的分布式理论之分布式事务。

### 2.3 成本问题

业务分库同时也带来了成本的代价，本来 1 台服务器搞定的事情，现在要 3 台，如果考虑备份，那就是 2 台变成了 6 台。

### 2.4 最佳实践

基于上述原因，对于业务量评估不是非常大、初期业务模式并不十分明确的系统，并不建议一开始就进行分库，主要有几个原因：

- 初创业务存在很大的不确定性，业务不一定能发展起来，业务开始的时候并没有真正的存储和访问压力，业务分库并不能为业务带来价值；
- 业务分库后，表之间的 join 查询、数据库事务无法简单实现，增加了额外复杂度；
- 业务分库后，因为不同的数据要读写不同的数据库，代码中需要增加根据数据类型映射到不同数据库的逻辑，增加了工作量。而业务初创期间最重要的是快速实现、快速验证，业务分库会拖慢业务节奏。

有的架构师可能会想：如果业务真的发展很快，岂不是很快就又要进行业务分库了？那为何不一开始就设计好呢？

首先，这里的“如果”事实上发生的概率比较低，做 10 个业务有1个业务能活下去就很不错了，更何况快速发展，和中彩票的概率差不多。如果业务真的发展很快，后面进行业务分库也不迟，因为只要在表结构设计环节多投入精力，做到良好的表结构设计，后面分库也并不困难。

其次，单台数据库服务器的性能其实也没有想象的那么弱，一般来说，单台Mysql数据库服务器能够支撑 10 万用户量量级的业务，初创业务从 0 发展到 10 万级用户，并不是想象得那么快。所以，初期应该[**尽量先从高可用角度去保证系统的稳定性。**]()

> 对于业务成熟的大公司来说，用户规模往往是海量的，由于已经有了业务分库的成熟解决方案，所以即使是尝试性的新业务，也最好在业务开始设计时就考虑业务分库。

## 三、分表

分库主要是从业务维度，将不同业务数据分散到不同的数据库服务器。但如果业务继续发展，同一业务的单表数据也会达到单台数据库服务器的处理瓶颈。例如，在大型的支付系统中，单日的订单规模可能会达到亿级，如果全部存放在一台数据库服务器的一张表中，肯定是无法满足性能要求的，此时就需要对单表数据进行拆分。

单表数据拆分有两种方式：垂直分表和水平分表。示意图如下：


![img](picture/分布式.assets/20200409171126300.png)

- 垂直分表：以字段为维度，将表中字段拆到不同的表（主表和扩展表）中，每个表的结构不一样；
- 水平分表：以字段为依据，按照一定策略（hash、range等），将一个表中的数据拆分到多个表中，每个表的结构都一样，我们最初的示例中就是按照orderId进行水平分表。

> 垂直分表，一般来说会将访问频率很高的字段放到一个表里去，然后将较多的访问频率很低的字段放到另外一个表里去。因为数据库是有缓存的，你访问频率高的数据行中的字段越少，就可以在缓存里缓存更多的行，性能就越好。另外，垂直拆分一般在表结构设计的时候就做好了，并不常用。

单表进行切分后，是否要将切分后的多个表分散在不同的数据库服务器中，可以根据实际的切分效果来确定，并不强制要求单表切分为多表后一定要分散到不同数据库中。原因在于：分表后即使新表在同一个数据库服务器中，也可能带来可观的性能提升，如果性能能够满足业务要求，是可以不拆分到多台数据库服务器的，毕竟分库也会引入很多复杂性；如果单表拆分为多表后，单台服务器依然无法满足性能要求，那就不得不再次进行业务分库的设计了。

分表能够有效地分散存储压力和带来性能提升，但和分库一样，也会引入各种复杂性。

### 3.1 路由问题

一般来说，单表数据量达到千万级别时，就需要进行水平分表了。水平分表后，某条数据具体属于哪个切分后的子表，需要增加路由算法进行计算，这个算法会引入一定的复杂性。常见的路由算法有：

#### 范围路由

范围路由，就是选取有序的数据列（例如，整形、时间戳等）作为路由的条件，不同分段（range）分散到不同的数据库表中，有点类似于 可扩展：Range Based。

> 以用户 ID 为例，路由算法可以按照 1000000 的范围大小进行分段，1 ~ 999999 放到数据库 1 的表中，1000000 ~ 1999999 放到数据库 2 的表中，以此类推。

复杂点：
主要体现在分段大小的选取上，分段太小会导致切分后子表数量过多，增加维护复杂度；分段太大可能会导致单表依然存在性能问题，一般建议分段大小在 100 万至 2000 万之间，具体需要根据业务选取合适的分段大小。

优点：
可以随着数据的增加平滑地扩充新的表。例如，现在的用户是 100 万，如果增加到 1000 万，只需要增加新的表就可以了，原有的数据不需要动。

缺点：
分布不均匀，假如按照 1000 万来进行分表，有可能某个分段实际存储的数据量只有 1000 条，而另外一个分段实际存储的数据量有 900 万条。

#### Hash路由

Hash路由，就是选取某个列（或者某几个列组合也可以）的值进行 Hash 运算，然后根据 Hash 结果分散到不同的数据库表中。

> 以用户 ID 为例，假如我们一开始就规划了 10 个数据库表，路由算法可以简单地用 user_id % 10 的值来表示数据所属的数据库表编号，ID 为 985 的用户放到编号为 5 的子表中，ID 为 10086 的用户放到编号为 6 的字表中。

复杂点：
主要体现在初始表数量的选取上，表数量太多维护比较麻烦，表数量太少又可能导致单表性能存在问题。而用了 Hash 路由后，增加子表数量是非常麻烦的，所有数据都要重分布。

优点：
子表分布比较均匀

缺点：
扩充新表很麻烦，所有数据都要重分布。

#### 配置路由

配置路由，就是用一张独立的路由表来记录原表到子表的路由信息。

> 以用户 ID 为例，我们新增一张 user_router 表，这个表包含 user_id 和 table_id 两列，根据 user_id 就可以查询对应的 table_id。

优点：
配置路由设计简单，使用起来非常灵活，尤其是在扩充表的时候，只需要迁移指定的数据，然后修改路由表就可以了。

缺点：
必须多查询一次，会影响整体性能；而且路由表本身如果太大（例如，几亿条数据），性能同样可能成为瓶颈，如果我们再次将路由表分库分表，则又面临一个死循环式的路由算法选择问题。

### 3.2 join问题

水平分表后，数据分散在多个表中，如果需要与其他表进行 join 查询，需要在业务代码或者数据库中间件中进行多次 join 查询，然后将结果合并。

### 3.3 count问题

水平分表后，虽然物理上数据分散到多个表中，但某些业务逻辑上还是会将这些表当作一个表来处理。例如，获取记录总数用于分页或者展示，水平分表前用一个 count() 就能完成的操作，在分表后就没那么简单了。常见的处理方式有下面两种：

#### count相加

具体做法是在业务代码或者数据库中间件中对每个子表进行 count() 操作，然后将结果相加。这种方式实现简单，缺点就是性能比较低。例如，水平分表后切分为 20 张表，则要进行 20 次 count 操作，如果串行的话，可能需要几秒钟才能得到结果。

#### 记录数表

具体做法是新建一张表，假如表名为“记录数表”，包含 table_name、row_count 两个字段，每次插入或者删除子表数据成功后，都更新“记录数表”。

优点：
性能要大大优于 count 相加的方式，因为只需要一次简单查询就可以获取数据。

缺点：
对子表的操作要同步操作“记录数表”，如果有一个业务逻辑遗漏了，数据就会不一致；且针对“记录数表”的操作和针对子表的操作无法放在同一事务中进行处理，异常的情况下会出现操作子表成功了而操作记录数表失败，同样会导致数据不一致。

> 记录数表的方式也增加了数据库的写压力，因为每次针对子表的 insert 和 delete 操作都要 update 记录数表，所以对于一些不要求记录数实时保持精确的业务，也可以通过后台定时更新记录数表。
> 定时更新实际上就是“count() 相加”和“记录数表”的结合，即定时通过 count() 相加计算表的记录数，然后更新记录数表中的数据。

### 3.3 order by问题

水平分表后，数据分散到多个子表中，排序操作无法在数据库中完成，只能由业务代码或者数据库中间件分别查询每个子表中的数据，然后汇总进行排序。

## 四、数据迁移

分库分表很多时候都是针对线上正在运行的系统，那么如何让系统从单库单表动态得切换到分库分表上？如何设计可以动态扩容缩容的分库分表方案呢？我们通过一个示例来看下整个过程。

假设已经有一个单库单表的系统在线上在跑，单表有600万数据 。我们已经设计好了一套分库分表方案——水平分3个库，每个库里水平分4个表，即每个表50万的数据量，见第一节示例。

### 4.1 停机迁移

顾名思义，就是系统停机，一般找个凌晨时间，然后用一个之前写好的一次性导数工具将单库单表的数据读出来，写到分库分表里面去。导数完了之后，修改系统的数据库连接配置啥的，一般还会包括代码和SQL的些许修改，最后进行验证。

但是这个方案仅仅适合一些夜间没有什么人使用的系统，对于一些核心支付系统来说，这种停机维护的方案几乎是不可接受的。笔者曾经就做过这种停机迁移，最后经历了停机数据迁移 -> 凌晨1:00没解决 -> 凌晨3:00没解决 -> 凌晨4:00没解决 -> 回滚 的整个过程，见了几次凌晨四五点的陆家嘴。

### 4.2 双写迁移

这个是我们常用的一种迁移方案，比较靠谱一些，不用停机，不用看上海凌晨4点的风景。

简单来说，就是改动系统代码，在所有涉及写库的地方（增删改操作），除了先对老库操作外，再加上对新库的增删改，这就是所谓双写，同时写俩库——老库和新库。


![img](picture/分布式.assets/20200409171126843.png)

系统部署之后，由于新库数据差太远，可以用一个自己写的导数工具，读取老库数据并写到新库中。写到新库时，首先判断新库中是否已有该条记录，没有则写入，[**有的话判断这条记录最后修改时间（标准规范的表结构设计中，都会包含一个最后修改时间字段）**]()，如果老库中的记录更加新，则覆盖新库中的记录。


![img](picture/分布式.assets/20200409171127740.png)

当数据迁移临时工具针对这600w记录执行完一轮后，有可能数据还是存在不一致，那么就[**再做一轮校验**]()，比对新和老库每个表的每条数据，如果有不一样的，就针对那些不一样的，从老库读数据再次判断是否更新到新库中。[**反复循环，直到两个库每个表的数据都完全一致为止。**]()（这个过程可能会持续好几天，直到某个凌晨几乎每什么新数据进来，那么一般新老库就完全一致了）

最后，当数据完全一致后，基于仅使用分库分表的最新代码重新部署系统就可以了。

## 五、动态伸缩

水平分库分表时，涉及库表的动态扩展和收缩，比如我们之前的示例中，从包含600万数据的单库单表水平拆分成3个库（每个库包含4张表，每张表50万数据）。假如此时日交易量持续上升，需要扩容成6个库，每个库需要12个表，怎么来动态增加更多库和表呢？

### 5.1 停机扩容

这个方案就跟停机迁移一样，步骤也几乎一致，唯一不同点就是自己写的导数工具的逻辑不同。但是最好别这么玩儿，有点不太靠谱，因为既然分库分表了，就说明数据量实在是太大了，可能多达数十亿条，这么玩儿很可能会出问题。

> 单库单表迁移到分库分表的时候，由于单表数据量并不是很大，最大也就一两千万 ，写个工具，多弄几台机器并行跑，1、2个小时也就导完了。 但是3个库，共12个表，数据量可能亿级，导入操作可能耗时数个小时，而且导完数据，还要进行系统部署、验证，一个通宵估计都搞不完。

### 5.2 事前规划

所谓事前规划，就是在系统设计阶段就考虑好要分库分表的数量。这也是目前最常用的一种动态扩容方案。
比如，XX银行的核心支付平台，一开始上来就是32个库，每个库32个表，共1024张表。我们来估算下，看看能支撑多大的并发量和数据量。

并发量：
假设每个库都单独部署在一台服务器上，承载1500的写并发，32个库就是32 * 1500 = 48000的写并发，前面再加一个MQ，用于削峰，假设每秒写入MQ 8万条数据，每秒消费4万条数据。接近5w的写并发，足够应付99%的公司的业务。

数据量：
假设每张表500万数据，1024张表就是50亿条数据。

我们以32个库，每个库32张表来看下具体如何扩？最初情况下，并不是每台数据库服务器上只有只有唯一一个DB实例，为了节省成本，我们可以在一台数据库服务器上部署8个DB实例，那总共需要4台数据库服务器：


![img](picture/分布式.assets/20200409171129116.png)

如果系统运行了一段时间后，需要支撑更高的并发，只需要将原来的某台数据库服务器上的DB实例迁移到新的机器上（比如扩一倍，让每台数据库服务器只有4个DB实例）：
![img](picture/分布式.assets/20200409171132027.png)

这样扩的好处是，只需要做DB实例的迁移，可以完全交给DBA去做。另外，对程序几乎是没有任何影响的，也不需要写工具去读就库里的数据导入到新库中，仅仅影响数据库服务器地址的配置。这样一直扩下去，最多可以扩到每台数据库服务器上只有一个DB实例。

数据路由的时候，一般通过全局流水号，比如orderId % 32 = 库编号，(orderId / 32） % 32 = 表编号，这样无论数据库服务器怎么伸缩，都不需要对路由规则进行改变。

## 六、数据库中间件介绍

前面几节介绍了分库分表的常见问题和方案，实际项目中，一般都会通过数据库中间件做分库分表，这里来比较下几种不同常见数据库中间件的优缺点：

| 中间件        | 介绍                                                         |
| :------------ | :----------------------------------------------------------- |
| cobar         | 阿里b2b团队开发和开源的，属于proxy层方案。早些年还可以用，但是最近几年都没更新了，基本没啥人用，差不多算是被抛弃的状态吧，而且不支持读写分离、存储过程、跨库join和分页等操作。 |
| TDDL          | 淘宝团队开发的，属于client层方案，不支持join、多表查询等语法，就是基本的crud语法是ok的，支持读写分离。目前使用的也不多，因为还依赖淘宝的diamond配置管理系统。 |
| Atlas         | 360开源的，属于proxy层方案，有一个很大的问题就是社区最新的维护都在5年前了。所以，现在用的公司基本也很少了 |
| sharding-jdbc | 当当开源的，属于client层方案。确实之前用的还比较多一些，因为SQL语法支持也比较多，没有太多限制，而且目前推出到了2.0版本，支持分库分表、读写分离、分布式id生成、柔性事务（最大努力送达型事务、TCC事务），目前社区也还一直在开发和维护，还算是比较活跃，个人认为算是一个现在也可以选择的方案 |
| Mycat         | 基于cobar改造的，属于proxy层方案，支持的功能非常完善，而且目前应该是非常火的而且不断流行的数据库中间件，社区很活跃，也有一些公司开始在用了。但是确实相比于sharding jdbc来说，年轻一些，经历的锤炼少一些 |

所以综上所述，现在其实建议考量的，就是sharding-jdbc和mycat，这两个都可以去考虑使用。

sharding-jdbc这种client层方案的优点在于[**不用部署，运维成本低**](http://www.mstacks.com/135/1436.html#content1436)，不需要代理层的二次转发请求，性能很高，但是如果遇到升级啥的需要各个系统都重新升级版本再发布，各个系统都需要耦合sharding-jdbc的依赖；

mycat这种proxy层方案的缺点在于需要[**部署，自己运维一套中间件，运维成本高**](http://www.mstacks.com/135/1436.html#content1436)，但是好处在于对于各个项目是透明的，如果遇到升级之类的都是自己中间件那里搞就行了。

通常来说，这两个方案其实都可以选用，但是我个人建议中小型公司选用sharding-jdbc，client层方案轻便，而且维护成本低，不需要额外增派人手，而且中小型公司系统复杂度会低一些，项目也没那么多；
但是中大型公司最好还是选用mycat这类proxy层方案，因为可能大公司系统和项目非常多，团队很大，人员充足，那么最好是专门弄个人来研究和维护mycat，然后大量项目直接透明使用即可。

## 七、总结

本文介绍了数据库分库分表的基本原理和常见问题，与数据库读写分离类似，分库分表具体的实现方式也是有“程序代码封装”和“中间件封装”两种，但实现会更复杂。

读写分离只要识别 SQL 操作是读操作还是写操作，通过简单的判断 SELECT、UPDATE、INSERT、DELETE 几个关键字就可以做到，而分库分表的实现除了要判断操作类型外，还要判断 SQL 中具体需要操作的表、操作函数（例如 count 函数)、order by、group by 操作等，然后再根据不同的操作进行不同的处理。例如 order by 操作，需要先从多个库查询到各个库的数据，然后再重新 order by 才能得到最终的结果。

所以目前分库分表和读写分离一样，大都引入数据库中间件来屏蔽细节，关于如何通过数据库中间件实现分库分表，我们会在进阶篇中详细介绍。

# 高性能：分布式缓存

## 一、简介

虽然我们可以通过读写分离、分库分表等各种手段来提升存储系统的性能，但在某些复杂的业务场景下，单纯依靠存储系统的性能提升是不够的，典型的场景有：

- **需要频繁进行复杂运算得出的数据**
  例如，一个论坛如果要实时展示用户同时在线数，则 MySQL 性能可能无法支撑，因为使用 MySQL 来存储当前用户状态，则每次获取这个总数都要“count(*)”大量数据，这样的操作无论怎么优化 MySQL，性能都不会太高。
- **读多写少的数据**
  绝大部分在线业务都是读多写少。例如，微博、淘宝、微信这类互联网业务，读业务占了整体业务量的 90% 以上。以微博为例：一个明星发一条微博，可能几千万人来浏览。如果使用 MySQL 来存储微博，明星写微博只有一条 insert 语句，但每个用户浏览时都要 select 一次，即使有索引，几千万条 select 语句对 MySQL 数据库的压力也会非常大。

> 缓存就是为了弥补存储系统在这些复杂业务场景下的不足而出现的，其基本原理是将可能重复使用的数据放到内存中，一次生成、多次使用，避免每次使用都去访问存储系统。

缓存能够带来性能的大幅提升，目前主流的开源分布式缓存有Memcache和Redis。以 Memcache 为例，单台 Memcache 服务器简单的 key-value 查询能够达到 QPS 50000 以上，其基本的架构是：


![img](picture/分布式.assets/20200409171357278.png)

缓存虽然能够大大减轻存储系统的压力，但同时也给架构引入了更多复杂性。架构设计时如果没有针对缓存的复杂性进行处理，某些场景下甚至会导致整个系统崩溃。

本文主要针对分布式缓存的架构设计要点作分析， 不对Memcache和Redis的使用方式和原理作介绍，读者可以参考相关专门的书籍，在后续的进阶篇中我们也会针对Redis的应用作深入讲解。

## 二、缓存穿透

缓存穿透，是指缓存没有发挥作用，业务系统虽然去缓存查询数据，但缓存中没有数据，业务系统需要再次去存储系统查询数据。

<img src="picture/分布式.assets/20200409171358089.png" alt="img" style="zoom:50%;" />

缓存穿透的问题很明显，本来我们加缓存就是为了提升系统性能，防止请求直接打到数据库，现在缓存命中不了了，所有请求会直接去数据库查询。如果并发量很高，会瞬间让数据库崩掉。

缓存穿透通常分为两种情况：缓存数据不存在、缓存数据耗时长。

### 2.1 缓存数据不存在

如果用户查询的时候，在缓存中找不到对应的数据，则每次都要去存储系统中再查询一遍，然后返回数据不存在。缓存在这个场景中并没有起到分担存储系统访问压力的作用。

通常情况下，未命中缓存的请求量并不会太大，但如果出现一些异常情况，例如黑客攻击，故意大量访问某些不存在的数据，则会导致大量请求直接打到后台的存储系统，进而将存储系统拖垮。

比如下图中，正常情况下，系统A会根据用户送的ID先去缓存查询数据，查询不到再去数据库查。如果黑客大量发起一些非法ID查询（比如全是负数），那么每次系统A从数据库中查询都得不到结果，这种恶意攻击场景的缓存穿透就把数据库打死了：

<img src="picture/分布式.assets/20200409171358709.png" alt="img" style="zoom:50%;" />

针对“缓存数据不存在”的穿透场景的解决办法比较简单：

1. 缓存空对象，并对其设置一个合适的过期时间，即如果没有查询到数据库中的数据，则直接设置一个默认值（可以是空值，也可以是具体的值）存到缓存中，这样第二次读取缓存时就会获取到默认值，且不会继续访问存储系统；
2. BloomFilter。在缓存层和存储层之前，将存在的有效key用 BloomFilter 提前保存起来，做第一层拦截，非法请求Key直接拦截掉，比如上述黑客攻击场景，可以限定只有正数ID是合法的。

### 2.2 缓存数据耗时长

第二种情况是存储系统中虽然存在数据，但生成缓存数据需要耗费较长时间或者耗费大量资源。如果刚好在业务访问的时候缓存失效了，那么也会出现缓存没有发挥作用，访问压力全部集中在存储系统上的情况。

最典型的就是电商的商品分页，假设我们在某个电商平台上选择“手机”这个类别查看，由于数据巨大，不能把所有数据都缓存起来，只能按照分页来进行缓存。由于难以预测用户到底会访问哪些分页，因此业务上最简单的实现方式就是每次点击分页的时按分页计算和生成缓存。通常情况下这样实现是基本满足需求的，但是如果被竞争对手用爬虫来遍历的时候，系统性能就可能出现问题。

具体的场景有：

1. 分页缓存的有效期设置为 1 天，因为设置太长时间的话，缓存不能反应真实的数据；
2. 通常情况下，用户不会从第 1 页到最后 1 页全部看完，一般用户访问集中在前 10 页，因此第 10 页以后的缓存过期失效的可能性很大；
3. 竞争对手每周来爬取数据，爬虫会将所有分类的所有数据全部遍历，从第 1 页到最后 1 页全部都会读取，此时很多分页缓存可能都失效了；
4. 由于很多分页都没有缓存数据，从数据库中生成缓存数据又非常耗费性能（order by limit 操作），因此爬虫会将整个数据库全部拖慢。

这种情况并没有太好的解决方案，因为爬虫会遍历所有的数据，而且什么时候来爬取也是不确定的，可能是每天都来，也可能是每周，也可能是一个月来一次，我们也不可能为了应对爬虫而将所有数据永久缓存。

通用方案要么就是[**识别爬虫然后禁止访问**]()，但这可能会影响 SEO 和推广；要么就是做好监控，发现问题后及时处理，因为爬虫不是攻击，不会进行暴力破坏，对系统的影响是逐步的，监控发现问题后有时间进行处理。

## 三、缓存雪崩

缓存雪崩，通常是指当缓存失效后引起系统性能急剧下降的情况。比如系统正常在高峰期能够抗住每秒5000次的并发请求，其中缓存承担掉4000次，剩余1000次直接落到数据库，但是由于缓存故障，导致1s内的5000个请求全部打到数据库，此时数据库就直接崩溃了。

系统一般在架构设计时，就需要针对缓存雪崩做一些防范措施：

- 事前：分布式缓存本身要做高可用，比如Redis的主从+哨兵、Redis Cluster，避免缓存本身不可用；
- 事中：服务本地做二级缓存（比如采用Ehcache或Guava），同时利用Hystrix做好限流和降级，避免数据库崩溃；
- 事后：缓存做持久化，以便崩溃后快速恢复缓存数据
  ![img](picture/分布式.assets/20200409171400451.png)

另一种缓存雪崩的场景是：当缓存过期被清除后，业务系统需要重新生成缓存，而对于一个高并发的业务系统来说，几百毫秒内可能会接到成百上千个请求。[**如果处理这些请求的线程都不知道另外有一个线程正在生成缓存**](http://www.mstacks.com/135/1436.html#content1436)，因此所有的请求都会去重新生成缓存，从而对存储系统造成巨大的性能压力。这些压力又会拖慢整个系统，严重的会造成数据库宕机，从而形成一系列连锁反应，造成整个系统崩溃。

应对这种缓存雪崩场景的常见解决方法有两种：[**更新锁机制和后台更新机制**]()。

### 3.1 更新锁机制

[**对缓存更新操作进行加锁保护，保证只有一个线程能够进行缓存更新，未能获取更新锁的线程要么等待锁释放后重新读取缓存，要么就返回空值或者默认值。**]()

对于分布式系统，即使单台服务器只有一个线程更新缓存，但几十上百台服务器一起算下来也会有几十上百个线程同时来更新缓存，同样存在雪崩的问题。因此分布式系统要实现更新锁机制，需要用到分布式锁，可以采用ZooKeeper来做分布式锁的实现。

### 3.2 后台更新机制

后台更新机制，就是由后台线程去定时更新缓存，而不是由业务线程来更新缓存，同时缓存本身的有效期设置为永久。后台更新机制需要考虑一种特殊场景：当缓存系统内存不够时，会“淘汰”一些缓存数据，即缓存淘汰策略。

从缓存被淘汰到下一次定时更新缓存的这段时间内，业务线程读取缓存返回空值，而业务线程本身又不会去更新缓存，因此业务上看到的现象就是数据丢了。解决的方式有两种：

1. 后台线程主动更新：[**除了定时更新缓存，还要频繁地去读取缓存（比如1 秒读取一次），如果发现缓存被淘汰就立刻更新缓存。**]()这种方式实现简单，但读取间隔不能设置太长，因为如果缓存被淘汰且缓存读取间隔时间又太长，则这段时间内业务访问都拿不到真正的数据而是一个空的缓存值，用户体验较差；
2. 业务线程主动通知：[**业务线程发现缓存失效后，通过MQ通知后台线程更新缓存。此时可能会出现多个业务线程都发送了缓存更新消息，但其实对后台线程没有影响，因为后台线程可以判断缓存是否存在，存在就不执行更新操作。**]()这种方式实现依赖MQ，复杂度会高一些，但缓存更新更及时，用户体验更好。

> 后台更新机制，相比更新锁机制要简单一些。同时也适合业务刚上线的时进行缓存预热。所谓缓存预热指系统上线后，将相关的缓存数据直接加载到缓存系统，而不是等待用户访问才来触发缓存加载。

## 四、双写一致性

业务线程在做更新数据操作时，通常需要在数据库操作成功后，再更新缓存中的数据。那么问题来了：如果数据库操作成功，缓存更新失败，就会出现数据不一致，这就是双写一致性问题。

比如下图的库存服务，初始时数据库和缓存中的库存都是1000，当请求扣减100库存时，数据库操作成功，但是缓存操作失败了，此时缓存和数据库中的数据就不一致了：

<img src="picture/分布式.assets/20200409171401787.png" alt="img" style="zoom:50%;" />

> 一般来说，如果你的系统使用缓存的场景并不是严格要求缓存+数据库必须保证数据一致性的话，缓存可以稍微的跟数据库偶尔有不一致的情况。

### 4.1 Cache Aside Pattern

解决双写一致性问题的基本模式就是Cache Aside Pattern，其基本思路如下：

1. 读操作：先读缓存，缓存不存在则读数据库，然后将数据库结果写入缓存，同时返回响应;
2. 写操作：[**先删除缓存，然后再更新数据库**]()。

写操作为什么是删除缓存，而不是更新完数据库后再直接更新缓存呢？

原因很简单，更新缓存的代价是很高的，很多时候复杂点的缓存场景，缓存中的数据并不仅仅是简单的直接从数据库中取出来的值。比如：商品详情页的系统，当修改库存时，可能先更新某个表的一个字段，然后查询另外两个表的数据，并进行运算，最后将算出的库存更新到缓存中去。

从另一个角度讲，如果每次修改数据库的时候，都将其对应的缓存更新一下，效率是很低的。举个例子：一个缓存涉及的表字段，可能在1分钟内被修改100次，那么缓存也要更新100次，但是实际上如果这个缓存在1分钟内就被读取了1次，就会出现大量冷数据。而如果采取先删除缓存，再更新数据库的方式，那么1分钟内，缓存不过就重新计算一次而已，开销大幅度降低。

> 其实删除缓存，而不是更新缓存，就是一个lazy计算的思想，不要每次都重新做复杂的计算，不管它会不会用到，而是让它到需要被使用的时候再重新计算。

但是，Cache Aside Pattern存在两个问题：

- 缓存旧数据
- 高并发环境下的数据不一致性

#### 缓存旧数据

我们先来看下“缓存旧数据”的情况：如果删除缓存成功，但是修改数据库失败了，那么就会出现数据库中是旧数据，但缓存是空的情况。此时如果有读请求过来，就会将数据库中的旧数据更新到缓存中。不过这种情况虽然是脏数据，但数据库和缓存的数据至少是一致的，所以整体没啥影响。


![img](picture/分布式.assets/20200409171402461.png)

#### 高并发数据不一致

第二种情况比较复杂，我们来看下高并发情况下是如何出现数据不一致的。

> 第一个请求过来删了缓存，第二个请求过来发现没有缓存去读了数据库放入缓存。此时第一个请求才修改数据库完成。此时，双写不一致。

1.一个更新请求过来，先删除了缓存，然后正要去修改数据库，但还没修改：

<img src="picture/分布式.assets/20200409171403343.png" alt="img" style="zoom:50%;" />
2.此时一个读请求过来，发现缓存为空，就去查询数据库，查到了修改前的旧数据，并更新到缓存中：<img src="picture/分布式.assets/20200409171404076.png" alt="img" style="zoom:50%;" />

3.此时第一个更新请求完成了修改数据库的操作，这样就出现了数据不一致的问题（缓存中是旧数据而数据库中是新数据）：
<img src="picture/分布式.assets/20200409171404630.png" alt="img" style="zoom:50%;" />

> 在对同一个数据进行高并发的读写时，比如1wQPS以上的情况，只要有数据更新的请求，就可能会出现上述的数据库和缓存数据不一致的情况。

### 4.2 内存队列方案

Cache Aside Pattern在高并发环境下之所以出现数据不一致，最主要的原因就是读操作在发现缓存数据不存在时，既然是针对同一份数据的读写并发引起的数据不一致，[**那最基本的思路就是将Cache Aside Pattern中的读操作和写操作串行化**]()，可以串到一个内存队列里去，这样就可以保证一定不会出现不一致的情况。

我们继续以商品库存服务为例来具体看下：

1. 首先，假设每种商品都有全局唯一的商品ID，我们需要将商品和内存队列映射起来，我们可以对商品ID进行Hash，其结果再对内存队列数N取模——CRC16(M_ID)%N，这样每一种商户就关联到了一个唯一的内存队列；
2. 对于每个更新请求（写操作），直接入队列，然后每个队列都有一个工作线程会依次从队列中取出请求，如果发现是更新请求，就执行删除缓存、更新数据库的操作；
3. 对于每个读请求，如果发现缓存中没有数据，也会先进入对应商品的队列中，等待工作线程处理，当工作线程发现是读请求时，就会先读数据库，然后根据结果再更新缓存。

这样一来，针对同一个商品的读/写操作就全部关联到了同一个队列中，串行化的操作可以保证不会出现数据不一致，整体流程如下图：
![img](picture/分布式.assets/20200409171406750.png)

#### 优化点

这里有一个优化点：一个队列中，针对同一份缓存数据的多个读请求串在一起是没意义的。也就是说对于一个高并发的业务系统，几百毫秒内可能会接到成百上千个读请求。如果处理这些请求的线程都不知道另外有一个线程正在生成缓存，那这些读请求都会去重新生成缓存，影响整个系统的效能。

因此需要做下过滤，[**如果发现队列中已经有某个商品的读请求了，那么就不用将这个读请求放进队列，而是去缓存中轮询下（比如200ms），如果轮询到了结果，就直接返回，如果轮询不到结果，就查数据库获取最新结果并返回。**]()

#### 读请求超时

上述优化点有一个大的风险点：[**如果数据更新很频繁，导致了队列中积压了大量写操作在里面，那读操作在轮询不到结果就会发生大量的超时，导致大量读请求直接走数据库。**]()

比如，如果一个队列里积压了100个商品的库存修改操作，每个库存修改操作要耗费20ms完成，那么最后一个商品的读请求，可能要等待20x100ms = 2s 后才能得到数据，这时就导致了读请求的长时阻塞。

所以，务必通过一些模拟真实的测试，看看更新操作的频率是怎样的。根据之前的项目经验，一般来说数据的更新频率是很低的，所以队列中积压的更新操作应该是很少的，一般更新操作能有500QPS就不错了。我们就以500QPS来估算下需要多少内存队列：

首先假设读操作最多允许阻塞200ms，每个写操作耗时20ms。那么200ms内一个队列中最多允许积压10个写操作，即1s内最多积压50个写操作。总QPS是500，所以10个内存队列一般就够了，这样能够保证每个读操作在200ms内返回。

假设1秒有500个写请求，即每200ms有100个写请求，放到20个内存队列里，每个队列最多也就积压5个写请求，写操作一般20ms完成，那么针对每个内存队列中的数据的读请求，也就最多200ms以内肯定能返回了。

极端情况下，写QPS扩大10倍，但是经过刚才的测算就知道单机支撑几百的写QPS没问题。既然QPS扩了10倍，那就把机器也扩10倍，10台机器，每个机器20个队列，共200个队列。

#### 读请求并发过高

在上述更新请求的处理期间，可能会有大量读请求同时轮询等待缓存结果，如果在超时结束后，这些读请求全部去数据库查询结果，就可能导致数据库瞬间被打崩。

所以就需要测算500QPS的写请求，在同一时间最多hang住多少读请求。根据经验，读写请求比例大概在2：1就差不多了，也就是说500QPS的写请求，对应每秒约有1000个读请求会hang住，而单机MySQL的大概能支撑2000TPS左右。

#### 请求路由

基于内存队列的方案，必须要保证针对同一个商品的读/写请求能够路由到同一台机器的同一个内存队列里。可以借助一些路由算法对特征值计算来实现路由，比如Nginx的hash路由等。

### 4.3 总结

本节，主要针对的缓存更新机制作了讲解，一般来说，如果并发量不高，或业务场景并不要求缓存和数据库的严格数据一致性的话，那么[Cache Aside Pattern](https://www.tpvlog.com/article/82)就足够应付了。只有当出现高并发场景且数据一致性要求非常高时，才会使用上述的内存队列方案，而且用了这个方案，会造成以下影响：

1. 读请求和写请求串行化后，会导致系统的吞吐量会大幅降低，用比正常情况下多几倍的机器去支撑线上的一个请求；
2. 系统的复杂度大幅度上升。

## 五、缓存热点

虽然缓存系统本身的性能比较高，但对于一些特别热点的数据，如果大部分甚至所有的业务请求都命中同一份缓存数据，则这份数据所在的缓存服务器的压力也很大。例如，某明星微博发布“我们”来宣告恋爱了，短时间内上千万的用户都会来围观。

缓存热点的解决方案就是复制多份缓存副本，将请求分散到多个缓存服务器上，减轻缓存热点导致的单台缓存服务器压力。以微博为例，对于粉丝数超过 100 万的明星，每条微博都可以生成 100 份缓存，缓存的数据是一样的，通过在缓存的 key 里面加上编号进行区分，每次读缓存时都随机读取其中某份缓存。

> 缓存副本设计有一个细节需要注意，就是不同的缓存副本不要设置统一的过期时间，否则就会出现所有缓存副本同时生成同时失效的情况，从而引发缓存雪崩效应。正确的做法是设定一个过期时间范围，不同的缓存副本的过期时间是指定范围内的随机值

## 六、总结

分布式缓存涉及的东西非常多，本文仅介绍了缓存设计中的几个关键点，更多内容读者可以去学习Redis或Memcache的实现和原理。在后续实战系列中，我们也会通过一个真实的案例去讲解分布式缓存的整体设计思路。

# 高性能：负载均衡

## 一、引言

前面的读写分离和分库分表主要是从存储压力来考虑性能问题，本章将介绍的负载均衡主要从计算能力的角度来考虑性能。

单服务器无论如何优化，无论采用多好的硬件，总会有一个性能天花板，当单服务器的性能无法满足业务需求时，就需要设计高性能集群来提升系统整体的处理性能。[**高性能集群的本质很简单——通过增加更多的服务器来提升系统整体的计算能力。**]()

高性能集群设计的复杂度主要体现在任务分配这部分，需要设计合理的任务分配策略，将计算任务分配到多台服务器上执行。这个任务分配器，就是“负载均衡器”。常见的负载均衡系统包括3种：DNS负载均衡、硬件负载均衡和软件负载均衡。

### 1.1 DNS负载均衡

DNS是最简单也是最常见的负载均衡方式，一般用来实现地理级别的均衡。例如，北方的用户访问北京的机房，南方的用户访问上海的机房。DNS负载均衡的本质是DNS解析同一个域名可以返回不同的IP地址。例如，同样是[www.baidu.com](https://www.tpvlog.com/article/www.baidu.com)，北方用户解析后获取的地址是61.135.165.224（这是北京机房的IP），南方用户解析后获取的地址是14.215.177.38（这是上海机房的IP）。

下面是DNS负载均衡的简单示意图：

<img src="picture/分布式.assets/20200409171529880.jpg" alt="img" style="zoom:50%;" />

DNS负载均衡实现简单、成本低，但也存在粒度太粗、负载均衡算法少等缺点。

优点：

- [**简单、成本低**]()：负载均衡工作交给DNS服务器处理，无须自己开发或者维护负载均衡设备；
- 就近访问，[**提升访问速度**]()：DNS解析时可以根据请求来源IP，解析成距离用户最近的服务器地址，可以加快访问速度，改善性能。

缺点：

- [**更新不及时**]()：DNS缓存的时间比较长，修改DNS配置后，由于缓存的原因，还是有很多用户会继续访问修改前的IP，这样的访问会失败，达不到负载均衡的目的，并且也影响用户正常使用业务；
- 扩展性差：DNS负载均衡的控制权在域名商那里，无法根据业务特点针对其做更多的定制化功能和扩展特性；
- [**分配策略比较简单**]()：DNS负载均衡支持的算法少；不能区分服务器的差异（不能根据系统与服务的状态来判断负载），也无法感知后端服务器的状态。

### 1.2 硬件负载均衡

硬件负载均衡是通过单独的硬件设备来实现负载均衡功能，这类设备[**和路由器、交换机类似**]()，可以理解为一个用于负载均衡的基础网络设备。目前业界典型的硬件负载均衡设备有两款：F5和A10。
这类设备性能强劲、功能强大，但价格都不便宜，一般只有大型公司才会考虑使用此类设备。普通业务量级的公司一是负担不起，二是业务量没那么大，用这些设备也是浪费。

优点：

- 功能强大：全面支持各层级的负载均衡，支持全面的负载均衡算法，支持全局负载均衡；
- 性能强大：硬件负载均衡可以支持100万以上的并发，而软件负载均衡能支持到10万级并发就很不错了；
- 稳定性高：商用硬件负载均衡，经过了良好的严格测试，经过大规模使用，稳定性高；
- 支持安全防护：硬件均衡设备除具备负载均衡功能外，还具备防火墙、防DDoS攻击等安全功能。

缺点：

- [**贵**]()
- 定制化较为困难

### 1.3 软件负载均衡

软件负载均衡是指通过负载均衡软件来实现负载均衡功能，常见的有Nginx和LVS，其中Nginx是软件的7层负载均衡，LVS是Linux内核的4层负载均衡。下面是Nginx的负载均衡架构示意图：

<img src="picture/分布式.assets/20200409171530427.jpg" alt="img" style="zoom: 33%;" />

> 4层和7层的区别就在于协议和灵活性，Nginx支持HTTP、E-mail协议；而LVS是4层负载均衡，和协议无关，几乎所有应用都可以做，例如，聊天、数据库等。

优点：

- 简单：无论是部署还是维护都比较简单；
- 便宜：只要买个Linux服务器，装上软件即可；
- 灵活：4层和7层负载均衡可以根据业务进行选择，也可以根据业务进行比较方便的扩展，例如，可以通过Nginx的插件来实现业务的定制化功能。

缺点：

- 性能一般：Ngxin的性能是万级，一个Nginx大约能支撑5万并发，LVS的性能是十万级；
- 一般不具备防火墙和防DDoS攻击等安全功能。

## 二、负载均衡架构

前面我们介绍了3种常见的负载均衡机制：DNS负载均衡、硬件负载均衡、软件负载均衡，每种方式都有一些优缺点，但并不意味着在实际应用中只能基于它们的优缺点进行非此即彼的选择，反而是基于它们的优缺点进行组合使用。具体来说，组合的基本原则为：

- DNS负载均衡用于实现地理级别的负载均衡；
- 硬件负载均衡用于实现集群级别的负载均衡；
- 软件负载均衡用于实现机器级别的负载均衡。

以一个假想的实例来说明一下这种组合方式，如下图所示：

![img](picture/分布式.assets/20200409171531639.png)

上述整个系统的负载均衡分为三层：

- 地理级别负载均衡：www.xxx.com部署在北京、广州、上海三个机房，当用户访问时，DNS会根据用户的地理位置来决定返回哪个机房的IP，图中返回了广州机房的IP地址，这样用户就访问到广州机房了。
- 集群级别负载均衡：广州机房的负载均衡用的是F5设备，F5收到用户请求后，进行集群级别的负载均衡，将用户请求发给3个本地集群中的一个，我们假设F5将用户请求发给了“广州集群2”。
- 机器级别的负载均衡：广州集群2的负载均衡用的是Nginx，Nginx收到用户请求后，将用户请求发送给集群里面的某台服务器，服务器处理用户的业务请求并返回业务响应。

> 需要注意的是，上图只是一个示例，一般在大型业务场景下才会这样用，如果业务量没这么大，则没有必要严格照搬这套架构。例如，一个普通的公司管理系统，完全可以不需要DNS负载均衡，也不需要F5设备，只需要用Nginx作为一个简单的负载均衡就足够了。

## 三、负载均衡算法

负载均衡算法数量较多，而且可以根据一些业务特性进行定制开发，抛开细节上的差异，根据算法期望达到的目的，大体上可以分为下面几类：

- 任务平分类：负载均衡系统将收到的任务平均分配给服务器进行处理，这里的“平均”可以是绝对数量的平均，也可以是比例或者权重上的平均；
- 负载均衡类：负载均衡系统根据服务器的负载来进行分配，这里的负载并不一定是通常意义上我们说的“CPU负载”，而是系统当前的压力，可以用CPU负载来衡量，也可以用连接数、I/O使用率、网卡吞吐量等来衡量系统的压力；
- 性能最优类：负载均衡系统根据服务器的响应时间来进行任务分配，优先将新任务分配给响应最快的服务器；
- Hash类：负载均衡系统根据任务中的某些关键信息进行Hash运算，将相同Hash值的请求分配到同一台服务器上。常见的有源地址Hash、目标地址Hash、session id hash、用户ID Hash等。

接下来介绍一下常见的负载均衡算法以及它们的优缺点。

### 3.1 普通轮询

负载均衡系统收到请求后，按照顺序轮流分配到服务器上。轮询是最简单的一个策略，无须关注服务器本身的状态。但如果服务器直接宕机了，或者服务器和负载均衡系统断连了，这时负载均衡系统是能够感知的，也需要做出相应的处理。

> “简单”是轮询算法的优点，也是它的缺点。

### 3.2 加权轮询

负载均衡系统根据服务器权重进行任务分配，权重一般是根据硬件配置进行静态配置的，采用动态的方式计算会更加契合业务，但复杂度也会更高。

加权轮询是普通轮询的一种特殊形式，其主要目的就是为了解决不同服务器处理能力有差异的问题。例如，集群中有新的机器是32核的，老的机器是16核的，那么理论上我们可以假设新机器的处理能力是老机器的2倍，负载均衡系统就可以按照2:1的比例分配更多的任务给新机器，从而充分利用新机器的性能。

> 加权轮询解决了普通轮询算法中无法根据服务器的配置差异进行任务分配的问题，但同样存在无法根据服务器的状态差异进行任务分配的问题。

### 3.3 负载最低优先

负载均衡系统将任务分配给当前负载最低的服务器，这里的负载根据不同的任务类型和业务场景，可以用不同的指标来衡量。例如：

- LVS这种4层网络负载均衡设备，可以以[**“连接数”**]()来判断服务器的状态，服务器连接数越大，表明服务器压力越大；
- Nginx这种7层网络负载系统，可以以[**“HTTP请求数”**]()来判断服务器状态（Nginx内置的负载均衡算法不支持这种方式，需要进行扩展）；
- 如果我们自己开发负载均衡算法，可以根据业务特点来选择指标衡量系统压力。如果是CPU密集型，可以以“CPU负载”来衡量系统压力；如果是I/O密集型，可以以“I/O负载”来衡量系统压力。

负载最低优先的算法解决了轮询算法中无法感知服务器状态的问题，由此带来的代价是复杂度要增加很多：

- 负载均衡系统需要统计每个服务器当前建立的连接，其应用场景仅限于负载均衡接收的任何连接请求都会转发给服务器进行处理，否则如果负载均衡系统和服务器之间是固定的连接池方式，就不适合采取这种算法。例如，LVS可以采取这种算法进行负载均衡，而一个通过连接池的方式连接MySQL集群的负载均衡系统就不适合采取这种算法进行负载均衡；
- CPU负载最低优先的算法要求负载均衡系统以某种方式收集每个服务器的CPU负载，而且要确定是以1分钟的负载为标准，还是以15分钟的负载为标准，不存在1分钟肯定比15分钟要好或者差。不同业务最优的时间间隔是不一样的，时间间隔太短容易造成频繁波动，时间间隔太长又可能造成峰值来临时响应缓慢。

> 负载最低优先算法基本上能够比较完美地解决轮询算法的缺点，因为采用这种算法后，负载均衡系统需要感知服务器当前的运行状态。当然，其代价是复杂度大幅上升。负载最低优先算法如果本身没有设计好，或者不适合业务的运行特点，算法本身就可能成为性能的瓶颈，或者引发很多莫名其妙的问题。所以负载最低优先算法虽然效果看起来很美好，但实际上真正应用的场景反而没有轮询（包括加权轮询）那么多。

### 3.4 性能最优优先

负载最低优先类算法是站在服务器的角度来进行分配的，而性能最优优先类算法则是站在客户端的角度来进行分配的，优先将任务分配给处理速度最快的服务器，通过这种方式达到最快响应客户端的目的。

[**性能最优优先类算法本质上也是感知了服务器的状态**]()，只是通过响应时间这个外部标准来衡量服务器状态而已。因此性能最优优先类算法存在的问题和负载最低优先类算法类似，复杂度都很高，主要体现在：

- 负载均衡系统需要收集和分析每个服务器每个任务的响应时间，在大量任务处理的场景下，这种收集和统计本身也会消耗较多的性能；
- 为了减少统计上的消耗，可以采取抽样统计：即不统计所有任务的响应时间，而是抽样统计部分任务的响应时间来估算整体任务的响应时间。抽样统计虽然能够减少性能消耗，但使得复杂度进一步上升，采样率太低会导致结果不准确，采样率太高会导致性能消耗较大，找到合适的采样率也是一件复杂的事情。

### 3.5 Hash类

负载均衡系统根据任务中的某些关键信息进行Hash运算，将相同Hash值的请求分配到同一台服务器上，这样做的目的主要是为了满足特定的业务需求。例如：

**源地址Hash：**
[**将来源于同一个源IP地址的任务分配给同一个服务器进行处理**]()，适合于存在事务、会话的业务。例如，当我们通过浏览器登录网上银行时，会生成一个会话信息，这个会话是临时的，关闭浏览器后就失效。网上银行后台无须持久化会话信息，只需要在某台服务器上临时保存这个会话就可以了，但需要保证用户在会话存在期间，每次都能访问到同一个服务器，这种业务场景就可以用源地址Hash来实现。

**ID Hash：**
[**将某个ID标识的业务分配到同一个服务器中进行处理**]()，这里的ID一般是临时性数据的ID（如session id）。例如，上述的网上银行登录的例子，对sessionId 进行hash同样可以实现同一个会话期间，用户每次都是访问到同一台服务器的目的。

## 四、总结

本文介绍了常见的负载均衡架构设计模式及负载均衡算法。目前业界用的最多的是软负载均衡器——Nginx，后续进阶篇中将详细介绍其特性和原理。

# 大数据处理问题

## 如何从大量的 URL 中找出相同的 URL？

### 题目描述

给定 a、b 两个文件，各存放 50 亿个 URL，每个 URL 各占 64B，内存限制是 4G。请找出 a、b 两个文件共同的 URL。

### 解答思路

#### 1. 分治策略

每个 URL 占 64B，那么 50 亿个 URL 占用的空间大小约为 320GB。

> 5, 000, 000, 000 _ 64B ≈ 5GB _ 64 = 320GB

由于内存大小只有 4G，因此，我们不可能一次性把所有 URL 加载到内存中处理。对于这种类型的题目，一般采用**分治策略**，即：把一个文件中的 URL 按照某个特征划分为多个小文件，使得每个小文件大小不超过 4G，这样就可以把这个小文件读到内存中进行处理了。

**思路如下**：

首先遍历文件 a，对遍历到的 URL 求 `hash(URL) % 1000` ，根据计算结果把遍历到的 URL 存储到 a0, a1, a2, ..., a999，这样每个大小约为 300MB。使用同样的方法遍历文件 b，把文件 b 中的 URL 分别存储到文件 b0, b1, b2, ..., b999 中。这样处理过后，所有可能相同的 URL 都在对应的小文件中，即 a0 对应 b0, ..., a999 对应 b999，不对应的小文件不可能有相同的 URL。那么接下来，我们只需要求出这 1000 对小文件中相同的 URL 就好了。

接着遍历 ai( `i∈[0,999]` )，把 URL 存储到一个 HashSet 集合中。然后遍历 bi 中每个 URL，看在 HashSet 集合中是否存在，若存在，说明这就是共同的 URL，可以把这个 URL 保存到一个单独的文件中。

#### 2. 前缀树

一般而言，URL 的长度差距不会不大，而且前面几个字符，绝大部分相同。这种情况下，非常适合使用**字典树**（trie tree） 这种数据结构来进行存储，降低存储成本的同时，提高查询效率。

### 方法总结

#### 分治策略

1. 分而治之，进行哈希取余；
2. 对每个子文件进行 HashSet 统计。

#### 前缀树

1. 利用字符串的公共前缀来降低存储成本，提高查询效率。

## 如何从大量数据中找出高频词？

### 题目描述

有一个 1GB 大小的文件，文件里每一行是一个词，每个词的大小不超过 16B，内存大小限制是 1MB，要求返回频数最高的 100 个词(Top 100)。

### 解答思路

由于内存限制，我们依然无法直接将大文件的所有词一次读到内存中。因此，同样可以采用**分治策略**，把一个大文件分解成多个小文件，保证每个文件的大小小于 1MB，进而直接将单个小文件读取到内存中进行处理。

**思路如下**：

首先遍历大文件，对遍历到的每个词 x，执行 `hash(x) % 5000` ，将结果为 i 的词存放到文件 ai 中。遍历结束后，我们可以得到 5000 个小文件。每个小文件的大小为 200KB 左右。如果有的小文件大小仍然超过 1MB，则采用同样的方式继续进行分解。

接着统计每个小文件中出现频数最高的 100 个词。最简单的方式是使用 HashMap 来实现。其中 key 为词，value 为该词出现的频率。具体方法是：对于遍历到的词 x，如果在 map 中不存在，则执行 `map.put(x, 1)` ；若存在，则执行 `map.put(x, map.get(x)+1)` ，将该词频数加 1。

上面我们统计了每个小文件单词出现的频数。接下来，我们可以通过维护一个**小顶堆**来找出所有词中出现频数最高的 100 个。具体方法是：依次遍历每个小文件，构建一个**小顶堆**，堆大小为 100。如果遍历到的词的出现次数大于堆顶词的出现次数，则用新词替换堆顶的词，然后重新调整为**小顶堆**，遍历结束后，小顶堆上的词就是出现频数最高的 100 个词。

### 方法总结

1. 分而治之，进行哈希取余；
2. 使用 HashMap 统计频数；
3. 求解**最大**的 TopN 个，用**小顶堆**；求解**最小**的 TopN 个，用**大顶堆**。

## 如何找出某一天访问百度网站最多的 IP？

### 题目描述

现有海量日志数据保存在一个超大文件中，该文件无法直接读入内存，要求从中提取某天访问百度次数最多的那个 IP。

### 解答思路

这道题只关心某一天访问百度最多的 IP，因此，可以首先对文件进行一次遍历，把这一天访问百度 IP 的相关信息记录到一个单独的大文件中。接下来采用的方法与上一题一样，大致就是先对 IP 进行哈希映射，接着使用 HashMap 统计重复 IP 的次数，最后计算出重复次数最多的 IP。

> 注：这里只需要找出出现次数最多的 IP，可以不必使用堆，直接用一个变量 max 即可。

### 方法总结

1. 分而治之，进行哈希取余；
2. 使用 HashMap 统计频数；
3. 求解**最大**的 TopN 个，用**小顶堆**；求解**最小**的 TopN 个，用**大顶堆**。

## 如何在大量的数据中找出不重复的整数？

### 题目描述

在 2.5 亿个整数中找出不重复的整数。注意：内存不足以容纳这 2.5 亿个整数。

### 解答思路

#### 方法一：分治法

与前面的题目方法类似，先将 2.5 亿个数划分到多个小文件，用 HashSet/HashMap 找出每个小文件中不重复的整数，再合并每个子结果，即为最终结果。

#### 方法二：位图法

**位图**，就是用一个或多个 bit 来标记某个元素对应的值，而键就是该元素。采用位作为单位来存储数据，可以大大节省存储空间。

位图通过使用位数组来表示某些元素是否存在。它可以用于快速查找，判重，排序等。不是很清楚？我先举个小例子。

假设我们要对 `[0,7]` 中的 5 个元素 (6, 4, 2, 1, 5) 进行排序，可以采用位图法。0~7 范围总共有 8 个数，只需要 8bit，即 1 个字节。首先将每个位都置 0：

```java
0 0 0 0 0 0 0 0
```

然后遍历 5 个元素，首先遇到 6，那么将下标为 6 的位的 0 置为 1；接着遇到 4，把下标为 4 的位 的 0 置为 1：

```
0 0 0 0 1 0 1 0
```

依次遍历，结束后，位数组是这样的：

```
0 1 1 0 1 1 1 0
```

每个为 1 的位，它的下标都表示了一个数：

```c
for i in range(8):
    if bits[i] == 1:
        print(i)
```

这样我们其实就已经实现了排序。

对于整数相关的算法的求解，**位图法**是一种非常实用的算法。假设 int 整数占用 4B，即 32bit，那么我们可以表示的整数的个数为 2^32。

**那么对于这道题**，我们用 2 个 bit 来表示各个数字的状态：

- 00 表示这个数字没出现过；
- 01 表示这个数字出现过一次（即为题目所找的不重复整数）；
- 10 表示这个数字出现了多次。

那么这 232 个整数，总共所需内存为 232*2b=1GB。因此，当可用内存超过 1GB 时，可以采用位图法。假设内存满足位图法需求，进行下面的操作：

遍历 2.5 亿个整数，查看位图中对应的位，如果是 00，则变为 01，如果是 01 则变为 10，如果是 10 则保持不变。遍历结束后，查看位图，把对应位是 01 的整数输出即可。

当然，本题中特别说明：**内存不足以容纳这 2.5 亿个整数**，2.5 亿个整数的内存大小为：2.5e8/1024/1024/1024=0.93G，也即是说内存不足 1G，而位图法所需要的内存大小为 1G，因此，本题并不适合用位图法解决。

### 方法总结

**判断数字是否重复的问题**，位图法是一种非常高效的方法，当然前提是：内存要满足位图法所需要的存储空间。

## 如何在大量的数据中判断一个数是否存在？

### 题目描述

给定 40 亿个不重复的没排过序的 unsigned int 型整数，然后再给定一个数，如何快速判断这个数是否在这 40 亿个整数当中？

### 解答思路

#### 方法一：分治法

依然可以用分治法解决，方法与前面类似，就不再次赘述了。

#### 方法二：位图法

由于 unsigned int 数字的范围是 `[0, 1 << 32)`，我们用 `1<<32=4,294,967,296` 个 bit 来表示每个数字。初始位均为 0，那么总共需要内存：4,294,967,296b≈512M。

我们读取这 40 亿个整数，将对应的 bit 设置为 1。接着读取要查询的数，查看相应位是否为 1，如果为 1 表示存在，如果为 0 表示不存在。

### 方法总结

**判断数字是否存在、判断数字是否重复的问题**，位图法是一种非常高效的方法。

## 如何查询最热门的查询串？

### 题目描述

搜索引擎会通过日志文件把用户每次检索使用的所有查询串都记录下来，每个查询串的长度不超过 255 字节。

假设目前有 1000w 个记录（这些查询串的重复度比较高，虽然总数是 1000w，但如果除去重复后，则不超过 300w 个）。请统计最热门的 10 个查询串，要求使用的内存不能超过 1G。（一个查询串的重复度越高，说明查询它的用户越多，也就越热门。）

### 解答思路

每个查询串最长为 255B，1000w 个串需要占用 约 2.55G 内存，因此，我们无法将所有字符串全部读入到内存中处理。

#### 方法一：分治法

分治法依然是一个非常实用的方法。

划分为多个小文件，保证单个小文件中的字符串能被直接加载到内存中处理，然后求出每个文件中出现次数最多的 10 个字符串；最后通过一个小顶堆统计出所有文件中出现最多的 10 个字符串。

方法可行，但不是最好，下面介绍其他方法。

#### 方法二：HashMap 法

虽然字符串总数比较多，但去重后不超过 300w，因此，可以考虑把所有字符串及出现次数保存在一个 HashMap 中，所占用的空间为 300w*(255+4)≈777M（其中，4 表示整数占用的 4 个字节）。由此可见，1G 的内存空间完全够用。

**思路如下**：

首先，遍历字符串，若不在 map 中，直接存入 map，value 记为 1；若在 map 中，则把对应的 value 加 1，这一步时间复杂度 `O(N)` 。

接着遍历 map，构建一个 10 个元素的小顶堆，若遍历到的字符串的出现次数大于堆顶字符串的出现次数，则进行替换，并将堆调整为小顶堆。

遍历结束后，堆中 10 个字符串就是出现次数最多的字符串。这一步时间复杂度 `O(Nlog10)` 。

#### 方法三：前缀树法

方法二使用了 HashMap 来统计次数，当这些字符串有大量相同前缀时，可以考虑使用前缀树来统计字符串出现的次数，树的结点保存字符串出现次数，0 表示没有出现。

**思路如下**：

在遍历字符串时，在前缀树中查找，如果找到，则把结点中保存的字符串次数加 1，否则为这个字符串构建新结点，构建完成后把叶子结点中字符串的出现次数置为 1。

最后依然使用小顶堆来对字符串的出现次数进行排序。

### 方法总结

前缀树经常被用来统计字符串的出现次数。它的另外一个大的用途是字符串查找，判断是否有重复的字符串等。

## 如何统计不同电话号码的个数？

### 题目描述

已知某个文件内包含一些电话号码，每个号码为 8 位数字，统计不同号码的个数。

### 解答思路

这道题本质还是求解**数据重复**的问题，对于这类问题，一般首先考虑位图法。

对于本题，8 位电话号码可以表示的号码个数为 108 个，即 1 亿个。我们每个号码用一个 bit 来表示，则总共需要 1 亿个 bit，内存占用约 100M。

**思路如下**：

申请一个位图数组，长度为 1 亿，初始化为 0。然后遍历所有电话号码，把号码对应的位图中的位置置为 1。遍历完成后，如果 bit 为 1，则表示这个电话号码在文件中存在，否则不存在。bit 值为 1 的数量即为 不同电话号码的个数。

### 方法总结

求解数据重复问题，记得考虑位图法。

## 如何从 5 亿个数中找出中位数？

### 题目描述

从 5 亿个数中找出中位数。数据排序后，位置在最中间的数就是中位数。当样本数为奇数时，中位数为 第 `(N+1)/2` 个数；当样本数为偶数时，中位数为 第 `N/2` 个数与第 `1+N/2` 个数的均值。

### 解答思路

如果这道题没有内存大小限制，则可以把所有数读到内存中排序后找出中位数。但是最好的排序算法的时间复杂度都为 `O(NlogN)` 。这里使用其他方法。

#### 方法一：双堆法

维护两个堆，一个大顶堆，一个小顶堆。大顶堆中最大的数**小于等于**小顶堆中最小的数；保证这两个堆中的元素个数的差不超过 1。

若数据总数为**偶数**，当这两个堆建好之后，**中位数就是这两个堆顶元素的平均值**。当数据总数为**奇数**时，根据两个堆的大小，**中位数一定在数据多的堆的堆顶**。

```java
class MedianFinder {

    private PriorityQueue<Integer> maxHeap;
    private PriorityQueue<Integer> minHeap;

    /** initialize your data structure here. */
    public MedianFinder() {
        maxHeap = new PriorityQueue<>(Comparator.reverseOrder());
        minHeap = new PriorityQueue<>(Integer::compareTo);
    }

    public void addNum(int num) {
        if (maxHeap.isEmpty() || maxHeap.peek() > num) {
            maxHeap.offer(num);
        } else {
            minHeap.offer(num);
        }

        int size1 = maxHeap.size();
        int size2 = minHeap.size();
        if (size1 - size2 > 1) {
            minHeap.offer(maxHeap.poll());
        } else if (size2 - size1 > 1) {
            maxHeap.offer(minHeap.poll());
        }
    }

    public double findMedian() {
        int size1 = maxHeap.size();
        int size2 = minHeap.size();

        return size1 == size2
            ? (maxHeap.peek() + minHeap.peek()) * 1.0 / 2
            : (size1 > size2 ? maxHeap.peek() : minHeap.peek());
    }
}Copy to clipboardErrorCopied
```

以上这种方法，需要把所有数据都加载到内存中。当数据量很大时，就不能这样了，因此，这种方法**适用于数据量较小的情况**。5 亿个数，每个数字占用 4B，总共需要 2G 内存。如果可用内存不足 2G，就不能使用这种方法了，下面介绍另一种方法。

#### 方法二：分治法

分治法的思想是把一个大的问题逐渐转换为规模较小的问题来求解。

对于这道题，顺序读取这 5 亿个数字，对于读取到的数字 num，如果它对应的二进制中最高位为 1，则把这个数字写到 f1 中，否则写入 f0 中。通过这一步，可以把这 5 亿个数划分为两部分，而且 f0 中的数都大于 f1 中的数（最高位是符号位）。

划分之后，可以非常容易地知道中位数是在 f0 还是 f1 中。假设 f1 中有 1 亿个数，那么中位数一定在 f0 中，且是在 f0 中，从小到大排列的第 1.5 亿个数与它后面的一个数的平均值。

> **提示**，5 亿数的中位数是第 2.5 亿与右边相邻一个数求平均值。若 f1 有一亿个数，那么中位数就是 f0 中从第 1.5 亿个数开始的两个数求得的平均值。

对于 f0 可以用次高位的二进制继续将文件一分为二，如此划分下去，直到划分后的文件可以被加载到内存中，把数据加载到内存中以后直接排序，找出中位数。

> **注意**，当数据总数为偶数，如果划分后两个文件中的数据有相同个数，那么中位数就是数据较小的文件中的最大值与数据较大的文件中的最小值的平均值。

### 方法总结

分治法，真香！

## 如何按照 query 的频度排序？

### 题目描述

有 10 个文件，每个文件大小为 1G，每个文件的每一行存放的都是用户的 query，每个文件的 query 都可能重复。要求按照 query 的频度排序。

### 解答思路

如果 query 的重复度比较大，可以考虑一次性把所有 query 读入内存中处理；如果 query 的重复率不高，那么可用内存不足以容纳所有的 query，这时候就需要采用分治法或其他的方法来解决。

#### 方法一：HashMap 法

如果 query 重复率高，说明不同 query 总数比较小，可以考虑把所有的 query 都加载到内存中的 HashMap 中。接着就可以按照 query 出现的次数进行排序。

#### 方法二：分治法

分治法需要根据数据量大小以及可用内存的大小来确定问题划分的规模。对于这道题，可以顺序遍历 10 个文件中的 query，通过 Hash 函数 `hash(query) % 10` 把这些 query 划分到 10 个小文件中。之后对每个小文件使用 HashMap 统计 query 出现次数，根据次数排序并写入到零外一个单独文件中。

接着对所有文件按照 query 的次数进行排序，这里可以使用归并排序（由于无法把所有 query 都读入内存，因此需要使用外排序）。

> 外部排序
>
> 1、算法描述
>   假设文件需要分成k块读入，需要从小到大进行排序。
>
> 1. 依次读入每个文件块，在内存中对当前文件块进行排序（应用恰当的内排序算法），此时，每块文件相当于一个由小到大排列的有序队列；
>
> 2. 在内存中建立一个最小堆，读入每块文件的队列头；
>
> 3. 弹出堆顶元素，如果元素来自第i块，则从第i块文件中补充一个元素到最小值堆。弹出的元素暂存至临时数组；
>
> 4. 当临时数组存满时，将数组写至磁盘，并清空数组内容；
>
> 5. 重复过程3、4，直至所有文件块读取完毕。

### 方法总结

- 内存若够，直接读入进行排序；
- 内存不够，先划分为小文件，小文件排好序后，整理使用外排序进行归并。

## 如何找出排名前 500 的数？

### 题目描述

有 20 个数组，每个数组有 500 个元素，并且有序排列。如何在这 20*500 个数中找出前 500 的数？

### 解答思路

对于 TopK 问题，最常用的方法是使用堆排序。对本题而言，假设数组降序排列，可以采用以下方法：

首先建立大顶堆，堆的大小为数组的个数，即为 20，把每个数组最大的值存到堆中。

接着删除堆顶元素，保存到另一个大小为 500 的数组中，然后向大顶堆插入删除的元素所在数组的下一个元素。

重复上面的步骤，直到删除完第 500 个元素，也即找出了最大的前 500 个数。

> 为了在堆中取出一个数据后，能知道它是从哪个数组中取出的，从而可以从这个数组中取下一个值，可以把数组的指针存放到堆中，对这个指针提供比较大小的方法。

```java
import lombok.Data;

import java.util.Arrays;
import java.util.PriorityQueue;

/**
 * @author https://github.com/yanglbme
 */
@Data
public class DataWithSource implements Comparable<DataWithSource> {
    /**
     * 数值
     */
    private int value;

    /**
     * 记录数值来源的数组
     */
    private int source;

    /**
     * 记录数值在数组中的索引
     */
    private int index;

    public DataWithSource(int value, int source, int index) {
        this.value = value;
        this.source = source;
        this.index = index;
    }

    /**
     *
     * 由于 PriorityQueue 使用小顶堆来实现，这里通过修改
     * 两个整数的比较逻辑来让 PriorityQueue 变成大顶堆
     */
    @Override
    public int compareTo(DataWithSource o) {
        return Integer.compare(o.getValue(), this.value);
    }
}

class Test {
    public static int[] getTop(int[][] data) {
        int rowSize = data.length;
        int columnSize = data[0].length;

        // 创建一个columnSize大小的数组，存放结果
        int[] result = new int[columnSize];

        PriorityQueue<DataWithSource> maxHeap = new PriorityQueue<>();
        for (int i = 0; i < rowSize; ++i) {
            // 将每个数组的最大一个元素放入堆中
            DataWithSource d = new DataWithSource(data[i][0], i, 0);
            maxHeap.add(d);
        }

        int num = 0;
        while (num < columnSize) {
            // 删除堆顶元素
            DataWithSource d = maxHeap.poll();
            result[num++] = d.getValue();
            if (num >= columnSize) {
                break;
            }

            d.setValue(data[d.getSource()][d.getIndex() + 1]);
            d.setIndex(d.getIndex() + 1);
            maxHeap.add(d);
        }
        return result;

    }

    public static void main(String[] args) {
        int[][] data = {
                {29, 17, 14, 2, 1},
                {19, 17, 16, 15, 6},
                {30, 25, 20, 14, 5},
        };

        int[] top = getTop(data);
        System.out.println(Arrays.toString(top)); // [30, 29, 25, 20, 19]
    }
}Copy to clipboardErrorCopied
```

### 方法总结

求 TopK，不妨考虑一下堆排序？