# Redis

## 1.什么是 Redis ？

**Re**mote **Di**ctionary **S**erver

![1](picture/中间件整理.assets/1.png)

## 2.Redis 优缺点

#### 优点

- **高性能**， **支持主从复制**
- **高并发**
- **支持数据持久化**，支持 AOF 和 RDB 两种持久化方式。
- **支持事务**，Redis 的所有操作都是原子性的，同时 Redis 还支持对几个操作合并后的原子性执行。
- **数据结构丰富**

#### 缺点

- 数据库 **容量受到物理内存的限制**，不能用作海量数据的高性能读写

**补充**

> - Redis **不具备自动容错和恢复功能**，主机从机的宕机都会导致前端部分读写请求失败，需要等待机器重启或者手动切换前端的 IP 才能恢复。
> - 主机宕机，宕机前有部分数据未能及时同步到从机，切换 IP 后还会引入数据不一致的问题，降低了 **系统的可用性**。
> - **Redis 较难支持在线扩容**，在集群容量达到上限时在线扩容会变得很复杂。为避免这一问题，运维人员在系统上线时必须确保有足够的空间，这对资源造成了很大的浪费。
>

## 3.为什么要用缓存？为什么使用 Redis？

#### 使用缓存的考虑

- [**命中率**]()，**业务数据常用吗？命中率如何？** 如果命中率很低，就没有必要写入缓存；
- [**写操作**]()，**该业务数据是读操作多，还是写操作多？** 如果写操作多，频繁需要写入数据库，也没有必要使用缓存；
- [**成本**]()，**业务数据大小如何？** 如果要存储几百兆字节的文件，会给缓存带来很大的压力，这样也没有必要；

## 4.使用缓存会出现什么问题？

### 1.缓存预热

#### 问题排查

- 请求数量较高
- 主从之间数据吞吐量较大，数据同步操作频度较高

#### 解决方案

- 前置准备工作：
  - 日常例行统计数据访问记录，统计访问频度较高的热点数据
- 准备工作：
  - 将统计结果中的数据分类，根据级别，redis优先加载级别较高的热点数据
  - 利用分布式多服务器同时进行数据读取，提速数据加载过程
  - 热点数据主从同时预热
- 实施：
  - 启动时触发数据预热过程，或者使用脚本触发数据预热（发送热点数据查询）
  - 如果条件允许，使用了CDN（内容分发网络），效果会更好

#### 总结

缓存预热就是系统启动前，提前将相关的缓存数据直接加载到缓存系统。避免在用户请求的时候，先查询数据库，然后再将数据缓存的问题！用户直接查询事先被预热的缓存数据！

### 2.缓存雪崩问题

![2](picture/中间件整理.assets/2.webp)

#### 问题分析

- 短时间范围内
- 大量key集中过期

#### 解决方案（道）

1. 构建**多级缓存架构** Nginx缓存+redis缓存+ehcache缓存
2. 检测Mysql严重耗时业务进行优化 对数据库的瓶颈排查：例如超时查询、耗时较高事务等
3. 灾难预警机制 监控redis服务器性能指标
   - CPU占用、CPU使用率
   - 内存容量
   - 查询平均响应时间
   - 线程数
4. 限流、降级 短时间范围内牺牲一些客户体验，限制一部分请求访问，降低应用服务器压力，待业务低速运转后再逐步放开访问

解决方案（术）

1. LRU与LFU切换
2. 数据有效期策略调整
   - 根据业务数据有效期进行**分类错峰**，A类90分钟，B类80分钟，C类70分钟
   - 过期时间使用固定时间+随机值的形式，**稀释**集中到期的key的数量
3. **超热**数据使用永久key
4. 定期维护（自动+人工） 对即将过期数据做访问量分析，确认是否延时，配合访问量统计，做热点数据的延时

#### 总结

缓存雪崩就是**瞬间过期数据量太大**，导致对数据库服务器造成压力。如能够**有效避免过期时间集中**，可以有效解决雪崩现象的出现 （约40%），配合其他策略一起使用，并监控服务器的运行数据，根据运行记录做快速调整。

另外对于 **"Redis 挂掉了，请求全部走数据库"** 这样的情况，我们还可以有如下的思路：[（重要）]()

- **事发前**：实现 Redis 的高可用(主从架构 + Sentinel 或者 Redis Cluster)，尽量避免 Redis 挂掉这种情况发生。
- **事发中**：万一 Redis 真的挂了，我们可以设置本地缓存(ehcache) + 限流(hystrix)，尽量避免我们的数据库被干掉(起码能保证我们的服务还是能正常工作的)
- **事发后**：Redis 持久化，重启后自动从磁盘上加载数据，快速恢复缓存数据。

### 3.缓存击穿

#### 问题分析

- 单个key高热数据
- key过期

#### 解决方案（术）

1. 预先设定-加大最近常用key的过期时长

   以电商为例，每个商家根据店铺等级，指定若干款主打商品，在购物节期间，**加大**此类信息key的**过期时长**

   注意：购物节不仅仅指当天，以及后续若干天，访问峰值呈现逐渐降低的趋势

2. 现场调整

   - 监控访问量，对自然流量激增的数据延长过期时间或设置为永久性key

3. 后台刷新数据

   - [**启动定时任务**]()，高峰期来临之前，刷新数据有效期，确保不丢失

4. 二级缓存

   - 设置不同的失效时间，保障不会被同时淘汰就行

5. 加锁 分布式锁，防止被击穿，但是要注意也是性能瓶颈，**慎重！**

#### 缓存穿透问题

![3](picture/中间件整理.assets/3-1615695264870.webp)

##### 原因

> 恶意请求
>
> 我们的数据库中的主键都是从0开始的，即使我们将数据库中的所有数据都放到了缓存中。当有人用id=-1来发生**恶意请求**时，**因为redis中没有这个数据，就会直接访问数据库，这就称谓缓存穿透**

##### 解决方案

**第一种是缓存空对象**

> 将数据库中的空值也缓存到缓存层中，这样查询该空值就不会再访问DB，而是直接在缓存层访问就行。
>
> 但是这样有个弊端就是缓存太多空值占用了更多的空间，可以通过给缓存层空值设立一个较短的过期时间来解决，例如60s。
>

**第二种是布隆过滤器**

> 我们可以提前将真实正确的商品Id，在添加完成之后便加入到过滤器当中，每次再进行查询时，先确认要查询的Id是否在过滤器当中，如果不在，则说明Id为非法Id，则不需要进行后续的查询步骤了。
>
> 当一个查询请求过来时，先经过布隆过滤器进行查，如果判断请求查询值存在，则继续查；如果判断请求查询不存在，直接丢弃。

### 4.缓存与数据库双写一致问题

![4](picture/中间件整理.assets/4-1615695254046.webp)

双写一致性上图还是稍微粗糙了些，你还需要知道两种方案 *(先操作数据库和先操作缓存)* 分别都有什么优势和对应的问题，这里不作赘述，可以参考一下下方的文章，写得非常详细。

面试前必须要知道的Redis面试题 | Java3y - [https://mp.weixin.qq.com/s/3Fmv7h5p2QDtLxc9n1dp5A](https://mp.weixin.qq.com/s?__biz=MzI4Njg5MDA5NA==&mid=2247484609&idx=1&sn=4c053236699fde3c2db1241ab497487b&scene=21#wechat_redirect)

## 5.Redis 为什么早期版本选择单线程？

#### 官方解释

因为 Redis 是基于内存的操作，**CPU 不是 Redis 的瓶颈**，Redis 的瓶颈最有可能是 **机器内存的大小** 或者 **网络带宽**。既然单线程容易实现，而且 CPU 不会成为瓶颈，那就顺理成章地采用单线程的方案了。

#### 简单总结一下

1. 使用单线程模型能带来更好的 **可维护性**，方便开发和调试；

   多线程模型虽然在某些方面表现优异，但是它却引入了程序执行顺序的不确定性，带来了**并发读写的一系列问题，增加了系统复杂度、同时可能存在线程切换、甚至加锁解锁、死锁造成的性能损耗**。

2. 使用单线程模型[**也能并发的处理客户端的请求**]()；*(I/O 多路复用机制)* ,而且处理性能非常高，因此没有必要使用多线程。

3. Redis 服务中运行的绝大多数操作的**性能瓶颈都不是 CPU**；

## 6.Redis6.0为什么要引入多线程？

### Redis的网络事件处理器

Redis基于Reactor模式开发了自己的网络事件处理器，这个处理器被称为文件事件处理器。

I/O多路复用程序负责监听多个套接字，并向文件事件分派器传送那些产生了事件的套接字。尽管多个文件事件可能会并发地出现，但I/O多路复用程序总是会将所有产生事件的套接字都放到一个队列里面，然后通过这个队列，以有序（sequentially）、同步（synchronously）、每次一个套接字的方式向文件事件分派器传送套接字。

当上一个套接字产生的事件被处理完毕之后（该套接字为事件所关联的事件处理器执行完毕），I/O多路复用程序才会继续向文件事件分派器传送下一个套接字。整个过程都在一个线程里完成，因此 Redis 被称为是单线程的操作。

![img](picture/中间件整理.assets/20210103213935324.png)

![img](picture/中间件整理.assets/20210103214048812.png)

### Redis6.0的多线程

Redis6 版本中引入了多线程。之前已经提到过 Redis 单线程处理有着很快的速度，那为什么还要引入多线程呢？单线程的瓶颈在什么地方？

先来看第二个问题，在 Redis 中，单线程的性能瓶颈主要在[**网络IO操作**]()上。也就是在读写网络的read/write系统调用执行期间会占用大部分CPU时间。

所以总结起来，Redis 支持多线程主要就是两个原因：

- 可以充分利用服务器 CPU 资源，目前主线程只能利用一个核。
- 多线程任务可以分摊 Redis 同步 IO 读写负荷。

Redis6.0的多线程是指，**将网络数据读写和协议解析通过多线程的方式来处理，对于命令执行来说，仍然使用单线程操作。**

也就是说，Redis6.0的多线程是为了解决其[网络IO]()的瓶颈。
主要是解决 IO 读写和计算分离的问题，而不是串行 读 + 计算 + 写

![img](picture/中间件整理.assets/v2-5cc79aa66caca62b3390d717270760c1_720w.jpg)

Round Robin(轮询)

![img](picture/中间件整理.assets/v2-3197beffebd110fd15e38c40747a3983_720w.jpg)

## 7.为什么 Redis 这么快?

简单总结：

1. **纯内存操作**：读取不需要进行磁盘 I/O，所以比传统数据库要快上不少；*(但不要有误区说磁盘就一定慢，例如 Kafka 就是使用磁盘顺序读取但仍然较快)*
2. **单线程，无锁竞争**：这保证了没有线程的上下文切换，不会因为多线程的一些操作而降低性能；
3. **多路 I/O 复用模型，非阻塞 I/O**：采用多路 I/O 复用技术可以让单个线程高效的处理多个网络连接请求
4. **高效的数据结构，加上底层做了大量优化**：Redis 对于底层的数据结构和内存占用做了大量的优化，例如不同长度的字符串使用不同的结构体表示，HyperLogLog 的密集型存储结构等等.

## 8.简述一下 Redis 常用数据结构及实现？

首先在 Redis 内部会使用一个 **RedisObject** 对象来表示所有的 `key` 和 `value`：

![5](picture/中间件整理.assets/5.webp)

其次 Redis 为了 **平衡空间和时间效率**，针对 `value` 的具体类型在底层会采用不同的数据结构来实现，下图展示了他们之间的映射关系：

![6](picture/中间件整理.assets/6.png)

1. [String:   ]()SDS
2. [List:   ]() ziplist+linkedlist
3. [Hash:   ]()哈希表+linkedlist
4. [Set:   ]()哈希表+instset
5. [ZSet:  ]()skiplist+ziplist

## 9.Redis 的 SDS 和 C 中字符串相比有什么优势？

#### 先简单总结一下

C 语言使用了一个长度为 `N+1` 的字符数组来表示长度为 `N` 的字符串，并且字符数组最后一个元素总是 `\0`，这种简单的字符串表示方式 **不符合 Redis 对字符串在安全性、效率以及功能方面的要求**。

#### 再来说 C 语言字符串的问题

这样简单的数据结构可能会造成以下一些问题：

- **获取字符串长度为 O(N) 级别的操作** → 因为 C 不保存数组的长度，每次都需要遍历一遍整个数组；
- 不能很好的杜绝 **缓冲区溢出/内存泄漏** 的问题 → 跟上述问题原因一样，如果执行拼接 or 缩短字符串的操作，如果操作不当就很容易造成上述问题；
- C 字符串 **只能保存文本数据** → 因为 C 语言中的字符串必须符合某种编码（比如 ASCII），例如中间出现的 `'\0'` 可能会被判定为提前结束的字符串而识别不了；

#### Redis 如何解决的 | SDS 的优势

如果去看 Redis 的源码 `sds.h/sdshdr` 文件，你会看到 SDS 完整的实现细节，这里简单来说一下 Redis 如何解决的：

1. **多[增加 len]() 表示当前字符串的长度**：这样就可以直接获取长度了，复杂度 O(1)；
2. [**自动扩展空间**]()：当 SDS 需要对字符串进行修改时，首先借助于 `len` 和 `alloc` 检查空间是否满足修改所需的要求，如果空间不够的话，SDS 会自动扩展空间，避免了像 C 字符串操作中的覆盖情况；
3. **有效降低内存分配次数**：C 字符串在涉及增加或者清除操作时会改变底层数组的大小造成重新分配，SDS 使用了 **空间预分配** 和 **惰性空间释放** 机制，简单理解就是[**每次在扩展时是成倍的多分配的，在缩容是也是先留着并不正式归还给 OS**]()；
4. **二进制安全**：C 语言字符串只能保存 `ascii` 码，对于图片、音频等信息无法保存，SDS 是二进制安全的，写入什么读取就是什么，不做任何过滤和限制；

## 10.字典是如何实现的？

除了 **hash** 结构的数据会用到字典外，整个 Redis 数据库的所有 `key` 和 `value` 也组成了一个 **全局字典**，还有带过期时间的 `key` 也是一个字典。*(存储在 RedisDb 数据结构中)*

#### 说明字典内部结构和 rehash

**Redis** 中的字典相当于 Java 中的 **HashMap**，内部实现也差不多类似，都是通过 **"数组 + 链表"** 的 **链地址法** 来解决部分哈希冲突

字典结构内部包含 **两个 hashtable**，通常情况下只有一个 `hashtable` 有值，但是在字典[**扩容缩容**]()时，需要分配新的 `hashtable`，然后进行 [**渐进式搬迁** *(rehash)*]()，这时候两个 `hashtable` 分别存储旧的和新的 `hashtable`，待搬迁结束后，旧的将被删除，新的 `hashtable` 取而代之。

#### 扩缩容的条件

正常情况下，当 hash 表中 **元素的个数等于第一维数组的长度时**，就会开始扩容，扩容的新数组是 **原数组大小的 2 倍**。不过如果 Redis 正在做 `bgsave(持久化命令)`，为了减少内存也得过多分离，Redis 尽量不去扩容，但是如果 hash 表非常满了，**达到了第一维数组长度的 5 倍了**，这个时候就会**强制扩容**。

当 hash 表因为元素逐渐被删除变得越来越稀疏时，Redis 会对 hash 表进行缩容来减少 hash 表的第一维数组空间占用。所用的条件是元素个数低于数组长度的 10%**，缩容不会考虑 Redis 是否在做 `bgsave`。

## 11.跳跃表是如何实现的？

### 跳跃表简介

跳跃表（skiplist）是一种随机化的数据结构，由 **William Pugh** 在论文《Skip lists: a probabilistic alternative to balanced trees》中提出，是一种可以与平衡树媲美的层次化链表结构——查找、删除、添加等操作都可以在对数期望时间下完成，以下是一个典型的跳跃表例子：

![image-20210314143541370](picture/中间件整理.assets/image-20210314143541370.png)

有一个叫做 **有序列表 zset** 的数据结构，它类似于 Java 中的 **SortedSet** 和 **HashMap** 的结合体，一方面它是一个 set 保证了内部 value 的唯一性，另一方面又可以给每个 value 赋予一个排序的权重值 score，来达到 **排序** 的目的。

它的内部实现就依赖了一种叫做 **「跳跃列表」** 的数据结构。

### 为什么使用跳跃表

首先，因为 [**zset 要支持随机的插入和删除**]()，所以它 **不宜使用数组来实现**，关于排序问题，我们也很容易就想到 **红黑树/ 平衡树** 这样的树形结构

1. **性能考虑：** 在高并发的情况下，树形结构需要执行一些类似于 rebalance 这样的可能涉及整棵树的操作，相对来说跳跃表的变化只涉及局部 ；
2. **实现考虑：** 在复杂度与红黑树相同的情况下，跳跃表实现起来更简单，看起来也更加直观；

#### 本质是解决查找问题

我们先来看一个普通的链表结构：

![image-20210314143733564](picture/中间件整理.assets/image-20210314143733564.png)

我们需要这个链表按照 score 值进行排序，这也就意味着，当我们需要添加新的元素时，我们需要定位到插入点，这样才可以继续保证链表是有序的，通常我们会使用 **二分查找法**，但二分查找是有序数组的，链表没办法进行位置定位，我们除了遍历整个找到第一个比给定数据大的节点为止 *（时间复杂度 O(n))* 似乎没有更好的办法。

但假如我们每相邻两个节点之间就增加一个指针，让指针指向下一个节点，如下图：

![image-20210314143857546](picture/中间件整理.assets/image-20210314143857546.png)

这样所有新增的指针连成了一个新的链表，但它包含的数据却只有原来的一半 *（图中的为 3，11）*。

现在假设我们想要查找数据时，可以根据这条新的链表查找，如果碰到比待查找数据大的节点时，再回到原来的链表中进行查找，比如，我们想要查找 7，查找的路径则是沿着下图中标注出的红色指针所指向的方向进行的：

![image-20210314144055864](picture/中间件整理.assets/image-20210314144055864.png)

这是一个略微极端的例子，但我们仍然可以看到，通过新增加的指针查找，我们不再需要与链表上的每一个节点逐一进行比较，这样改进之后需要比较的节点数大概只有原来的一半。

利用同样的方式，我们可以在新产生的链表上，继续为每两个相邻的节点增加一个指针，从而产生第三层链表：

![image-20210314144135865](picture/中间件整理.assets/image-20210314144135865.png)

在这个新的三层链表结构中，我们试着 **查找 13**，那么沿着最上层链表首先比较的是 11，发现 11 比 13 小，于是我们就知道只需要到 11 后面继续查找，**从而一下子跳过了 11 前面的所有节点。**

可以想象，当链表足够长，这样的多层链表结构可以帮助我们跳过很多下层节点，从而加快查找的效率。

### 更进一步的跳跃表

**跳跃表 skiplist** 就是受到这种多层链表结构的启发而设计出来的。按照上面生成链表的方式，上面每一层链表的节点个数，是下面一层的节点个数的一半，这样查找过程就非常类似于一个二分查找，使得查找的时间复杂度可以降低到 *O(logn)*。

但是，这种方法在插入数据的时候有很大的问题。新插入一个节点之后，就会打乱上下相邻两层链表上节点个数严格的 2:1 的对应关系。如果要维持这种对应关系，就必须把新插入的节点后面的所有节点 *（也包括新插入的节点）* 重新进行调整，这会让时间复杂度重新蜕化成 *O(n)*。删除数据也有同样的问题。

**skiplist** 为了避免这一问题，它不要求上下相邻两层链表之间的节点个数有严格的对应关系，而是 **为每个节点随机出一个层数(level)**。比如，一个节点随机出的层数是 3，那么就把它链入到第 1 层到第 3 层这三层链表中。为了表达清楚，下图展示了如何通过一步步的插入操作从而形成一个 skiplist 的过程：

![image-20210314144424270](picture/中间件整理.assets/image-20210314144424270.png)

从上面的创建和插入的过程中可以看出，每一个节点的层数（level）是随机出来的，而且新插入一个节点并不会影响到其他节点的层数，因此，**插入操作只需要修改节点前后的指针，而不需要对多个节点都进行调整**，这就降低了插入操作的复杂度。

现在我们假设从我们刚才创建的这个结构中查找 23 这个不存在的数，那么查找路径会如下图：

![3556547](picture/中间件整理.assets/3556547.png)

> 直观上期望的目标是 50% 的概率被分配到 `Level 1`，25% 的概率被分配到 `Level 2`，12.5% 的概率被分配到 `Level 3`，以此类推...有 2-63 的概率被分配到最顶层，因为这里每一层的晋升率都是 50%。
>
> 也就是第一层又0.5的概率跃迁到第二层，0.25跃迁到第三层

https://mp.weixin.qq.com/s?__biz=Mzg5MzU2NDgyNw==&mid=2247487151&idx=1&sn=30b017e0d25b80848e645fc77d772655&source=41#wechat_redirect

## 12.HyperLogLog 有了解吗？

**基数统计(Cardinality Counting)** [**通常是用来统计一个集合中不重复的元素个数**]()。

**思考这样的一个场景：** 如果你负责开发维护一个大型的网站，有一天老板找产品经理要网站上每个网页的 **UV(独立访客，每个用户每天只记录一次)**，然后让你来开发这个统计模块，你会如何实现？

如果统计 **PV(浏览量，用户点一次记录一次)**，那非常好办，给每个页面配置一个独立的 Redis 计数器就可以了，把这个计数器的 key 后缀加上当天的日期。这样每来一个请求，就执行 `INCRBY` 指令一次，最终就可以统计出所有的 **PV** 数据了。

但是 **UV** 不同，它要去重，**同一个用户一天之内的多次访问请求只能计数一次**。这就要求了每一个网页请求都需要带上用户的 ID，无论是登录用户还是未登录的用户，都需要一个唯一 ID 来标识。

你也许马上就想到了一个 *简单的解决方案*：那就是 **为每一个页面设置一个独立的 set 集合** 来存储所有当天访问过此页面的用户 ID。但这样的 **问题** 就是：

1. **存储空间巨大：** 如果网站访问量一大，你需要用来存储的 set 集合就会非常大，如果页面再一多.. 为了一个去重功能耗费的资源就可以直接让你 **老板打死你**；
2. **统计复杂：** 这么多 set 集合如果要聚合统计一下，又是一个复杂的事情；

我们来思考一个抛硬币的游戏：你连续掷 n 次硬币，然后说出其中**连续掷为正面的最大次数，我来猜你一共抛了多少次**。这很容易理解吧，例如：你说你这一次 *最多连续出现了 2 次* 正面，那么我就可以知道你这一次投掷的次数并不多，所以 *我可能会猜是 5* 或者是其他小一些的数字。这期间我可能会要求你重复实验，然后我得到了更多的数据之后就会估计得更准。**我们来把刚才的游戏换一种说法**：

<img src="picture/中间件.assets/2312413466.webp" alt="2312413466" style="zoom:67%;" />

这张图的意思是，我们给定一系列的随机整数，**记录下低位连续零位的最大长度 K**，即为图中的 `maxbit`，**通过这个 K 值我们就可以估算出随机数的数量 N**。

（HyperLogLog实际上不会存储每个元素的值，它使用的是概率算法，通过存储元素的hash值的第一个1的位置，来计算元素数量。）

## 13.布隆过滤器有了解吗？

上一次 我们学会了使用 **HyperLogLog** 来对大数据进行一个估算，它非常有价值，可以解决很多精确度不高的统计需求。但是如果我们想知道某一个值是不是已经在 **HyperLogLog** 结构里面了，它就无能为力了，它只提供了 `pfadd` 和 `pfcount` 方法，没有提供类似于 `contains` 的这种方法。

就举一个场景吧，比如你 **刷抖音**：

你有 **刷到过重复的推荐内容** 吗？这么多的推荐内容要推荐给这么多的用户，它是怎么保证每个用户在看推荐内容时，保证不会出现之前已经看过的推荐视频呢？也就是说，抖音是如何实现 **推送去重** 的呢？

你会想到服务器 **记录** 了用户看过的 **所有历史记录**，当推荐系统推荐短视频时会从每个用户的历史记录里进行 **筛选**，过滤掉那些已经存在的记录。问题是当 **用户量很大**，每个用户看过的短视频又很多的情况下，这种方式，推荐系统的去重工作 **在性能上跟的上么？**

实际上，如果历史记录存储在关系数据库里，去重就需要频繁地对数据库进行 `exists` 查询，当系统并发量很高时，数据库是很难抗住压力的。

你可能又想到了 **缓存**，但是这么多用户这么多的历史记录，如果全部缓存起来，那得需要 **浪费多大的空间** 啊

![image-20210314151242162](picture/中间件整理.assets/image-20210314151242162.png)

如上图所示，**布隆过滤器(Bloom Filter)** 就是这样一种专门用来解决去重问题的高级数据结构。但是跟 **HyperLogLog** 一样，它也一样有那么一点点不精确，也存在一定的误判概率，但它能在解决去重的同时，在 **空间上能节省 90%** 以上，也是非常值得的。

### 布隆过滤器是什么

**布隆过滤器（Bloom Filter）**  **实际上** 是一个很长的二进制向量和一系列随机映射函数，实际上你也可以把它 **简单理解** 为一个不怎么精确的 **set** 结构，当你使用它的 `contains` 方法判断某个对象是否存在时，它可能会误判。但是布隆过滤器也不是特别不精确，只要参数设置的合理，它的精确度可以控制的相对足够精确，只会有小小的误判概率。

> 当布隆过滤器说某个值存在时，这个值 **可能不存在**；当它说不存在时，那么 **一定不存在**。打个比方，当它说不认识你时，那就是真的不认识，但是当它说认识你的时候，可能是因为你长得像它认识的另外一个朋友 *(脸长得有些相似)*，所以误判认识你。

### 布隆过滤器的使用场景

基于上述的功能，我们大致可以把布隆过滤器用于以下的场景之中：

- **大数据判断是否存在**：这就可以实现出上述的去重功能，如果你的服务器内存足够大的话，那么使用 HashMap 可能是一个不错的解决方案，理论上时间复杂度可以达到 O(1)的级别，但是当数据量起来之后，还是只能考虑布隆过滤器。
- **解决缓存穿透**：我们经常会把一些热点数据放在 Redis 中当作缓存，例如产品详情。通常一个请求过来之后我们会先查询缓存，而不用直接读取数据库，这是提升性能最简单也是最普遍的做法，但是 **如果一直请求一个不存在的缓存**，那么此时一定不存在缓存，那就会有 **大量请求直接打到数据库** 上，造成 **缓存穿透**，布隆过滤器也可以用来解决此类问题。
- **爬虫/ 邮箱等系统的过滤**：平时不知道你有没有注意到有一些正常的邮件也会被放进垃圾邮件目录中，这就是使用布隆过滤器 **误判** 导致的。

### 布隆过滤器原理解析

布隆过滤器 **本质上** 是由长度为 `m` 的位向量或位列表（仅包含 `0` 或 `1` 位值的列表）组成，最初所有的值均设置为 `0`，所以我们先来创建一个稍微长一些的位向量用作展示：

![image-20210314151605593](picture/中间件整理.assets/image-20210314151605593.png)

当我们向布隆过滤器中添加数据时，会使用 **多个** `hash` 函数对 `key` 进行运算，算得一个证书索引值，然后对位数组长度进行取模运算得到一个位置，每个 `hash` 函数都会算得一个不同的位置。再把位数组的这几个位置都置为 `1` 就完成了 `add` 操作，例如，我们添加一个 `wmyskxz`：

![image-20210314151642755](picture/中间件整理.assets/image-20210314151642755.png)

向布隆过滤器查查询 `key` 是否存在时，跟 `add` 操作一样，会把这个 `key` 通过相同的多个 `hash` 函数进行运算，查看 **对应的位置** 是否 **都** 为 `1`，**只要有一个位为 `0`**，那么说明布隆过滤器中这个 `key` 不存在。如果这几个位置都是 `1`，并不能说明这个 `key` 一定存在，只能说极有可能存在，因为这些位置的 `1` 可能是因为其他的 `key` 存在导致的。

就比如我们在 `add` 了一定的数据之后，查询一个 **不存在** 的 `key`：

![image-20210314151720332](picture/中间件整理.assets/image-20210314151720332.png)

## 14.压缩列表了解吗？

这是 Redis **为了节约内存** 而使用的一种数据结构，**zset** 和 **hash** 容器对象会在元素个数较少的时候，采用压缩列表（ziplist）进行存储。压缩列表是 **一块连续的内存空间**，元素之间紧挨着存储，没有任何冗余空隙。

 听到“压缩”两个字，直观的反应就是节省内存。之所以说这种存储结构节省内存,是相较于数组的存储思路而言的。我们知道,数组要求每个元素的大小相同,如果我们要存储不同长度的字符串,那我们就需要用最大长度的字符串大小作为元素的大小(假设是20个字节)。存储小于 20 个字节长度的字符串的时候，便会浪费部分存储空间。

![image-20210314152900621](picture/中间件整理.assets/image-20210314152900621.png)

数组的优势占用一片连续的空间可以很好的利用CPU缓存访问数据。如果我们想要保留这种优势，又想节省存储空间我们可以对数组进行压缩。

![image-20210314152920295](picture/中间件整理.assets/image-20210314152920295.png)

 但是这样有一个问题，我们在遍历它的时候由于不知道每个元素的大小是多少，因此也就无法计算出下一个节点的具体位置。这个时候我们可以给每个节点增加一个length的属性。

![image-20210314152940448](picture/中间件整理.assets/image-20210314152940448.png)

 如此。我们在遍历节点的之后就知道每个节点的长度(占用内存的大小)，就可以很容易计算出下一个节点再内存中的位置。这种结构就像一个简单的压缩列表了。

## 15.快速列表 quicklist 了解吗？

Redis 早期版本存储 list 列表数据结构使用的是压缩列表 ziplist 和普通的双向链表 linkedlist，也就是说当元素少时使用 ziplist，当元素多时用 linkedlist。但考虑到链表的附加空间相对较高，`prev` 和 `next` 指针就要占去 `16` 个字节（64 位操作系统占用 `8` 个字节），另外每个节点的内存都是单独分配，会加剧内存的碎片化，影响内存管理效率。

后来 Redis 新版本（3.2）对列表数据结构进行了改造，使用 `quicklist` 代替了 `ziplist` 和 `linkedlist`。

quickList是一个ziplist组成的双向链表。每个节点使用ziplist来保存数据。
本质上来说，quicklist里面保存着一个一个小的ziplist。结构如下：

<img src="picture/中间件.assets/v2-800dbf77bba29897de1ad769d0149f8f_720w.jpg" alt="img" style="zoom: 50%;" />

## 16.什么是持久化？

#### 解释一下持久化发生了什么

我们来稍微考虑一下 **Redis** 作为一个 **"内存数据库"** 要做的关于持久化的事情。通常来说，从客户端发起请求开始，到服务器真实地写入磁盘，需要发生如下几件事情：

![5453566](picture/中间件整理.assets/5453566.webp)

**详细版** 的文字描述大概就是下面这样：

1. 客户端向数据库 **发送写命令** *(数据在客户端的内存中)*
2. 数据库 **接收** 到客户端的 **写请求** *(数据在服务器的内存中)*
3. 数据库 **调用系统 API** 将数据写入磁盘 *(数据在内核缓冲区中)*
4. 操作系统将 **写缓冲区** 传输到 **磁盘控控制器** *(数据在磁盘缓存中)*
5. 操作系统的磁盘控制器将数据 **写入实际的物理媒介** 中 *(数据在磁盘中)*

#### 分析如何保证持久化安全

如果我们故障仅仅涉及到 **软件层面** *(该进程被管理员终止或程序崩溃)* 并且没有接触到内核，那么在 *上述步骤 3* 成功返回之后，我们就认为成功了。即使进程崩溃，操作系统仍然会帮助我们把数据正确地写入磁盘。

如果我们考虑 **停电/ 火灾** 等 **更具灾难性** 的事情，那么只有在完成了第 **5** 步之后，才是安全的。

## 17.Redis 中的两种持久化方式？

### **RDB机制**

RDB就是把数据以快照的形式保存在磁盘上。快照可以理解成把当前时刻的数据拍成一张照片保存下来。

> RDB持久化是**指在指定的时间间隔内将内存中的数据集快照写入磁盘**。也是默认的持久化方式，这种方式是就是将内存中数据以快照的方式写入到二进制文件中,默认的文件名为dump.rdb。

在我们安装了redis之后，所有的配置都是在redis.conf文件中，里面保存了RDB和AOF两种持久化机制的各种配置。

既然RDB机制是通过把某个时刻的所有数据生成一个快照来保存，那么就应该有一种触发机制，是实现这个过程。对于RDB来说，提供了三种机制：save、bgsave、自动化。我们分别来看一下

**1、save触发方式**

该命令会阻塞当前Redis服务器，执行save命令期间，Redis不能处理其他命令，直到RDB过程完成为止。具体流程如下：

![img](picture/中间件整理.assets/e7cd7b899e510fb3aa8c05042b22c093d0430ca7.jpeg)

执行完成时候如果存在老的RDB文件，就把新的替代掉旧的。我们的客户端可能都是几万或者是几十万，这种方式显然不可取。

**2、bgsave触发方式**

执行该命令时，Redis会在后台异步进行快照操作，同时还可以响应客户端请求。具体流程如下：

![img](picture/中间件整理.assets/023b5bb5c9ea15cefb035bc8431132f53b87b21e.jpeg)

> 具体操作是Redis进程执行fork操作创建子进程，RDB持久化过程由子进程负责，完成后自动结束。
>
> 阻塞只发生在fork阶段，一般时间很短。基本上 Redis 内部所有的RDB操作都是采用 bgsave 命令。

**3、自动触发**

自动触发是由我们的配置文件来完成的。在redis.conf配置文件中，里面有如下配置，我们可以去设置：

**①save：**这里是用来配置触发 Redis的 RDB 持久化条件，也就是什么时候将内存中的数据保存到硬盘。比如“save m n”。表示m秒内数据集存在n次修改时，自动触发bgsave。

默认如下配置：

\#表示900 秒内如果至少有 1 个 key 的值变化，则保存save 900 1#表示300 秒内如果至少有 10 个 key 的值变化，则保存save 300 10#表示60 秒内如果至少有 10000 个 key 的值变化，则保存save 60 10000

不需要持久化，那么你可以注释掉所有的 save 行来停用保存功能。

![img](picture/中间件整理.assets/1c950a7b02087bf43b4490d50ac25f2a11dfcf7e.jpeg)

**4、RDB 的优势和劣势**

①、优势

（1）RDB[**文件紧凑，全量备份**]()，非常适合用于进行备份和灾难恢复。

（2）生成RDB文件的时候，redis主进程会fork()一个子进程来处理所有保存工作，主进程不需要进行任何磁盘IO操作。

（3）**RDB 在恢复大数据集时的速度比 AOF 的恢复速度要快。**

②、劣势

子进程会拥有父进程的内存数据。**但是父进程修改内存子进程不会反应出来，所以在[快照持久化期间]()修改的数据不会被保存，可能丢失数据。**

### AOF机制

全量备份总是耗时的，有时候我们提供一种更加高效的方式AOF

redis会将每一个收到的写命令都通过write函数追加到文件中。

**1、持久化原理**

他的原理看下面这张图：

![img](picture/中间件整理.assets/32fa828ba61ea8d3c2502e396b1b3848251f58b0.jpeg)

每当有一个写命令过来时，就直接保存在我们的AOF文件中。

**2、文件重写原理**

AOF的方式也同时带来了另一个问题。持久化文件会变的越来越大。为了压缩aof的持久化文件。redis提供了bgrewriteaof命令。将内存中的数据以命令的方式保存到临时文件中，同时会fork出一条新进程来将文件重写。

![img](picture/中间件整理.assets/09fa513d269759ee28454d2c4cea4b106c22dfd3.jpeg)

> 重写aof文件的操作，并没有读取旧的aof文件，而是[将整个内存中的数据库内容用命令的方式重写了一个新的aof文件]()。

**3、AOF也有三种触发机制**

（1）每修改同步always：同步持久化 每次发生数据变更会被立即记录到磁盘 性能较差但数据完整性比较好

（2）每秒同步everysec：异步操作，每秒记录 如果一秒内宕机，有数据丢失

（3）不同步no：从不同步

![img](picture/中间件整理.assets/b17eca8065380cd7df69859ba056a5325982816c.jpeg)

**4、优点**

> （1）AOF可以更好的**保护数据不丢失**，一般AOF会每隔1秒，通过一个后台线程执行一次fsync操作，最多丢失1秒钟的数据。
>
> （2）AOF日志文件没有任何磁盘寻址的开销，**写入性能非常高**，文件不容易破损。
>
> （3）AOF日志文件**即使过大的时候**，出现后台重写操作，也不会影响客户端的读写。
>
> （4）**AOF日志文件的命令通过非常可读的方式进行记录，这个特性非常适合做灾难性的误删除的紧急恢复**。比如某人不小心用flushall命令清空了所有数据，只要这个时候后台rewrite还没有发生，那么就可以立即拷贝AOF文件，将最后一条flushall命令给删了，然后再将该AOF文件放回去，就可以通过恢复机制，自动恢复所有数据

**5、缺点**

> （1）对于同一份数据来说[，AOF日志文件通常比RDB数据快照文件更大]()
>
> （2）AOF开启后，[**支持的写QPS会比RDB支持的写QPS低**]()，因为AOF一般会配置成每秒fsync一次日志文件，当然，每秒一次fsync，性能也还是很高的

**四、RDB和AOF到底该如何选择**

选择的话，两者加一起才更好。因为两个持久化机制你明白了，剩下的就是看自己的需求了，需求不同选择的也不一定，但是通常都是结合使用。有一张图可供总结：

![img](picture/中间件整理.assets/8326cffc1e178a82c532308ef2117b8ba977e8ae.jpeg)

### Redis 4.0 的混合持久化

重启 Redis 时，我们很少使用 `rdb` 来恢复内存状态，因为会丢失大量数据。我们通常使用 AOF 日志重放，但是重放 AOF 日志性能相对 `rdb` 来说要慢很多，这样在 Redis 实例很大的情况下，启动需要花费很长的时间。

**Redis 4.0** 为了解决这个问题，带来了一个新的持久化选项——**混合持久化**。将 `rdb` 文件的内容和增量的 AOF 日志文件存在一起。这里的 AOF 日志不再是全量的日志，**而是[自持久化开始到持久化结束]() 的这段时间发生的增量 AOF 日志，通常这部分 AOF 日志很小：**

![324324325](picture/中间件整理.assets/324324325.webp)

于是在 Redis 重启的时候，可以先加载 `rdb` 的内容，然后再重放增量 AOF 日志就可以完全替代之前的 AOF 全量文件重放，重启效率因此大幅得到提升。

## 18.主从同步了解吗？

![342343257](picture/中间件整理.assets/342343257.webp)

**主从复制**，是指将一台 Redis 服务器的数据，复制到其他的 Redis 服务器。

Redis 主从复制支持 **主从同步** 和 **从从同步** 两种。

#### 主从复制主要的作用

- **数据冗余：** 主从复制实现了数据的热备份，是持久化之外的一种数据冗余方式。
- **故障恢复：** 当主节点出现问题时，可以由从节点提供服务，实现快速的故障恢复 *(实际上是一种服务的冗余)*。
- **读写分离：** 可以由主节点提供写服务，由从节点提供读服务 ，分担服务器负载。
- **高可用基石：**主从复制还是哨兵和集群能够实施的 **基础**

#### 实现原理

![45346579](picture/中间件整理.assets/45346579.webp)

## 19.Redis 过期键的删除策略？

#### 简单描述

先抛开 Redis 想一下几种可能的删除策略：

1. **定时删除**:在设置键的过期时间的同时，创建一个定时器 timer). 让定时器在键的过期时间来临时，立即执行对键的删除操作。
2. **惰性删除**:放任键过期不管，但是每次访问这个key就会对过期时间进行检查，如果过期就立即删除。
3. **定期删除**:每隔一段时间程序就对数据库进行一次检查，删除里面的过期键。至于要删除多少过期键，以及要检查多少个数据库，则由算法决定。

在上述的三种策略中定时删除和定期删除属于不同时间粒度的 **主动删除**，惰性删除属于 **被动删除**。

#### 三种策略都有各自的优缺点

1. 定时删除对内存使用率有优势，但是对 CPU 不友好；
2. 惰性删除对内存不友好，如果某些键值对一直不被使用，那么会造成一定量的内存浪费；
3. 定期删除是定时删除和惰性删除的折中。

#### Redis 中的实现

Reids 采用的是 **惰性删除和定时删除** 的结合。[redis会将每个设置了过期时间的key放入一个独立的字典中，以后会定时遍历这个字典来删除到期的key。除了定时遍历，还会使用惰性删除。]()

定时扫描是一种贪心策略：

1.从过期字典随机取出20个key 

2.删除其中已经过期的key 

3.如果过期的key比例超过1/4就重复

## 20.Redis 的淘汰策略有哪些？

#### Redis 有六种淘汰策略

lru需要一个链表，最常使用的放在最前面，最不常使用的放在最后面。一经使用直接放到最前面

| 策略            | 描述                                                         |
| :-------------- | :----------------------------------------------------------- |
| volatile-lru    | 从已设置过期时间的 KV 集中优先对最近最少使用(less recently used)的数据淘汰 |
| volitile-ttl    | 从已设置过期时间的 KV 集中马上要过期的(time to live)的数据淘汰 |
| volitile-random | 从已设置过期时间的 KV 集中随机选择数据淘汰                   |
| allkeys-lru     | 从所有 KV 集中优先对最近最少使用(less recently used)的数据淘汰 |
| allKeys-random  | 从所有 KV 集中随机选择数据淘汰                               |
| noeviction      | 不淘汰策略，若超过最大内存，返回错误信息                     |



#### 4.0 版本后增加以下两种

- volatile-lfu：从已设置过期时间的数据集(server.db[i].expires)中挑选最不经常使用的数据淘汰
- allkeys-lfu：当内存不足以容纳新写入数据时，在键空间中，移除最不经常使用的 key

## 21.假如Redis里面有1亿个key，其中有10w个key是以某个固定的已知的前缀开头的，如何将它们全部找出来？

使用 `keys` 指令可以扫出指定模式的 key 列表。但是要注意 keys 指令会导致线程阻塞一段时间，线上服务会停顿，直到指令执行完毕，服务才能恢复。

这个时候可以使用 `scan` 指令，`scan` 指令可以无阻塞的提取出指定模式的 `key` 列表，但是会有一定的重复概率，在客户端做一次去重就可以了，但是整体所花费的时间会比直接用 `keys` 指令长。

## 22.你知道分布式锁吗

首先分布式锁和我们平常讲到的锁原理基本一样，目的就是确保，在多个线程并发时，只有一个线程在同一刻操作这个业务或者说方法、变量。

在一个进程中，也就是一个jvm 或者说应用中，我们很容易去处理控制，在jdk java.util 并发包中已经为我们提供了这些方法去加锁， 比如synchronized 关键字 或者Lock 锁，都可以处理。

说到分布式锁的实现，还是有很多的，有数据库方式的，有redis分布式锁，有zookeeper分布式锁等等

### 基于数据库实现分布式锁

#### 基于数据库表

要实现分布式锁，最简单的方式可能就是直接创建一张锁表，然后通过操作该表中的数据来实现了。

当我们要锁住某个方法或资源时，我们就在该表中增加一条记录，想要释放锁的时候就删除这条记录。

创建这样一张数据库表：

```sql
CREATE TABLE `methodLock` (
  `id` int(11) NOT NULL AUTO_INCREMENT COMMENT '主键',
  `method_name` varchar(64) NOT NULL DEFAULT '' COMMENT '锁定的方法名',
  `desc` varchar(1024) NOT NULL DEFAULT '备注信息',
  `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '保存数据时间，自动生成',
  PRIMARY KEY (`id`),
  UNIQUE KEY `uidx_method_name` (`method_name `) USING BTREE
) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT='锁定中的方法';
```

当我们想要锁住某个方法时，执行以下SQL：

```sql
insert into methodLock(method_name,desc) values (‘method_name’,‘desc’)
```

因为我们对`method_name`做了唯一性约束，这里如果有多个请求同时提交到数据库的话，数据库会保证只有一个操作可以成功，那么我们就可以认为操作成功的那个线程获得了该方法的锁，可以执行方法体内容。

当方法执行完毕之后，想要释放锁的话，需要执行以下Sql:

```sql
delete from methodLock where method_name ='method_name'
```

上面这种简单的实现有以下几个问题：

> 1、这把锁**强依赖数据库的可用性**，数据库是一个单点，一旦数据库挂掉，会导致业务系统不可用。
>
> 2、这把锁**没有失效时间**，一旦解锁操作失败，就会导致锁记录一直在数据库中，其他线程无法再获得到锁。
>
> 3、这把锁只能是**非阻塞的**，因为数据的insert操作，一旦插入失败就会直接报错。没有获得锁的线程并不会进入排队队列，要想再次获得锁就要再次触发获得锁操作。
>
> 4、这把锁是**非重入的**，同一个线程在没有释放锁之前无法再次获得该锁。因为数据中数据已经存在了。

当然，我们也可以有其他方式解决上面的问题。

- 数据库是单点？搞两个数据库，数据之前双向同步。一旦挂掉快速切换到备库上。
- 没有失效时间？只要做一个定时任务，每隔一定时间把数据库中的超时数据清理一遍。
- 非阻塞的？搞一个while循环，直到insert成功再返回成功。
- 非重入的？在数据库表中加个字段，**记录当前获得锁的机器的主机信息和线程信息**，那么下次再获取锁的时候先查询数据库，如果当前机器的主机信息和线程信息在数据库可以查到的话，直接把锁分配给他就可以了。

------

#### 基于数据库排他锁

除了可以通过增删操作数据表中的记录以外，其实还可以借助数据中自带的锁来实现分布式的锁。

我们还用刚刚创建的那张数据库表。可以通过数据库的排他锁来实现分布式锁。 基于MySql的InnoDB引擎，可以使用以下方法来实现加锁操作：

```java
public boolean lock(){
    connection.setAutoCommit(false)
    while(true){
        try{
            result = select * from methodLock where method_name=xxx for update;
            if(result==null){
                return true;
            }
        }catch(Exception e){

        }
        sleep(1000);
    }
    return false;
}
```

在查询语句后面增加`for update`，数据库会在查询过程中给数据库表增加排他锁.

> 这里再多提一句，InnoDB引擎在加锁的时候，只有通过索引进行检索的时候才会使用行级锁，否则会使用表级锁。这里我们希望使用行级锁，就要给method_name添加索引，值得注意的是，这个索引一定要创建成唯一索引，否则会出现多个重载方法之间无法同时被访问的问题。重载方法的话建议把参数类型也加上。

当某条记录被加上排他锁之后，其他线程无法再在该行记录上增加排他锁。

我们可以认为获得排它锁的线程即可获得分布式锁，当获取到锁之后，可以执行方法的业务逻辑，执行完方法之后，再通过以下方法解锁：

```java
public void unlock(){
    connection.commit();
}
```

通过`connection.commit()`操作来释放锁。

这种方法可以有效的解决上面提到的无法释放锁和阻塞锁的问题。

- 阻塞锁？ `for update`语句会在执行成功后立即返回，在执行失败时一直处于阻塞状态，直到成功。
- 锁定之后服务宕机，无法释放？使用这种方式，服务宕机之后数据库会自己把锁释放掉。

但是还是无法直接解决数据库单点和可重入问题。

**这里还可能存在另外一个问题，虽然我们对`method_name` 使用了唯一索引，并且显示使用`for update`来使用行级锁。但是，MySql会对查询进行优化，即便在条件中使用了索引字段，但是否使用索引来检索数据是由 MySQL 通过判断不同执行计划的代价来决定的，如果 MySQL 认为全表扫效率更高，比如对一些很小的表，它就不会使用索引，这种情况下 InnoDB 将使用表锁，而不是行锁。如果发生这种情况就悲剧了。**

------

**还有一个问题，就是我们要使用排他锁来进行分布式锁的lock，那么一个排他锁长时间不提交，就会占用数据库连接。一旦类似的连接变得多了，就可能把数据库连接池撑爆**

#### 总结

总结一下使用数据库来实现分布式锁的方式，这两种方式都是依赖数据库的一张表，一种是通过表中的记录的存在情况确定当前是否有锁存在，另外一种是通过数据库的排他锁来实现分布式锁。

**数据库实现分布式锁的优点**

- 直接借助数据库，容易理解。

**数据库实现分布式锁的缺点**

- 会有各种各样的问题，在解决问题的过程中会使整个方案变得越来越复杂。
- 操作数据库需要一定的开销，性能问题需要考虑。
- 使用数据库的行级锁并不一定靠谱，尤其是当我们的锁表并不大的时候。

### redis实现分布式锁

首先redis是单线程的，这里的单线程指的是网络请求模块使用了一个线程（所以不需考虑并发安全性），即一个线程处理所有网络请求，其他模块仍用了多个线程。

在实际的操作中过程大致是这样子的：

服务器1要去访问发红包的妹子，也就是redis，那么他会在redis中通过"setnx key value" 操作设置一个key 进去，value是啥不重要，重要的是要有一个key，也就是一个标记，而且这个key你爱叫啥叫啥，只要所有的服务器设置的key相同就可以。

假设我们设置一个，如下图

![img](picture/中间件整理.assets/1063420-20190308120715682-477272935.png)

那么我们可以看到会返回一个1，那就代表了成功。

如果再来一个请求去设置同样的key，如下图：

![img](picture/中间件整理.assets/1063420-20190308120908502-1662769569.png)

这个时候会返回0，那就代表失败了。

那么我们就可以通过这个操作去判断是不是当前可以拿到锁，或者说可以去访问“负责发红包的妹子”，如果返回1，那我就开始去执行后面的逻辑，如果返回0，那就说明已经被人占用了，我就要继续等待。

当服务器1拿到锁之后，进行了业务处理，完成后，还需要释放锁，如下图所示：

![img](picture/中间件整理.assets/1063420-20190308141949946-1555008493.png)

删除成功返回1，那么其他的服务器就可以继续重复上面的步骤去设置这个key，以达到获取锁的目的。

当然以上的操作是在redis客户端直接进行的，通过程序调用的话，肯定就不能这么写，比如java 就需要通过jedis 去调用，但是整个处理逻辑基本都是一样的

通过上面的方式，我们好像是解决了分布式锁的问题，但是想想还有没有什么问题呢？？

 对，问题还是有的，可能会有死锁的问题发生，比如服务器1设置完之后，获取了锁之后，忽然发生了宕机。

那后续的删除key操作就没法执行，这个key会一直在redis中存在，其他服务器每次去检查，都会返回0，他们都会认为有人在使用锁，我需要等。

为了解决这个死锁的问题，我们就就需要给key 设置有效期了。

 设置的方式有2种

1，第一种就是在set完key之后，直接设置[**key的有效期 "expire key timeout"**]() ，为key设置一个超时时间，单位为second，超过这个时间锁会自动释放，避免死锁。

这种方式相当于，把锁持有的有效期，交给了redis去控制。如果时间到了，你还没有给我删除key，那redis就直接给你删了，其他服务器就可以继续去setnx获取锁。

2，第二种方式，[**就是把删除key权利交给其他的服务器，那这个时候就需要用到value值了**]()，

比如服务器1，设置了value 也就是 timeout 为 当前时间+10秒 ，这个时候服务器2 通过get 发现时间已经超过系统当前时间了，那就说明服务器1没有释放锁，服务器1可能出问题了，

服务器2就开始执行删除key操作，并且继续执行setnx 操作。

但是这块有一个问题，也就是，不光你服务器2可能会发现服务器1超时了，服务器3也可能会发现，如果刚好，服务器2，setnx操作完成，服务器3就接着删除，是不是服务器3也可以setnx成功了？

那就等于是服务器2和服务器3都拿到锁了，那就问题大了。这个时候怎么办呢？

这个时候需要用到 [**“GETSET key value”**]() 命令了。这个命令的意思就是获取当前key的值，并且设置新的值。

假设服务器2发现key过期了，开始调用 getset 命令，然后用获取的时间判断是否过期，如果获取的时间仍然是过期的，那就说明拿到锁了。

如果没有，则说明在服务2执行getset之前，服务器3可能也发现锁过期了，并且在服务器2之前执行了getset操作，重新设置了过期时间。

那么服务器2就需要放弃后续的操作，继续等待服务器3释放锁或者去监测key的有效期是否过期。

这块其实有一个小问题是，服务器3已经修改了有效期，拿到锁之后，服务器2，也修改了有效期，但是没能拿到锁，但是这个有效期的时间已经被在服务器3的基础上有增加一些，但是这种影响其实还是很小的，几乎可以忽略不计。

------

#### 总结

可以使用缓存来代替数据库来实现分布式锁，这个可以提供更好的性能，同时，很多缓存服务都是集群部署的，可以避免单点问题。并且很多缓存服务都提供了可以用来实现分布式锁的方法，比如Tair的put方法，redis的setnx方法等。并且，这些缓存服务也都提供了对数据的过期自动删除的支持，可以直接设置超时时间来控制锁的释放。

**使用缓存实现分布式锁的优点**

性能好，实现起来较为方便。

**使用缓存实现分布式锁的缺点**

[**通过超时时间来控制锁的失效时间并不是十分的靠谱。如何设置的失效时间太短，方法没等执行完，锁就自动释放了**]()，那么就会产生并发问题。

如果[**设置的时间太长，其他获取锁的线程就可能要平白的多等一段时间**]()。这个问题使用数据库实现分布式锁同样存在

### zookeeper实现分布式锁

百度百科是这么介绍的：ZooKeeper是一个分布式的，开放源码的分布式应用程序协调服务，是Google的Chubby一个开源的实现，是Hadoop和Hbase的重要组件。

那对于我们初次认识的人，可以理解成ZooKeeper就像是我们的电脑文件系统，我们可以在d盘中创建文件夹a，并且可以继续在文件夹a中创建文件夹a1，a2。

那我们的文件系统有什么特点？？那就是同一个目录下文件名称不能重复，同样ZooKeeper也是这样的。

 在ZooKeeper所有的节点，也就是文件夹称作 Znode，而且这个Znode节点是可以存储数据的。

我们可以通过“ create /zkjjj nice” 来创建一个节点，这个命令就表示，在跟目录下创建一个zkjjj的节点，值是nice。同样这里的值，和我在前面说的redis中的一样，没什么意义，你随便给。

另外ZooKeeper可以创建4种类型的节点，分别是：

1，持久性节点

2，持久性顺序节点

3，临时性节点

4，临时性顺序节点

首先说下持久性节点和临时性节点的区别，持久性节点表示只要你创建了这个节点，那不管你ZooKeeper的客户端是否断开连接，ZooKeeper的服务端都会记录这个节点。

临时性节点刚好相反，一旦你ZooKeeper客户端断开了连接，那ZooKeeper服务端就不再保存这个节点。

再说下顺序性节点，顺序性节点是指，在创建节点的时候，ZooKeeper会自动给节点编号比如0000001 ，0000002 这种的。

最后说下，[**zookeeper有一个监听机制，客户端注册监听它关心的目录节点，当目录节点发生变化（数据改变、被删除、子目录节点增加删除）等，zookeeper会通知客户端。**]()

下面我们继续结合我们上面的分红包场景，描述下在zookeeper中如何加锁。

假设服务器1，创建了一个节点 /zkjjj ,成功了，那服务器1就获取了锁，服务器2再去创建相同的锁，那么他就会失败，这个时候他就就只能监听这个节点的变化。

等到服务器1，处理完业务，删除了节点后，他就会得到通知，然后去创建同样的节点，获取锁处理业务，再删除节点，后续的100台服务器与之类似

注意这里的100台服务器并不是挨个去执行上面的创建节点的操作，而是并发的，当服务器1创建成功，那么剩下的99个就都会注册监听这个节点，等通知，以此类推。

 但是大家有没有注意到，这里还是有问题的，还是会有死锁的情况存在，对不对？

当服务器1创建了节点后挂了，没能删除，那其他99台服务器就会一直等通知，那就完蛋了。。。

这个时候呢，就需要用到**临时性节点**了，我们前面说过了，临时性节点的特点是客户端一旦断开，就会丢失，也就是当服务器1创建了节点后，如果挂了。

那这个节点会自动被删除，这样后续的其他服务器，就可以继续去创建节点，获取锁了。

但是我们可能还需要注意到一点，就是惊群效应：举一个很简单的例子,当你往一群鸽子中间扔一块食物,虽然最终只有一个鸽子抢到食物,但所有鸽子都会被惊动来争夺,没有抢到..

就是当服务器1节点有变化，会通知其余的99个服务器，但是最终只有1个服务器会创建成功，这样98还是需要等待监听，那么为了处理这种情况，就需要用到**临时顺序性节点**

大致意思就是，之前是所有99个服务器都监听一个节点，现在就是每一个服务器监听自己前面的一个节点。

假设100个服务器同时发来请求，这个时候会在 /zkjjj 节点下创建 100 个临时顺序性节点 /zkjjj/000000001, /zkjjj/000000002,一直到 /zkjjj/000000100,这个编号就等于是已经给他们设置了获取锁的先后顺序了。

当001节点处理完毕，删除节点后，002收到通知，去获取锁，开始执行，执行完毕，删除节点，通知003~以此类推。

# Redis架构

## 1.Redis线程模型

### 1.介绍

Redis是典型的单线程架构，所有的读写操作都是在服务端的一条主线程中完成的。Redis客户端与服务端的模型如下图，每次客户端调用都经历了发送命令、执行命令、返回结果三个过程：

<img src="picture/中间件整理.assets/20200410213423895.png" alt="img" style="zoom:50%;" />

既然是单线程，为什么Redis能实现这么高的读写性能呢？

因为Redis基于Reactor 模式开发了自己的网络事件处理器——[**文件事件处理器**]()（file event handler）。

<img src="picture/%E4%B8%AD%E9%97%B4%E4%BB%B6%E6%95%B4%E7%90%86.assets/20210103213935324.png?lastModify=1618199516" alt="img" style="zoom:100%;" />

可以看到，I/O多路复用程序负责监听多个socket，并向文件事件派发器传递那些产生了事件的socket。

尽管多个文件事件可能会并发地出现，但I/O多路复用程序总是会将所有产生的socket都放到同一个队列里边，然后文件事件处理器会以有序、同步的方式处理该队列中的每个socket：根据每个socket当前产生的事件，来选择对应的事件处理器来处理。

#### 1.1 套接字（socket）

Redis服务端通过Socket与客户端连接。每当一个套接字准备好执行连接应答（accept）、读取（read）、写入（write）、关闭（close）等操作时，就会产生一个文件事件。 因为一个服务器通常会连接多个套接字， 所以多个文件事件有可能会并发地出现。

比如，当socket变得可读时，socket就会产生一个AE_READABLE事件；当socket变得可写的时候，socket会产生一个AE_WRITABLE事件。

#### 1.2 I/O 多路复用程序

I/O 多路复用程序主要负责监听多个套接字，并向文件事件分派器传送那些产生了事件的套接字。

I/O多路复用程序会将所有产生事件的套接字都入队到一个队列里， 然后通过这个队列，**以有序（sequentially）、同步（synchronously）、每次一个套接字**的方式向文件事件分派器传送套接字。 当上一个套接字产生的事件被处理完毕之后， I/O多路复用程序才会继续向文件事件分派器传送下一个套接字。

<img src="picture/中间件整理.assets/20200410213427494.png" alt="img" style="zoom:50%;" />

Redis 使用的IO多路复用技术主要有：select、epoll、evport和kqueue等。每个IO多路复用函数库在 Redis 源码中都对应一个单独的文件，比如ae_select.c，ae_epoll.c， ae_kqueue.c等。[**Redis 会根据不同的操作系统，按照不同的优先级选择多路复用技术。**]()事件响应框架一般都采用该架构，比如 netty 和 libevent：

<img src="picture/中间件整理.assets/20200410213430681.jpg" alt="img" style="zoom:67%;" />

#### 1.3 文件事件分派器

文件事件分派器，负责监听多个套接字，接收 IO 多路复用程序传来的套接字，并根据套接字产生的事件类型， 调用相应的事件处理器，比如：
如果是客户端要连接redis，那么会为socket关联连接应答处理器；
如果是客户端要写数据到redis，那么会为socket关联命令请求处理器；
如果是客户端要从redis读数据，那么会为socket关联命令回复处理器。

#### 1.4 事件处理器

服务器会为执行不同任务的套接字关联不同的事件处理器，[这些处理器是一个个函数， 它们定义了某个事件发生时， 服务器应该执行的动作]()。

### 2.示例

来看客户端与 Redis 的一次通信过程：

![Redis-single-thread-model](picture/中间件整理.assets/redis-single-thread-model.png)

首先，Redis 服务端进程初始化的时候，会将 server socket 的 `AE_READABLE` 事件与连接应答处理器关联。

客户端 socket01 向 Redis 进程的 server socket 请求建立连接，此时 server socket 会产生一个 `AE_READABLE` 事件，IO 多路复用程序监听到 server socket 产生的事件后，将该 socket 压入队列中。文件事件分派器从队列中获取 socket，交给**连接应答处理器**。连接应答处理器会创建一个能与客户端通信的 socket01，并将该 socket01 的 `AE_READABLE` 事件与命令请求处理器关联。

假设此时客户端发送了一个 `set key value` 请求，此时 Redis 中的 socket01 会产生 `AE_READABLE` 事件，IO 多路复用程序将 socket01 压入队列，此时事件分派器从队列中获取到 socket01 产生的 `AE_READABLE` 事件，由于前面 socket01 的 `AE_READABLE` 事件已经与命令请求处理器关联，因此事件分派器将事件交给命令请求处理器来处理。命令请求处理器读取 socket01 的 `key value` 并在自己内存中完成 `key value` 的设置。操作完成后，它会将 socket01 的 `AE_WRITABLE` 事件与命令回复处理器关联。

如果此时客户端准备好接收返回结果了，那么 Redis 中的 socket01 会产生一个 `AE_WRITABLE` 事件，同样压入队列中，事件分派器找到相关联的命令回复处理器，由命令回复处理器对 socket01 输入本次操作的一个结果，比如 `ok` ，之后解除 socket01 的 `AE_WRITABLE` 事件与命令回复处理器的关联。

### 3.总结

Redis属于单线程模型，但是效率非常高，每秒能够读写上万条数据，其主要原因如下：

1. 纯内存操作；
2. 核心是基于非阻塞的IO多路复用机制；
3. 单线程反而避免了多线程的频繁上下文切换问题。

### 4.Redis6.0的多线程

在 Redis 中，单线程的性能瓶颈主要在**网络IO操作**上。也就是在读写网络的read/write系统调用执行期间会占用大部分CPU时间。

所以总结起来，Redis 支持多线程主要就是两个原因：

- 可以充分利用服务器 CPU 资源，目前主线程只能利用一个核。
- 多线程任务可以分摊 Redis 同步 IO 读写负荷。

Redis6.0的多线程是指，**将网络数据读写和协议解析通过多线程的方式来处理，对于命令执行来说，仍然使用单线程操作。**

也就是说，Redis6.0的多线程是为了解决其[网络IO]()的瓶颈。
主要是解决 IO 读写和计算分离的问题，而不是串行 读 + 计算 + 写

![img](picture/中间件整理.assets/v2-5cc79aa66caca62b3390d717270760c1_720w.jpg)

Round Robin(轮询)

![img](picture/中间件整理.assets/v2-3197beffebd110fd15e38c40747a3983_720w.jpg)

## 2.内存管理

### 1.内存模型

#### 1.1 对象内存

对象内存，是Redis内存占用最大的一块，存储着用户所有的数据。Redis所有的数据都采用key-value格式，每次创建键值对时，至少创建两个类型对象：key对象和value对象。key对象都是字符串，value对象包含五种数据结构类型。所以，对象内存消耗可以简单理解为sizeof（keys）+sizeof（values）。

#### 1.2 缓冲内存

缓冲内存主要包括：客户端缓冲、复制积压缓冲区、AOF缓冲区：

- **客户端缓冲**：指的是所有接入到Redis服务器TCP连接的[**输入输出缓冲**]()；
- **复制积压缓冲区**：Redis在2.8版本之后提供了一个可重用的固定大小缓冲区，用于实现部分复制功能，根据repl-backlog-size参数控制，默认1MB。对于[**复制积压缓冲区**]()，只有Master节点存在，所有Slave节点共享此缓冲区。
- **AOF缓冲区**：用于在[**Redis AOF重写期间**]()保存最近的写入命令，其消耗的内存取决于AOF重写时间和写入量，这部分空间占用通常很小。

#### 1.3 子进程内存消耗

子进程内存消耗，主要是在执行AOF/RDB重写时，Redis创建的子进程内存消耗。

[**Redis执行fork操作产生的子进程，内存占用量与父进程相同，理论上需要一倍的物理内存来完成重写操作。**]()

Redis提供了一个命令：info memory，可以获取自身使用内存的统计数据：

| 属性名                  | 属性说明                                                     |
| :---------------------- | :----------------------------------------------------------- |
| used_memory             | Redis分配器分配的内存总量，也就是内部存储的所有数据内存占用量 |
| used_memory_rss         | 从操作系统的角度显示Redis进程占用的物理内存总量              |
| mem_fragmentation_ratio | 内存碎片率，used_memory_rss/used_memory的比值                |

我们需要重点关注的指标有：used_memory_rss和used_memory，以及它们的比值mem_fragmentation_ratio ：

- mem_fragmentation_ratio>1时，说明多出的部分内存并没有用于数据存储，而是被内存碎片所消耗；
- mem_fragmentation_ratio<1时 ，说明可能是操作系统把Redis内存swap到磁盘导致。

<img src="picture/中间件整理.assets/20200410214236098.png" alt="img" style="zoom:50%;" />

### 2.内存管理

Redis主要通过[**控制内存上限和回收策略**]()实现内存管理。

控制内存上限的方法一般也叫缓存淘汰策略。

控制内存上限就是使用maxmemory参数限制最大可用内存，当超出内存上限时，将使用一些算法来释放内存空间。

#### 2.1 回收策略

定期删除 + 惰性删除

##### 定期删除

定期删除，指的是Redis默认每隔100ms就随机抽取一些设置了过期时间的key，检查其是否过期，如果过期就删除。

> 会有一个比例，如果高了。就会再进行一次。

##### 惰性删除

当客户端获取某个key的时候，Redis会检查这个key是否设置了过期时间，如果设置了是否已过期？如果过期了就会删除，不会给客户端返回任何东西。

#### 2.2 淘汰策略

Redis一共有六种可选的缓存淘汰策略：

1. noeviction：默认策略，不会删除任何数据，拒绝所有写入操作并返回客户端错误信息；（一般没人用）
2. volatile-lru：根据LRU算法，删除设置了超时属性（expire）的键，直到腾出足够空间为止。如果没有可删除的键对象，回退到noeviction策略；（一般很少使用）
3. allkeys-lru：根据LRU算法删除key，不管数据有没有设置超时属性，直到腾出足够空间为止；（最常用）
4. allkeys-random：随机删除所有键，直到腾出足够空间为止；（一般没人用）
5. volatile-random：随机删除过期键，直到腾出足够空间为止；
6. volatile-ttl：在设置了过期时间的键空间中，有更早过期时间的key优先移除。如果没有，回退到noeviction策略。

### 3.LRU算法

LRU的淘汰规则是基于访问时间。每个缓存都有个最近使用时间戳，每次缓存命中的时候都会更新这个时间戳为当前时间点，每次淘汰缓存的时候，就淘汰时间戳距当前时间点最早的数据。

#### 3.1 LinkedHashMap

JDK集合框架中有一个类LinkedHashMap，顾名思义，是一种链表与Hash结合的数据结构。


![img](picture/中间件整理.assets/20200410214236630.png)

LinkedHashMap是一种将所有Entry节点链入一个双向链表的HashMap，额外维护了一个双向链表用于保持迭代顺序（按元素插入顺序，尾插法）：

<img src="picture/中间件整理.assets/20200410214237559.png" alt="img" style="zoom:50%;" />

LinkedHashMap中有一个accessOrder字段，true表示按照LRU原则维护元素次序，false表示则表示按插入顺序维护元素次序：

```java
public LinkedHashMap(int initialCapacity, float loadFactor, boolean accessOrder) {
    super(initialCapacity, loadFactor);
    this.accessOrder = accessOrder;
}
```

所以，基于LinkedHashMap的这种特性，我们很容易实现一个基于LRU算法的简单缓存：

```java
public class LRUCache<K, V> extends LinkedHashMap<K, V> {

    /**
     * 缓存数据量大小
     */
    private final int CACHE_SIZE;

    public LRUCache(int cacheSize) {
        // 设置hashmap的初始大小，最后一个true让linkedhashmap按照访问顺序来进行排序
        super((int) Math.ceil(cacheSize / 0.75) + 1, 0.75f, true); 
        CACHE_SIZE = cacheSize;
    }

    /**
     * 当map中的数据量大于最大缓存数据量时，就自动删除最老的数据
     */
    @Override
    protected boolean removeEldestEntry(Map.Entry eldest) {
        return size() > CACHE_SIZE; 
    }
}
```

## 3.Redis持久化

> 如果我们仅将Redis作为纯内存的缓存来用，那么可以禁止RDB和AOF。

### 1.RDB持久化

RDB持久化，是一次性生成内存快照的方式，也就是把当前Redis进程的数据生成快照，保存到磁盘上。触发RDB持久化的方式有手动触发和自动触发，[**生产环境一般都是自动触发，都是通过bgsave命令**]()。

#### 1.1 持久化原理

执行bgsave命令后，Redis主进程会执行fork操作创建子进程，RDB持久化过程由子进程负责，完成后自动结束。阻塞只发生在fork阶段，一般时间很短，整个过程如下图：

<img src="picture/中间件整理.assets/20200410214358799.png" alt="img" style="zoom:50%;" />

1. 执行bgsave命令，Redis父进程判断当前是否存在正在执行的子进程，如RDB/AOF子进程，如果存在则直接返回；
2. 父进程执行fork操作创建子进程，fork操作过程中父进程会阻塞；
3. 父进程fork完成后，bgsave命令返回“Background saving started”信息，并不再阻塞父进程，父进程可以继续响应其他命令；
4. 子进程创建RDB文件，根据父进程内存生成临时快照文件，完成后对原有文件进行原子替换；
5. 子进程[发送信号通知父进程]()RDB持久化完成，父进程更新统计信息。

#### 1.2 优点

1. RDB会生成多个数据文件，每个数据文件都代表了某一个时刻中Redis的数据，这种多个数据文件的方式，非常适合做定期的[**冷备**]()；
2. RDB[对Redis正常的读写服务影响非常小]()，可以让Redis保持高性能，因为Redis主进程只需要fork一个子进程，让子进程执行磁盘IO操作来进行RDB持久化即可；
3. 相对于AOF持久化机制来说，直接基于RDB数据文件来[**重启和恢复数据更加快速**]()。

#### 1.3 缺点

1. RDB持久化不能做到实时，因为频繁[**fork子进程的开销是很大的**]()。所以如果想要在Redis故障时，尽可能减少数据丢失，那应该开启AOF。一般来说，RDB数据快照文件，都是每隔5分钟或者更长时间生成一次；
2. RDB每次fork子进程生成数据快照文件时，如果数据文件特别大，可能会导致对客户端提供的服务暂停数毫秒，甚至数秒。

### 2.AOF持久化

AOF（append only file）持久化，是对每条写入命令以append-only的模式写入一个AOF日志文件中，在Redis重启的时候，可以通过回放AOF日志中的写入指令来重新构建整个数据集。

#### 2.1 持久化原理

Redis可以通过配置参数appendonly来开启AOF持久化。AOF持久化主要分为三个步骤：

1. 命令追加（append）
2. 文件写入（sync）
3. 文件重写（rewrite）

整个流程如下图：

<img src="picture/中间件整理.assets/20200410214359528.png" alt="img" style="zoom:50%;" />

#### 命令追加

Redis的所有写入命令（以文本协议格式）会追加到aof_buf缓冲区的末尾，这个缓冲区其实就是操系统的filesystem cache，可以大幅提升磁盘IO的性能。

#### 文件写入

AOF缓冲区的数据，会根据一定的策略写入到磁盘上的AOF日志文件中，具体有以下几种策略，由参数appendfsync控制：

**1. always**
[**每次写入AOF缓存后，立即**]()调用操作系统的fsync命令，将数据刷到磁盘。一般不建议配置，因为会大幅降低吞吐量，除非对数据可用性有非常高的要求。

**2. no**
由操作系统自身去控制何时将os cache中的数据刷到磁盘上，同步周期不可控。一般也不建议配置。

**3. everysec**

每隔1秒钟（默认），调用操作系统的fsync命令将数据刷到磁盘。推荐配置。

#### 文件重写

随着AOF日志文件越来越大，需要定期对AOF文件进行rewrite，以达到压缩的目的。

因为AOF会记录每一条写命令，所以会导致对于同一个Key，存在多个冗余命令。Redis会创建一个新的AOF文件来替换老文件，新旧两个文件所保存的数据状态完全相同，但是新文件不会包含任何浪费空间的冗余命令。

主进程通过bgrewriteaof命令进行重写：

1. 父进程fork一个子进程；
2. 子进程根据当前的内存快照，按照命令合并规则（避免命令冗余），写入到新的AOF文件；
3. 在此期间，主进程会把接收到的新的写入命令追加到AOF缓存区（aof_buf），同时再写入一份到AOF重写缓存区（aof_rewrite_buf）；
4. 子进程完成新的AOF文件写入后，会发送信号给父进程；
5. 父进程接受到信号后，把aof_rewrite_buf的数据写入到新的AOF文件中，这样新AOF文件中的数据状态就和当前数据状态是一致的了；
6. 重命名新的AOF文件，覆盖掉老文件，完成AOF重写。

整个流程的示意图如下：

<img src="picture/中间件整理.assets/20200410214400358.png" alt="img" style="zoom:50%;" />

#### 2.2 优点

1. AOF可以[**更好的保护数据不丢失**]()，AOF默认每隔1秒，通过一个后台线程执行一次fsync操作，所以最多丢失1秒钟的数据；
2. AOF日志文件以append-only模式写入，所以[**没有任何磁盘寻址的开销，写入性能非常高**]()，而且文件不容易损坏，即使文件尾部破损，也很容易修复；
3. AOF日志文件过大的时候，即使出现重写，由于是后台操作，所以也不会影响客户端的读写。
4. AOF日志文件的命令以可读的方式进行记录，非常适合做灾难性的误删除操作的紧急恢复。比如某人不小心用flushall命令清空了所有数据，只要这个时候后台rewrite还没有发生，那么就可以立即拷贝AOF文件，将最后一条flushall命令删除，然后再将该AOF文件放回去，就可以通过恢复机制，自动恢复所有数据。

#### 2.3 缺点

1. 对于同一份数据来说，AOF日志文件通常比RDB数据快照文件更大；
2. AOF开启后，[**支持的写QPS会比RDB低**]()，因为AOF一般会配置成每秒fsync一次日志文件；
3. AOF这种基于命令日志回放的方式，比RDB每次持久化一份完整数据快照的方式，更加脆弱一些，容易有bug。不过AOF就是为了避免rewrite过程导致的bug，因此每次rewrite并不是基于旧的指令日志进行merge的，而是基于当时内存中的数据，进行指令的重新构建，这样健壮性会好很多。

### 3混合持久化

重启 Redis 时，我们很少使用 `rdb` 来恢复内存状态，因为会丢失大量数据。

我们通常使用 AOF 日志重放，但是重放 AOF 日志性能相对 `rdb` 来说要慢很多，这样在 Redis 实例很大的情况下，启动需要花费很长的时间。

**Redis 4.0** 为了解决这个问题，带来了一个新的持久化选项——**混合持久化**。将 `rdb` 文件的内容和增量的 AOF 日志文件存在一起。这里的 AOF 日志不再是全量的日志，**而是 自持久化开始到持久化结束 的这段时间发生的增量 AOF 日志，通常这部分 AOF 日志很小：**

![324324325](picture/中间件整理.assets/324324325.webp)

于是在 Redis 重启的时候，可以先加载 `rdb` 的内容，然后再重放增量 AOF 日志就可以完全替代之前的 AOF 全量文件重放，重启效率因此大幅得到提升。

### 4.总结

生产环境一般会综合使用AOF和RDB，[**用AOF来保证数据不丢失，作为数据恢复的第一选择，用RDB来做周期型的冷备**]()，在AOF文件都丢失或损坏不可用的时候，还可以使用RDB来进行快速的数据恢复。

# Redis生产问题

## 1.为什么要用缓存？

用缓存，主要有两个用途：[**高性能**、**高并发**]()。

mysql 这么重的数据库，压根儿设计不是让你玩儿高并发的，虽然也可以玩儿，但是天然支持不好。mysql 单机支撑到 `2000QPS` 也开始容易报警了。

[单机承载并发量是 mysql 单机的几十倍]()。

> 缓存是走内存的，内存天然就支撑高并发。

## 2.Redis 和 Memcached 有啥区别？

#### Redis 支持复杂的数据结构

Redis 相比 Memcached 来说，拥有[**更多的数据结构**]()，能支持更丰富的数据操作。

#### Redis 原生支持集群模式

在 Redis3.x 版本中，便能支持 cluster 模式，而 [**Memcached 没有原生的集群模式**]()，需要依靠客户端来实现往集群中分片写入数据。

#### 性能对比

[由于 Redis 只使用**单核**，而 Memcached 可以使用**多核**]()，所以平均每一个核上 Redis 在存储小数据时比 Memcached 性能更高。而在 100k 以上的数据中，Memcached 性能要高于 Redis。虽然 Redis 最近也在存储大数据的性能上进行优化，但是比起 Memcached，还是稍有逊色。

## 3.如何保证 redis 的高并发和高可用？



如果你用 redis 缓存技术的话，肯定要考虑如何用 redis 来加多台机器，保证 redis 是高并发的

如何让 redis 保证自己不是挂掉以后就直接死掉了，即 redis 高可用。

- [redis 主从架构](https://doocs.github.io/advanced-java/#/docs/high-concurrency/redis-master-slave)
- [redis 基于哨兵实现高可用](https://doocs.github.io/advanced-java/#/docs/high-concurrency/redis-sentinel)

redis 实现**高并发**主要依靠**主从架构**，一主多从，一般来说，很多项目其实就足够了，单主用来写入数据，单机几万 QPS，多从用来查询数据，多个从实例可以提供每秒 10w 的 QPS。

如果想要在实现高并发的同时，容纳大量的数据，那么就需要 redis 集群，使用 redis 集群之后，可以提供每秒几十万的读写并发。

redis 高可用，如果是做主从架构部署，那么加上哨兵就可以了，就可以实现，任何一个实例宕机，可以进行主备切换。

总结：

> 高并发：主从架构
>
> 高并发+高可用：主从+哨兵
>
> 高并发+高可用+容纳大量数据：集群Cluster

## 4.Redis 主从架构

单机的 Redis，能够承载的 QPS 大概就在上万到几万不等。对于缓存来说，一般都是用来支撑**读高并发**的。因此架构做成主从(master-slave)架构，一主多从，主负责写，并且将数据复制到其它的 slave 节点，从节点负责读。所有的**读请求全部走从节点**。这样也可以很轻松实现水平扩容，**支撑读高并发**。

![Redis-master-slave](picture/中间件整理.assets/redis-master-slave.png)

Redis replication -> 主从架构 -> 读写分离 -> 水平扩容支撑读高并发

### Redis replication 的核心机制

- Redis 采用**异步方式**复制数据到 slave 节点，不过 Redis2.8 开始，slave node 会周期性地确认自己每次复制的数据量；
- 一个 master node 是可以配置多个 slave node 的；slave node 也可以连接其他的 slave node；
- slave node 在做复制的时候，也不会 block 对自己的查询操作，它会用旧的数据集来提供服务；但是复制完成的时候，需要删除旧数据集，加载新数据集，这个时候就会暂停对外服务了；[**(相当于是copyonwrite)**]()
- [**slave node 主要用来进行横向扩容，做读写分离，扩容的 slave node 可以提高读的吞吐量**]()。

注意，如果采用了主从架构，那么建议必须**开启** master node 的[持久化](https://doocs.github.io/advanced-java/#/docs/high-concurrency/redis-persistence)，不建议用 slave node 作为 master node 的数据热备，因为那样的话，如果你关掉 master 的持久化，可能在 master 宕机重启的时候数据是空的，然后可能一经过复制， slave node 的数据也丢了。

另外，master 的各种备份方案，也需要做。万一本地的所有文件丢失了，从备份中挑选一份 rdb 去恢复 master，这样才能**确保启动的时候，是有数据的**，即使采用了后续讲解的[高可用机制](https://doocs.github.io/advanced-java/#/docs/high-concurrency/redis-sentinel)，slave node 可以自动接管 master node，但也可能 sentinel 还没检测到 master failure，master node 就自动重启了，还是可能导致上面所有的 slave node 数据被清空。

### Redis 主从复制的核心原理

当启动一个 slave node 的时候，它会发送一个 `PSYNC` 命令给 master node。

如果这是 slave node 初次连接到 master node，那么会触发一次 `full resynchronization` 全量复制。此时 master 会启动一个后台线程，开始生成一份 `RDB` 快照文件，同时还会将从客户端 client 新收到的所有写命令缓存在内存中。 `RDB` 文件生成完毕后， master 会将这个 `RDB` 发送给 slave，slave 会先**写入本地磁盘，然后再从本地磁盘加载到内存**中，接着 master 会将内存中缓存的写命令发送到 slave，slave 也会同步这些数据。slave node 如果跟 master node 有网络故障，断开了连接，会自动重连，连接之后 master node 仅会复制给 slave 部分缺少的数据。

![Redis-master-slave-replication](picture/中间件整理.assets/redis-master-slave-replication.png)

### 主从复制的断点续传

从 Redis2.8 开始，就支持主从复制的断点续传，如果主从复制过程中，网络连接断掉了，那么可以接着上次复制的地方，继续复制下去，而不是从头开始复制一份。

master node 会在内存中维护一个 backlog，master 和 slave 都会保存一个 replica offset 还有一个 master run id，offset 就是保存在 backlog 中的。如果 master 和 slave 网络连接断掉了，slave 会让 master 从上次 replica offset 开始继续复制，如果没有找到对应的 offset，那么就会执行一次 `resynchronization` 。

### 1.全量复制

如果这个slave node是第一次连接master node，那么会触发一次全量复制（full resynchronization）。全量复制的整体流程如下：

1. master node收到psync命令后执行bgsave命令，在后台生成RDB文件，同时将从现在开始执行的所有写命令记录到缓冲区；
2. RDB文件生成完毕后，master node将其发送给slave node。slave node接收保存，然后载入这个RDB文件；
3. master node将缓冲区中的命令发送给slave node，slave node将自己的状态更新至Master当前所处的状态，保持数据最终一致。　

<img src="picture/中间件整理.assets/20200410214503911.png" alt="img" style="zoom:50%;" />

### 2.增量复制

增量复制，主要用于处理在主从复制过程中，因网络闪断等原因造成的数据丢失场景。如果全量复制过程中，master-slave网络连接断掉，那么salve重新连接master时，可能会触发增量复制。

此时，[**master直接从自己的backlog中获取部分丢失的数据**]()，发送给slave node，默认backlog就是1MB。因为补发的数据远远小于全量数据，所以可以有效避免全量复制的开销。

那么问题来了，slave如何知道要从哪个地方开始获取数据呢？从Redis 2.8开始，Redis开始支持主从复制的断点续传，整个过程如下：

<img src="picture/中间件整理.assets/20200410214504793.png" alt="img" style="zoom:50%;" />

看不懂？没关系，我来详细讲解下，部分复制的核心包含三部分：

- master node的复制偏移量（replication offset）和slave node的复制偏移量；
- master node的复制积压缓存区（replication backlog）；
- 服务器的运行ID（run ID）

#### 2.1 replication offset

复制偏移量（replication offset），标识着master和slave的数据同步进度。

当slave因为网络闪断等原因与master失去连接后，如果再次连接上master，会通过PSYNC命令将自己的offset发送给master，master根据偏移量决定执行全量复制还是部分复制：

- [**如果offset之后的数据（offset+1开始）仍存在于master的复制积压缓存区中，则进行部分复制**]()；
- 否则，进行全量复制。

当master每次向slave传播N个字节的数据时，就将自己的offset值加上N；slave每次收到master传播来的N个字节数据时，就将自己的offset值加上N：

<img src="picture/中间件整理.assets/20200410214505559.png" alt="img" style="zoom:50%;" />

#### 2.2 replication backlog

复制积压缓存区，是由master维护的一个固定大小的FIFO队列（默认为1MB），主要是用来做全量复制中断后的增量复制。

当master进行命令传播（给slave发送数据）时，它不仅会将写命令发送给所有salve，还会将写命令入队到复制积压缓冲区里面。也就是说，复制积压缓存区保存着一部分最近传播的写命令：

<img src="picture/中间件整理.assets/20200410214506737.png" alt="img" style="zoom:50%;" />

> 当主从节点网络中断后，slave再次连上master时，会发送psync{offset}{runId}命令请求部分复制。如果请求的offset不在master的积压缓冲区内，则无法提供给slave数据，因此部分复制会退化为全量复制。

#### 2.3 run ID

每个Redis服务节点，在启动时都会生成自己的run ID。run ID由40个随机的十六进制字符串组成。当进行初次全量复制时，master会将自己的运行ID传送给slave，而slave则将这个ID保存起来。当slave断线重连成功后，会将之前保存的运行ID发送给master：

- 如果Master发现Slave上送的运行ID与自身的运行ID不同，则进行全量复制；
- 如果Master发现Slave上送的运行ID与自身的运行ID相同，则进行部分复制。

> [**如果master因故障重启，那么它的run ID会改变**]()，slave发现与master的运行ID不匹配时，会认为自己复制的是一个新的master节点，从而进行全量复制。
>
> 这种情况应从架构上规避，比如提供故障转移功能：当主节点发生故障后，自动提升从节点为主节点。

### 3.脑裂问题

脑裂，也就是说，某个master突然脱离了正常的网络，跟其他slave不能连接了，但是实际上master还运行着。此时哨兵可能就会认为master宕机了。然后将其他slave切换成了master，这个时候，集群里就会有两个master，也就是所谓的脑裂。

此时，Client可能还没来得及切换到新的master，继续向旧master写数据。[**当网络分区恢复后，旧master会被作为一个slave挂到新的master上去，自己的数据会清空，重新从新的master复制数据，这样就发生了数据丢失。**]()

#### 3.1 解决方案

针对脑裂问题，我们可以通过设置一些Redis参数来解决：

- min-slaves-to-write：要求至少有多少个slave；
- min-slaves-max-lag：数据复制和同步的延迟最多不能超过多少秒。

比如，我们将min-slaves-to-write设置为1，min-slaves-max-lag设置为10，一旦slave复制数据和ack延时超过10秒，就认为master宕机后损失的数据太多了，那么就拒绝写请求。

如果一个master出现了脑裂，跟其他slave丢了连接，那么上面两个配置可以确保说：[**如果不能继续给指定数量的slave发送数据，而且slave超过10秒没有给自己ack消息，那么就直接拒绝客户端的写请求。**]()也就是配置之后脑裂后的旧master就不会接受client的新数据，也就避免了数据丢失。

这样，在脑裂场景下，上面的配置最多就丢失10秒的数据。

## 5.Redis 哨兵

我们在搭建Redis的主从架构时，主节点一旦由于故障不能提供服务，需要人工将从节点晋升为主节点，同时还要通知应用方更新主节点地址，对于很多应用场景这种故障处理的方式是无法接受的。

**要实现Redis的真正高可用，需要主从架构下的故障自动转移。**Redis官方提供了一套Redis Sentinel机制，用于当主节点出现故障时，[**自动完成故障发现和故障转移**]()。

### 1.简介

哨兵模式下，我们需要配置一些哨兵节点，这些哨兵节点构成了一个集群，监控着普通的主从节点的状态：

<img src="picture/中间件整理.assets/20200410214607618.png" alt="img" style="zoom:50%;" />

> Redis Sentinel包含了若个Sentinel节点，这样做也带来了两个好处：
>
> ①对于节点的故障判断是由多个Sentinel节点共同完成，这样[**可以有效地防止误判**]()；
>
> ②即使个别Sentinel节点不可用，整个Sentinel[**集群依然是可用**]()的。

哨兵模式提供了以下核心功能：

- 监控：每个Sentinel节点会对数据节点（Redis master/slave 节点）和其余Sentinel节点进行监控；
- 通知：Sentinel节点会将故障转移的结果通知给应用方；
- 故障转移：实现slave晋升为master，并维护后续正确的主从关系；
- 配置中心：在Redis Sentinel模式中，客户端在初始化的时候连接的是Sentinel节点集合，从中获取主节点信息。

### 2.哨兵的核心知识

- **哨兵至少需要 3 个实例，来保证自己的健壮性**
- 哨兵 + Redis 主从的部署架构，是**不保证数据零丢失**的，只能保证 Redis 集群的高可用性。
- 对于哨兵 + Redis 主从这种复杂的部署架构，尽量在测试环境和生产环境，都进行充足的测试和演练。

### 3.原理

#### 3.1 监控

Sentinel节点需要监控master、slave以及其它Sentinel节点的状态。这一过程是通过Redis的[**pub/sub**](pub/sub)系统实现的。Redis Sentinel一共有三个[**定时监控任务**]()，完成对各个节点发现和监控：

1. （info命令）监控主从拓扑信息：每隔10秒，每个Sentinel节点，会向master和slave[**发送INFO命令**]()获取最新的拓扑结构；

2. （sentinel:hello）Sentinel节点信息交换：每隔2秒，每个Sentinel节点，会向Redis数据节点的__sentinel__:hello频道上，发送自身的信息，以及对主节点的判断信息。这样，Sentinel节点之间就可以交换信息；

   > 1. 每隔两秒钟，每个哨兵都会往自己监控的某个 master+slaves 对应的 `__sentinel__:hello` channel 里**发送一个消息**，内容是自己的 host、ip 和 runid 还有对这个 master 的监控配置。这时候所有其他哨兵都可以消费到这个消息，并感知到其他的哨兵的存在。
   >
   > 2. 每个哨兵也会去**监听**自己监控的每个 master+slaves 对应的 `__sentinel__:hello` channel，然后去感知到同样在监听这个 master+slaves 的其他哨兵的存在。
   >
   > 3. 每个哨兵还会跟其他哨兵交换对 `master` 的监控配置，互相进行监控配置的同步。

3. （ping）节点状态监控：每隔1秒，每个Sentinel节点，会向master、slave、其余Sentinel节点[**发送PING命令做心跳检测**]()，来确认这些节点当前是否可达。

#### 2.2 主观/客观下线

##### 主观下线

每个Sentinel节点，**每隔1秒会对数据节点发送ping命令做心跳检测**，当这些节点超过down-after-milliseconds没有进行有效回复时，Sentinel节点会对该节点做失败判定，这个行为叫做主观下线。

##### 客观下线

客观下线，是指当大多数Sentinel节点，都认为master节点宕机了，那么这个判定就是客观的，叫做客观下线。

那么这个大多数是指多少呢？这其实就是分布式协调中的quorum判定了，大多数就是过半数，比如哨兵数量是3，那么大多数就是3/2+1=2个，哨兵数量是5，大多数就是5/2+1=3个。

> Sentinel节点的数量至少为3个，否则不满足quorum判定条件。

#### 2.3 哨兵选举

如果发生了客观下线，那么哨兵节点会选举出一个Leader来进行实际的故障转移工作。Redis使用了**[Raft算法]()**来实现哨兵领导者选举，大致思路如下：

1. 每个Sentinel节点都有资格成为领导者，[**当它主观认为某个数据节点宕机后**]()，会向其他Sentinel节点发送sentinel is-master-down-by-addr命令，要求自己成为领导者；
2. 收到命令的Sentinel节点，如果没有同意过其他Sentinel节点的sentinelis-master-down-by-addr命令，将同意该请求，否则拒绝（每个Sentinel节点只有1票）；
3. 如果该Sentinel节点发现自己的票数已经大于等于MAX(quorum, num(sentinels)/2+1)，那么它将成为领导者；
4. 如果此过程没有选举出领导者，将进入下一次选举。

#### 2.4 故障转移

选举出的Leader Sentinel节点将负责故障转移，也就是进行master/slave节点的主从切换。故障转移，首先要从slave节点中筛选出一个作为新的master，主要考虑以下slave信息：

1. 跟master[**断开连接的时长**]()：如果一个slave跟master的断开连接时长已经超过了down-after-milliseconds的10倍，外加master宕机的时长，那么该slave就被认为不适合选举为master；
2. slave的[**优先级**]()配置：slave priority参数值越小，优先级就越高；
3. [**复制offset**]()：当优先级相同时，哪个slave复制了越多的数据（offset越靠后），优先级越高；
4. [**run id**]()：如果offset和优先级都相同，则哪个slave的run id越小，优先级越高。

接着，筛选完slave后， 会对它执行slaveof no one命令，让其成为主节点。

最后，Sentinel领导者节点会向剩余的slave节点发送命令，让它们成为新的master节点的从节点，复制规则与parallel-syncs参数有关。

Sentinel节点集合会将原来的master节点更新为slave节点，并保持着对其关注，[**当其恢复后命令它去复制新的主节点**]()。

> Leader Sentinel节点，会从新的master节点那里得到一个configuration epoch，本质是个version版本号，每次主从切换的version号都必须是唯一的。其他的哨兵都是根据vetsion来更新自己的master配置。

## 6.Redis Cluster

在集群模式下，Redis 的 key 是如何寻址的？分布式寻址都有哪些算法？了解一致性 hash 算法吗？

### 面试官心理分析

如果你的数据量很少，主要是承载高并发高性能的场景，比如你的缓存一般就几个 G，单机就足够了，可以使用 replication，一个 master 多个 slaves，要几个 slave 跟你要求的读吞吐量有关，然后自己搭建一个 sentinel 集群去保证 Redis 主从架构的高可用性。

> [Redis cluster，主要是针对**海量数据+高并发+高可用**的场景]()。
>
> Redis cluster 支撑 N 个 Redis master node，[**每个 master node 都可以挂载多个 slave node。这样整个 Redis 就可以横向扩容了**]()。如果你要支撑更大数据量的缓存，那就横向扩容更多的 master 节点，每个 master 节点就能存放更多的数据了。

### Redis cluster 介绍

> - 自动将数据进行分片，[**每个 master 上放一部分数据**]()
> - 提供内置的高可用支持，部分 master 不可用时，还是可以继续工作的

在 Redis cluster 架构下，每个 Redis 要放开两个端口号，比如一个是 6379，另外一个就是 加 1w 的端口号，比如 16379。

16379 端口号是用来进行节点间通信的，也就是 cluster bus 的东西，cluster bus 的通信，用来进行故障检测、配置更新、故障转移授权。cluster bus 用了另外一种二进制的协议， `gossip` 协议，用于节点间进行高效的数据交换，占用更少的网络带宽和处理时间。

### 节点间的内部通信机制

#### 基本通信原理

集群元数据的维护有两种方式：集中式、Gossip 协议。Redis cluster 节点间采用 gossip 协议进行通信。

**集中式**是将集群元数据（节点信息、故障等等）几种存储在某个节点上。集中式元数据集中存储的一个典型代表，就是大数据领域的 `storm` 。它是分布式的大数据实时计算引擎，是集中式的元数据存储的结构，底层基于 zookeeper（分布式协调的中间件）对所有元数据进行存储维护。

![zookeeper-centralized-storage](picture/中间件整理.assets/zookeeper-centralized-storage.png)

Redis 维护集群元数据采用另一个方式， `gossip` 协议，所有节点都持有一份元数据，不同的节点如果出现了元数据的变更，就不断将元数据发送给其它的节点，让其它节点也进行元数据的变更。

![Redis-gossip](picture/中间件整理.assets/redis-gossip.png)

**集中式**的**好处**在于，元数据的读取和更新，时效性非常好，一旦元数据出现了变更，就立即更新到集中式的存储中，其它节点读取的时候就可以感知到；**不好**在于，所有的元数据的更新压力全部集中在一个地方，可能会导致元数据的存储有压力。

gossip 好处在于，元数据的更新比较分散，不是集中在一个地方，更新请求会陆陆续续打到所有节点上去更新，降低了压力；不好在于，元数据的更新有延时，可能导致集群中的一些操作会有一些滞后。

- 10000 端口：每个节点都有一个专门用于节点间通信的端口，就是自己提供服务的端口号+10000，比如 7001，那么用于节点间通信的就是 17001 端口。每个节点每隔一段时间都会往另外几个节点发送 `ping` 消息，同时其它几个节点接收到 `ping` 之后返回 `pong` 。
- 交换的信息：信息包括故障信息，节点的增加和删除，hash slot 信息等等。

#### gossip 协议

gossip 协议包含多种消息，包含 `ping` , `pong` , `meet` , `fail` 等等。

- meet：某个节点发送 meet 给新加入的节点，让新节点加入集群中，然后新节点就会开始与其它节点进行通信。

```bash
Redis-trib.rb add-nodeCopy to clipboardErrorCopied
```

其实内部就是发送了一个 gossip meet 消息给新加入的节点，通知那个节点去加入我们的集群。

- ping：每个节点都会频繁给其它节点发送 ping，其中包含自己的状态还有自己维护的集群元数据，互相通过 ping 交换元数据。
- pong：返回 ping 和 meeet，包含自己的状态和其它信息，也用于信息广播和更新。
- fail：某个节点判断另一个节点 fail 之后，就发送 fail 给其它节点，通知其它节点说，某个节点宕机啦。

#### ping 消息深入

ping 时要携带一些元数据，如果很频繁，可能会加重网络负担。

每个节点每秒会执行 10 次 ping，每次会选择 5 个最久没有通信的其它节点。当然如果发现某个节点通信延时达到了 `cluster_node_timeout / 2` ，那么立即发送 ping，避免数据交换延时过长，落后的时间太长了。比如说，两个节点之间都 10 分钟没有交换数据了，那么整个集群处于严重的元数据不一致的情况，就会有问题。所以 `cluster_node_timeout` 可以调节，如果调得比较大，那么会降低 ping 的频率。

每次 ping，会带上自己节点的信息，还有就是带上 1/10 其它节点的信息，发送出去，进行交换。至少包含 `3` 个其它节点的信息，最多包含 `总节点数减 2` 个其它节点的信息。

### 分布式寻址算法

- hash 算法（大量缓存重建）
- 一致性 hash 算法（自动缓存迁移）+ 虚拟节点（自动负载均衡）
- Redis cluster 的 hash slot 算法

#### hash 算法

来了一个 key，首先计算 hash 值，然后对节点数取模。然后打在不同的 master 节点上。

> 一旦某一个 master 节点宕机，所有请求过来，都会基于最新的剩余 master 节点数去取模，尝试去取数据。这会导致**大部分的请求过来，全部无法拿到有效的缓存**，导致大量的流量涌入数据库。

![hash](picture/中间件整理.assets/hash.png)

#### 一致性 hash 算法

一致性 hash 算法将整个 hash 值空间组织成一个虚拟的圆环，整个空间按顺时针方向组织，下一步将各个 master 节点（使用服务器的 ip 或主机名）进行 hash。这样就能确定每个节点在其哈希环上的位置。

来了一个 key，首先计算 hash 值，并确定此数据在环上的位置，从此位置沿环**顺时针“行走”**，遇到的第一个 master 节点就是 key 所在位置。

在一致性哈希算法中，如果一个节点挂了，受影响的数据仅仅是此节点到环空间前一个节点（沿着逆时针方向行走遇到的第一个节点）之间的数据，其它不受影响。增加一个节点也同理。

燃鹅，一致性哈希算法在节点太少时，容易因为节点分布不均匀而造成**缓存热点**的问题。为了解决这种热点问题，一致性 hash 算法引入了虚拟节点机制，即[**对每一个节点计算多个 hash，每个计算结果位置都放置一个虚拟节点**]()。这样就实现了数据的均匀分布，负载均衡。

<img src="picture/中间件整理.assets/consistent-hashing-algorithm.png" alt="consistent-hashing-algorithm" style="zoom:67%;" />

#### Redis cluster 的 hash slot 算法

Redis cluster 有固定的 `16384` 个 hash slot，对每个 `key` 计算 `CRC16` 值，然后对 `16384` 取模，可以获取 key 对应的 hash slot。

Redis cluster 中每个 master 都会持有部分 slot，比如有 3 个 master，那么可能每个 master 持有 5000 多个 hash slot。hash slot 让 node 的增加和移除很简单，增加一个 master，就将其他 master 的 hash slot 移动部分过去，减少一个 master，就将它的 hash slot 移动到其他 master 上去。移动 hash slot 的成本是非常低的。客户端的 api，可以对指定的数据，让他们走同一个 hash slot，通过 `hash tag` 来实现。

任何一台机器宕机，另外两个节点，不影响的。因为 key 找的是 hash slot，不是机器。

![hash-slot](picture/中间件整理.assets/hash-slot.png)

### Redis cluster 的高可用与主备切换原理

Redis cluster 的高可用的原理，几乎跟哨兵是类似的。

#### [判断节点宕机](https://doocs.github.io/advanced-java/#/./docs/high-concurrency/redis-cluster?id=判断节点宕机)

如果一个节点认为另外一个节点宕机，那么就是 `pfail` ，**主观宕机**。如果多个节点都认为另外一个节点宕机了，那么就是 `fail` ，**客观宕机**，跟哨兵的原理几乎一样，sdown，odown。

在 `cluster-node-timeout` 内，某个节点一直没有返回 `pong` ，那么就被认为 `pfail` 。

如果一个节点认为某个节点 `pfail` 了，那么会在 `gossip ping` 消息中， `ping` 给其他节点，如果**超过半数**的节点都认为 `pfail` 了，那么就会变成 `fail` 。

#### [从节点过滤](https://doocs.github.io/advanced-java/#/./docs/high-concurrency/redis-cluster?id=从节点过滤)

对宕机的 master node，从其所有的 slave node 中，选择一个切换成 master node。

检查每个 slave node 与 master node 断开连接的时间，如果超过了 `cluster-node-timeout * cluster-slave-validity-factor` ，那么就**没有资格**切换成 `master` 。

#### [从节点选举](https://doocs.github.io/advanced-java/#/./docs/high-concurrency/redis-cluster?id=从节点选举)

每个从节点，都根据自己对 master 复制数据的 offset，来设置一个选举时间，offset 越大（复制数据越多）的从节点，选举时间越靠前，优先进行选举。

所有的 master node 开始 slave 选举投票，给要进行选举的 slave 进行投票，如果大部分 master node `（N/2 + 1）` 都投票给了某个从节点，那么选举通过，那个从节点可以切换成 master。

从节点执行主备切换，从节点切换为主节点。

#### [与哨兵比较](https://doocs.github.io/advanced-java/#/./docs/high-concurrency/redis-cluster?id=与哨兵比较)

整个流程跟哨兵相比，非常类似，所以说，Redis cluster 功能强大，直接集成了 replication 和 sentinel 的功能。

## 9.Redis 的雪崩、穿透和击穿

### [**缓存雪崩**]()

对于系统 A，假设每天高峰期每秒 5000 个请求，本来缓存在高峰期可以扛住每秒 4000 个请求，但是缓存机器意外发生了全盘宕机。缓存挂了，此时 1 秒 5000 个请求全部落数据库，数据库必然扛不住，它会报一下警，然后就挂了。此时，如果没有采用什么特别的方案来处理这个故障，DBA 很着急，重启数据库，但是数据库立马又被新的流量给打死了。

这就是缓存雪崩。

![redis-caching-avalanche](picture/中间件整理.assets/redis-caching-avalanche.png)

大约在 3 年前，国内比较知名的一个互联网公司，曾因为缓存事故，导致雪崩，后台系统全部崩溃，事故从当天下午持续到晚上凌晨 3~4 点，公司损失了几千万。

缓存雪崩的事前事中事后的解决方案如下：

- 事前：Redis 高可用，主从+哨兵，Redis cluster，避免全盘崩溃。
- 事中：本地 ehcache 缓存 + hystrix 限流&降级，避免 MySQL 被打死。
- 事后：Redis 持久化，一旦重启，自动从磁盘上加载数据，快速恢复缓存数据。

![redis-caching-avalanche-solution](picture/中间件整理.assets/redis-caching-avalanche-solution.png)

用户发送一个请求，系统 A 收到请求后，先查本地 ehcache 缓存，如果没查到再查 Redis。如果 ehcache 和 Redis 都没有，再查数据库，将数据库中的结果，写入 ehcache 和 Redis 中。

限流组件，可以设置每秒的请求，有多少能通过组件，剩余的未通过的请求，怎么办？**走降级**！可以返回一些默认的值，或者友情提示，或者空值。

好处：

- 数据库绝对不会死，限流组件确保了每秒只有多少个请求能通过。
- 只要数据库不死，就是说，对用户来说，2/5 的请求都是可以被处理的。
- 只要有 2/5 的请求可以被处理，就意味着你的系统没死，对用户来说，可能就是点击几次刷不出来页面，但是多点几次，就可以刷出来了。

### **[缓存穿透]()**

对于系统 A，假设一秒 5000 个请求，结果其中 4000 个请求是黑客发出的恶意攻击。

黑客发出的那 4000 个攻击，缓存中查不到，每次你去数据库里查，也查不到。

举个栗子。数据库 id 是从 1 开始的，结果黑客发过来的请求 id 全部都是负数。这样的话，缓存中不会有，请求每次都“**视缓存于无物**”，直接查询数据库。这种恶意攻击场景的缓存穿透就会直接把数据库给打死。

![redis-caching-penetration](picture/中间件整理.assets/redis-caching-penetration.png)

解决方式很简单，每次系统 A 从数据库中只要没查到，就写一个空值到缓存里去，比如 `set -999 UNKNOWN` 。然后设置一个过期时间，这样的话，下次有相同的 key 来访问的时候，在缓存失效之前，都可以直接从缓存中取数据。

### **[缓存击穿]()**

缓存击穿，就是说某个 key 非常热点，访问非常频繁，处于集中式高并发访问的情况，当这个 key 在失效的瞬间，大量的请求就击穿了缓存，直接请求数据库，就像是在一道屏障上凿开了一个洞。

不同场景下的解决方式可如下：

- 若缓存的数据是基本不会发生更新的，则可尝试将该热点数据设置为[永不过期]()。
- 若缓存的数据更新不频繁，且缓存刷新的整个流程耗时较少的情况下，则可以采用基于 Redis、zookeeper 等分布式中间件的分布式互斥锁，或者本地互斥锁以保证仅少量的请求能请求数据库并重新构建缓存，其余线程则在锁释放后能访问到新缓存。
- 若缓存的数据更新频繁或者在缓存刷新的流程耗时较长的情况下，可以[利用定时线程在缓存过期前主动地重新构建缓存或者延后缓存的过期时间]()，以保证所有的请求能一直访问到对应的缓存。

## 10.如何保证缓存与数据库的双写一致性？

一般来说，如果允许缓存可以稍微的跟数据库偶尔有不一致的情况，也就是说如果你的系统**不是严格要求** “缓存+数据库” 必须保持一致性的话，最好不要做这个方案，即：[**读请求和写请求串行化，串到一个内存队列里去]()。**

串行化可以保证一定不会出现不一致的情况，但是它也会导致系统的吞吐量大幅度降低，用比正常情况下多几倍的机器去支撑线上的一个请求。

### [Cache Aside Pattern]()

最经典的缓存+数据库读写的模式，就是 Cache Aside Pattern。

> - 读的时候，先读缓存，缓存没有的话，就读数据库，然后取出数据后放入缓存，同时返回响应。
> - 更新的时候，**先更新数据库，然后再删除缓存**。

**为什么是删除缓存，而不是更新缓存？**

原因很简单，很多时候，在复杂点的缓存场景，缓存不单单是数据库中直接取出来的值。

比如可能更新了某个表的一个字段，然后其对应的缓存，是需要查询另外两个表的数据并进行运算，才能计算出缓存最新的值的。

另外[**更新缓存的代价有时候是很高的**]()。是不是说，每次修改数据库的时候，都一定要将其对应的缓存更新一份？也许有的场景是这样，但是对于**比较复杂的缓存数据计算的场景**，就不是这样了。如果你频繁修改一个缓存涉及的多个表，缓存也频繁更新。但是问题在于，**这个缓存到底会不会被频繁访问到？**

举个栗子，一个缓存涉及的表的字段，在 1 分钟内就修改了 20 次，或者是 100 次，那么缓存更新 20 次、100 次；但是这个缓存在 1 分钟内只被读取了 1 次，有**大量的冷数据**。实际上，如果你只是删除缓存的话，那么在 1 分钟内，这个缓存不过就重新计算一次而已，开销大幅度降低。**用到缓存才去算缓存。**

其实删除缓存，而不是更新缓存，就是一个 lazy 计算的思想，不要每次都重新做复杂的计算，不管它会不会用到，而是让它到需要被使用的时候再重新计算。像 mybatis，hibernate，都有**[懒加载思想]()**。查询一个部门，部门带了一个员工的 list，没有必要说每次查询部门，都把里面的 1000 个员工的数据也同时查出来啊。80% 的情况，查这个部门，就只是要访问这个部门的信息就可以了。先查部门，同时要访问里面的员工，那么这个时候只有在你要访问里面的员工的时候，才会去数据库里面查询 1000 个员工。

### [最初级的缓存不一致问题及解决方案]()

问题：先更新数据库，再删除缓存。如果删除缓存失败了，那么会导致数据库中是新数据，缓存中是旧数据，数据就出现了不一致。

![redis-junior-inconsistent](picture/中间件整理.assets/redis-junior-inconsistent.png)

解决思路：

> 先删除缓存，再更新数据库。如果数据库更新失败了，那么数据库中是旧数据，缓存中是空的，那么数据不会不一致。因为读的时候缓存没有，所以去读了数据库中的旧数据，然后更新到缓存中。

### [比较复杂的数据不一致问题分析]()

> 数据发生了变更，先删除了缓存，然后要去修改数据库，此时还没修改。一个请求过来，去读缓存，发现缓存空了，去查询数据库，**查到了修改前的旧数据**，放到了缓存中。随后数据变更的程序完成了数据库的修改。完了，数据库和缓存中的数据不一样了...

**解决方案如下：**

更新数据的时候，根据**数据的唯一标识**，将操作路由之后，发送到一个 jvm 内部队列中。读取数据的时候，如果发现数据不在缓存中，那么将重新执行“读取数据+更新缓存”的操作，根据唯一标识路由之后，也发送到同一个 jvm 内部队列中。

一个队列对应一个工作线程，每个工作线程**串行**拿到对应的操作，然后一条一条的执行。这样的话，一个数据变更的操作，先删除缓存，然后再去更新数据库，但是还没完成更新。此时如果一个读请求过来，没有读到缓存，那么可以先将缓存更新的请求发送到队列中，此时会在队列中积压，然后同步等待缓存更新完成。

这里有一个**优化点**，一个队列中，其实**多个更新缓存请求串在一起是没意义的**，因此可以做过滤，如果发现队列中已经有一个更新缓存的请求了，那么就不用再放个更新请求操作进去了，直接等待前面的更新操作请求完成即可。

待那个队列对应的工作线程完成了上一个操作的数据库的修改之后，才会去执行下一个操作，也就是缓存更新的操作，此时会从数据库中读取最新的值，然后写入缓存中。

如果请求还在等待时间范围内，不断轮询发现可以取到值了，那么就直接返回；如果请求等待的时间超过一定时长，那么这一次直接从数据库中读取当前的旧值。

高并发的场景下，该解决方案要注意的问题：

- 读请求长时阻塞

由于读请求进行了非常轻度的异步化，所以一定要注意读超时的问题，每个读请求必须在超时时间范围内返回。

该解决方案，最大的风险点在于说，**可能数据更新很频繁**，导致队列中积压了大量更新操作在里面，然后**读请求会发生大量的超时**，最后导致大量的请求直接走数据库。务必通过一些模拟真实的测试，看看更新数据的频率是怎样的。

另外一点，因为一个队列中，可能会积压针对多个数据项的更新操作，因此需要根据自己的业务情况进行测试，可能需要**部署多个服务**，每个服务分摊一些数据的更新操作。如果一个内存队列里居然会挤压 100 个商品的库存修改操作，每个库存修改操作要耗费 10ms 去完成，那么最后一个商品的读请求，可能等待 10 * 100 = 1000ms = 1s 后，才能得到数据，这个时候就导致**读请求的长时阻塞**。

一定要做根据实际业务系统的运行情况，去进行一些压力测试，和模拟线上环境，去看看最繁忙的时候，内存队列可能会挤压多少更新操作，可能会导致最后一个更新操作对应的读请求，会 hang 多少时间，如果读请求在 200ms 返回，如果你计算过后，哪怕是最繁忙的时候，积压 10 个更新操作，最多等待 200ms，那还可以的。

**如果一个内存队列中可能积压的更新操作特别多**，那么你就要**加机器**，让每个机器上部署的服务实例处理更少的数据，那么每个内存队列中积压的更新操作就会越少。

其实根据之前的项目经验，一般来说，数据的写频率是很低的，因此实际上正常来说，在队列中积压的更新操作应该是很少的。像这种针对读高并发、读缓存架构的项目，一般来说写请求是非常少的，每秒的 QPS 能到几百就不错了。

我们来**实际粗略测算一下**。

如果一秒有 500 的写操作，如果分成 5 个时间片，每 200ms 就 100 个写操作，放到 20 个内存队列中，每个内存队列，可能就积压 5 个写操作。每个写操作性能测试后，一般是在 20ms 左右就完成，那么针对每个内存队列的数据的读请求，也就最多 hang 一会儿，200ms 以内肯定能返回了。

经过刚才简单的测算，我们知道，单机支撑的写 QPS 在几百是没问题的，如果写 QPS 扩大了 10 倍，那么就扩容机器，扩容 10 倍的机器，每个机器 20 个队列。

- 读请求并发量过高

这里还必须做好压力测试，确保恰巧碰上上述情况的时候，还有一个风险，就是突然间大量读请求会在几十毫秒的延时 hang 在服务上，看服务能不能扛的住，需要多少机器才能扛住最大的极限情况的峰值。

但是因为并不是所有的数据都在同一时间更新，缓存也不会同一时间失效，所以每次可能也就是少数数据的缓存失效了，然后那些数据对应的读请求过来，并发量应该也不会特别大。

- 多服务实例部署的请求路由

可能这个服务部署了多个实例，那么必须**保证**说，执行数据更新操作，以及执行缓存更新操作的请求，都通过 Nginx 服务器**路由到相同的服务实例上**。

比如说，对同一个商品的读写请求，全部路由到同一台机器上。可以自己去做服务间的按照某个请求参数的 hash 路由，也可以用 Nginx 的 hash 路由功能等等。

- 热点商品的路由问题，导致请求的倾斜

万一某个商品的读写请求特别高，全部打到相同的机器的相同的队列里面去了，可能会造成某台机器的压力过大。就是说，因为只有在商品数据更新的时候才会清空缓存，然后才会导致读写并发，所以其实要根据业务系统去看，如果更新频率不是太高的话，这个问题的影响并不是特别大，但是的确可能某些机器的负载会高一些。

## 11.Redis 的并发竞争问题是什么？

如何解决这个问题？了解 Redis 事务的 CAS 方案吗？

> **多客户端同时并发写**一个 key，可能本来应该先到的数据后到了，导致数据版本错了；
>
> 或者是多客户端同时获取一个 key，修改值之后再写回去，只要顺序错了，数据就错了。
>
> 而且 Redis 自己就有天然解决这个问题的 CAS 类的乐观锁方案。

某个时刻，多个系统实例都去更新某个 key。可以基于 zookeeper 实现分布式锁。每个系统通过 zookeeper 获取分布式锁，确保同一时间，只能有一个系统实例在操作某个 key，别人都不允许读和写。

![zookeeper-distributed-lock](picture/中间件整理.assets/zookeeper-distributed-lock.png)

> 你要写入缓存的数据，都是从 mysql 里查出来的，都得写入 mysql 中，写入 mysql 中的时候必须保存一个时间戳，从 mysql 查出来的时候，时间戳也查出来。
>
> 每次要**写之前，先判断**一下当前这个 value 的时间戳是否比缓存里的 value 的时间戳要新。如果是的话，那么可以写，否则，就不能用旧的数据覆盖新的数据。

------

## 12.生产环境中的 Redis 是怎么部署的？

> 看看你了解不了解你们公司的 Redis 生产集群的部署架构，如果你不了解，那么确实你就很失职了，你的 Redis 是主从架构？集群架构？用了哪种集群方案？有没有做高可用保证？有没有开启持久化机制确保可以进行数据恢复？线上 Redis 给几个 G 的内存？设置了哪些参数？压测后你们 Redis 集群承载多少 QPS？

Redis cluster，10 台机器，5 台机器部署了 Redis 主实例，另外 5 台机器部署了 Redis 的从实例，每个主实例挂了一个从实例，5 个节点对外提供读写服务，每个节点的读写高峰 QPS 可能可以达到每秒 5 万，5 台机器最多是 25 万读写请求每秒。

机器是什么配置？32G 内存+ 8 核 CPU + 1T 磁盘，但是分配给 Redis 进程的是 10g 内存，一般线上生产环境，Redis 的内存尽量不要超过 10g，超过 10g 可能会有问题。

5 台机器对外提供读写，一共有 50g 内存。

因为每个主实例都挂了一个从实例，所以是高可用的，任何一个主实例宕机，都会自动故障迁移，Redis 从实例会自动变成主实例继续提供读写服务。

你往内存里写的是什么数据？每条数据的大小是多少？商品数据，每条数据是 10kb。100 条数据是 1mb，10 万条数据是 1g。常驻内存的是 200 万条商品数据，占用内存是 20g，仅仅不到总内存的 50%。目前高峰期每秒就是 3500 左右的请求量。

# MQ

## 1.为什么使用消息队列

### 面试题剖析

#### 为什么使用消息队列

面试官问你这个问题，**期望的一个回答**是说，你们公司有个什么**业务场景**，这个业务场景有个什么技术挑战，如果不用 MQ 可能会很麻烦，但是你现在用了 MQ 之后带给了你很多的好处。

核心3 个：**解耦**、**异步**、**削峰**。

##### 解耦

A 系统（譬如说是订单系统）发送数据到 BCD （譬如说是供应商，总公司，消费者）三个系统，通过接口调用发送。

如果 E 系统也要这个数据呢？那如果 C 系统现在不需要了呢？

![mq-1](picture/中间件整理.assets/mq-1.png)

在这个场景中，A 系统跟其它各种乱七八糟的系统严重耦合，A 系统产生一条比较关键的数据，很多系统都需要 A 系统将这个数据发送过来。[A 系统要时时刻刻考虑 BCDE 四个系统如果挂了该咋办？要不要重发，要不要把消息存起来？]()

如果使用 MQ，A 系统产生一条数据，发送到 MQ 里面去，哪个系统需要数据自己去 MQ 里面消费。如果新系统需要数据，直接从 MQ 里消费即可；如果某个系统不需要这条数据了，就取消对 MQ 消息的消费即可。[这样下来，A 系统压根儿不需要去考虑要给谁发送数据，不需要维护这个代码，也不需要考虑人家是否调用成功、失败超时等情况。]()

![mq-2](picture/中间件整理.assets/mq-2.png)

**总结**：通过一个 MQ，Pub/Sub 发布订阅消息这么一个模型，A 系统就跟其它系统彻底解耦了。

##### 异步

一般互联网类的企业，对于用户直接的操作，一般要求是每个请求都必须在 200 ms 以内完成，对用户几乎是无感知的。

![mq-4](picture/中间件整理.assets/mq-4.png)

##### 削峰

![mq-6](picture/中间件整理.assets/mq-6.png)

#### 消息队列有什么缺点

缺点有以下几个：

- **系统可用性降低**

> 系统引入的外部依赖越多，越容易挂掉。
>
> 本来你就是 A 系统调用 BCD 三个系统的接口就好了，ABCD 四个系统还好好的，没啥问题，你偏加个 MQ 进来，万一 MQ 挂了咋整？MQ 一挂，整套系统崩溃，你不就完了？

- **系统复杂度提高**

> [怎么保证消息没有重复消费？怎么处理消息丢失的情况？怎么保证消息传递的顺序性？]()

- **一致性问题**

> A 系统处理完了直接返回成功了，人都以为你这个请求就成功了；但是问题是，要是 BCD 三个系统那里，BD 两个系统写库成功了，结果 C 系统写库失败了，咋整？你这数据就不一致了。

#### Kafka、ActiveMQ、RabbitMQ、RocketMQ 有什么优缺点？

| 特性                     | ActiveMQ                              | RabbitMQ                                           | RocketMQ                                                     | Kafka                                                        |
| ------------------------ | ------------------------------------- | -------------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 单机吞吐量               | 万级，比 RocketMQ、Kafka 低一个数量级 | 同 ActiveMQ                                        | 10 万级，支撑高吞吐                                          | 10 万级，高吞吐，一般配合大数据类的系统来进行实时数据计算、日志采集等场景 |
| topic 数量对吞吐量的影响 |                                       |                                                    | topic 可以达到几百/几千的级别，吞吐量会有较小幅度的下降，这是 RocketMQ 的一大优势，在同等机器下，可以支撑大量的 topic | topic 从几十到几百个时候，吞吐量会大幅度下降，在同等机器下，Kafka 尽量保证 topic 数量不要过多，如果要支撑大规模的 topic，需要增加更多的机器资源 |
| 时效性                   | ms 级                                 | 微秒级，这是 RabbitMQ 的一大特点，延迟最低         | ms 级                                                        | 延迟在 ms 级以内                                             |
| 可用性                   | 高，基于主从架构实现高可用            | 同 ActiveMQ                                        | 非常高，分布式架构                                           | 非常高，分布式，一个数据多个副本，少数机器宕机，不会丢失数据，不会导致不可用 |
| 消息可靠性               | 有较低的概率丢失数据                  | 基本不丢                                           | 经过参数优化配置，可以做到 0 丢失                            | 同 RocketMQ                                                  |
| 功能支持                 | MQ 领域的功能极其完备                 | 基于 erlang 开发，并发能力很强，性能极好，延时很低 | MQ 功能较为完善，还是分布式的，扩展性好                      | 功能较为简单，主要支持简单的 MQ 功能，在大数据领域的实时计算以及日志采集被大规模使用 |

综上，各种对比之后，有如下建议：

一般的业务系统要引入 MQ，最早大家都用 ActiveMQ，不推荐用这个了；

后来大家开始用 RabbitMQ，但是确实 erlang 语言阻止了大量的 Java 工程师去深入研究和掌控它，对公司而言，几乎处于不可控的状态，但是确实人家是开源的，比较稳定的支持，活跃度也高；

不过现在确实越来越多的公司会去用 RocketMQ，确实很不错，毕竟是阿里出品，但社区可能有突然黄掉的风险（目前 RocketMQ 已捐给 [Apache](https://github.com/apache/rocketmq)，但 GitHub 上的活跃度其实不算高）对自己公司技术实力有绝对自信的，推荐用 RocketMQ，否则回去老老实实用 RabbitMQ 吧，人家有活跃的开源社区，绝对不会黄。

所以**中小型公司**，技术实力较为一般，技术挑战不是特别高，用 RabbitMQ 是不错的选择；**大型公司**，基础架构研发实力较强，用 RocketMQ 是很好的选择。

如果是**大数据领域**的**[实时计算、日志采集等]()**场景，用 Kafka 是业内标准的，绝对没问题，社区活跃度很高，绝对不会黄，何况几乎是全世界这个领域的事实性规范。

## 2.kafka和rabbitmq对比

### 1.实际场景选择

在实际生产应用中，通常会使用kafka作为消息传输的数据管道，rabbitmq作为交易数据作为数据传输管道。

主要的取舍因素则是[是否存在丢数据的可能]()；

> rabbitmq在金融场景中经常使用，具有较高的严谨性，[数据丢失的可能性更小同时具备更高的实时性]()；
>
> 而[kafka优势主要体现在吞吐量上]()，虽然可以通过策略实现数据不丢失，但从严谨性角度来讲，大不如rabbitmq；而且由于kafka保证每条消息最少送达一次，有较小的概率会出现数据重复发送的情况；

#### 2.吞吐量方面

RabbitMQ：支持消息的可靠的传递，支持事务，不支持批量操作，基于存储的可靠性的要求存储可以采用内存或硬盘，吞吐量小。
kafka：内部采用消息的批量处理，数据的存储和获取是本地磁盘顺序批量操作，消息处理的效率高，吞吐量高。

### 2.集群负载均衡方面

RabbitMQ：

> 本身不支持负载均衡，需要loadbalancer的支持

kafka：

> 采用zookeeper对集群中的broker，consumer进行管理，可以注册topic到zookeeper上，通过zookeeper的协调机制，producer保存对应的topic的broker信息，可以随机或者轮询发送到broker上，producer可以基于语义指定分片，消息发送到broker的某个分片上。

## 3.高可用

### RabbitMQ 的高可用性

RabbitMQ 有三种模式：单机模式、普通集群模式、镜像集群模式。

#### 单机模式

单机模式，就是 Demo 级别的，一般就是你本地启动了玩玩儿的 😄，没人生产用单机模式。

#### 普通集群模式（无高可用性）

[普通集群模式，意思就是在多台机器上启动多个 RabbitMQ 实例，每个机器启动一个。]()

你**创建的 queue，只会放在一个 RabbitMQ 实例上**，但是每个实例都同步 queue 的元数据（元数据可以认为是 queue 的一些配置信息，通过元数据，可以找到 queue 所在实例）。

你消费的时候，实际上如果连接到了另外一个实例，那么那个实例会从 queue 所在实例上拉取数据过来。

![mq-7](picture/中间件整理.assets/mq-7.png)

1. 客户端每次请求都可能需要从真正queue实例那拉取数据，具有额外开销；
2. queue所在的RabbitMQ实例本身成为了性能瓶颈；
3. 如果 queue所在的RabbitMQ实例挂了，那queue的数据就丢失了，没法保证高可用。如果你**开启了消息持久化**，让 RabbitMQ 落地存储消息的话，**消息不一定会丢**，得等这个实例恢复了，然后才可以继续从这个 queue 拉取数据。

这就**没有什么所谓的高可用性**，[**这方案主要是提高吞吐量的**]()，就是说让集群中多个节点来服务某个 queue 的读写操作。

#### 镜像集群模式（高可用性）

[在镜像集群模式下，你创建的 queue，无论元数据还是 queue 里的消息都会**存在于多个实例上**，就是说，每个 RabbitMQ 节点都有这个 queue 的一个**完整镜像**，包含 queue 的全部数据的意思。]()

然后每次你写消息到 queue 的时候，都会自动把**消息同步**到多个实例的 queue 上。

![mq-8](picture/中间件整理.assets/mq-8.png)

> **如何开启这个镜像集群模式**？
>
> 其实很简单，RabbitMQ 有很好的管理控制台，就是在后台新增一个策略，这个策略是**镜像集群模式的策略**，指定的时候是可以要求数据同步到所有节点的，也可以要求同步到指定数量的节点，再次创建 queue 的时候，应用这个策略，就会自动将数据同步到其他的节点上去了。

好处在于，你任何一个机器宕机了，没事儿，其它机器（节点）还包含了这个 queue 的完整数据，别的 consumer 都可以到其它节点上去消费数据。

坏处在于

> 1. 单个节点需要容纳完整的数据，如果数据量很大，是没法水平扩展的；
>2. 同步消息数据到其它所有节点的性能开销是很大的。

### Kafka 的高可用性

Kafka 一个最基本的架构认识：

> 由多个 broker 组成，每个 broker 是一个节点；
>
> 你创建一个 topic，这个 topic 可以划分为多个 partition；
>
> 每个 partition 可以存在于不同的 broker 上，每个 partition 就放一部分数据。

这就是**天然的分布式消息队列**，就是说一个 topic 的数据，是**分散放在多个机器上的，每个机器就放一部分数据**。

> RabbitMQ 一个 queue 的数据都是放在一个节点里的，镜像集群下，也是每个节点都放这个 queue 的完整数据。
>

Kafka 0.8 以后，提供了 HA 机制，就是 replica（复制品） 副本机制。每个 partition 的数据都会同步到其它机器上，形成自己的多个 replica 副本。所有 replica 会选举一个 leader 出来，那么[**生产和消费都跟这个 leader 打交道**]()，然后其他 replica 就是 follower。

写的时候，leader 会负责把数据同步到所有 follower 上去，读的时候就直接读 leader 上的数据即可。

Kafka 会均匀地将一个 partition 的所有 replica 分布在不同的机器上，这样才可以提高容错性。

<img src="picture/中间件整理.assets/20200410110215078.png" alt="adv3-5" style="zoom: 50%;" />

如果某个 broker 宕机了，那个 broker 上面的 partition 在其他机器上都有副本的。如果这个宕机的 broker 上面有某个 partition 的 leader，那么此时会从 follower 中**重新选举**一个新的 leader 出来，大家继续读写那个新的 leader 即可。这就有所谓的高可用性了。

**写数据**:  消息会路由到leader，[**由leader负责将数据落地磁盘后，其它follower会自动从leader同步数据**]()，当所有follower都同步完成后，会发送ack确认消息给leader，leader收到所有响应后，返回响应给生产者。（当然，这只是其中一种模式，还可以适当调整这个行为）

**消费**:  [**只会从 leader 去读**]()，但是只有当一个消息已经被所有 follower 都同步成功返回 ack 的时候，这个消息才会被消费者读到。

## 4.消费幂等性

[**幂等**]()，就是当出现消费者对某条消息重复消费的情况时，重复消费的结果与消费一次的结果是相同的，并且多次消费并未对业务系统产生任何负面影响，那么这整个过程就实现可消息幂等。

例如，多次扣款，只扣除一次。

本章针对消息队列讲解消息消费的幂等性，本质其实是讲消费时接口设计的幂等性。

挑一个 Kafka 来举个例子，说说怎么重复消费吧。

> Kafka 实际上有个 offset 的概念，就是每个消息写进去，都有一个 offset，代表消息的序号，然后 consumer 消费了数据之后，**每隔一段时间**（定时定期），[**会把自己消费过的消息的 offset 提交一下**]()，表示“我已经消费过了，下次我要是重启啥的，你就让我继续从上次消费到的 offset 来继续消费吧”。

举个栗子。

![img](picture/中间件整理.assets/20200410110302466.png)

注意：新版的 Kafka 已经将 offset 的存储从 Zookeeper 转移至 Kafka brokers，并使用内部位移主题 `__consumer_offsets` 进行存储。

如果消费者消费完offfset=153的消息，并本地处理完成后，还没来得及提交offset就自己挂掉了，那么当消费者进程重启恢复后，此时再次去找Kafka消费，Kafka会把offset=153那条消息再次给消费者，消费者就重复消费153这条消息两次。

如果消费者干的事儿是拿一条数据就往数据库里写一条，会导致说，你可能就把数据 1/2 在数据库里插入了 2 次，那么数据就错啦。

> 事实上，消费者消费完一条数据后，不会立即提交offset，而是定时定期提交一次。

#### 幂等性设计

下面我们来看一个示例。

假如有一个支付服务接口，该服务对等的部署在3台机器上。前端上操作的时候，不小心针对同笔订单发起了两次支付请求，然后这俩请求分散在了不同的机器上。这个时候，如果不对支付接口做幂等性设计，就可能导致重复扣款。

<img src="picture/中间件整理.assets/20200410110303560.png" alt="img" style="zoom: 50%;" />

所以，保证幂等性主要是三点：

1. 每个请求必须有一个唯一的标识，比如订单支付请求，肯定得包含订单id，一个订单id最多支付一次；
2. 每次处理完请求之后，必须有一个[**状态标识**]()这个请求处理过了，最常见的方案就是在mysql中记录这个标识的状态，[**比如支付成功后，记录订单的状态为“已支付”**]()；
3. 每次接收请求后，首先根据标识判断之前是否已经处理过了，比如有一个订单已经支付了，就已经有了一条支付流水，那么如果重复发送这个请求，则此时先插入支付流水，orderId已经存在了，唯一键约束生效，会报错插入不进去，那就不会再扣款了。

其实还是得结合业务来思考，我这里给几个思路：

- 比如你拿个数据要写库，你先根据主键查一下，如果这数据都有了，你就别插入了，update 一下好吧。
- 比如你是写 Redis，那没问题了，反正每次都是 set，天然幂等性。
- 比如你不是上面两个场景，那做的稍微复杂一点，你需要让生产者发送每条数据的时候，里面加一个全局唯一的 id，类似订单 id 之类的东西，然后你这里消费到了之后，先根据这个 id 去比如 Redis 里查一下，之前消费过吗？如果没有消费过，你就处理，然后这个 id 写 Redis。如果消费过了，那你就别处理了，保证别重复处理相同的消息即可。（利用布隆过滤器，记录offset）
- 比如基于数据库的唯一键来保证重复数据不会重复插入多条。因为有唯一键约束了，重复数据插入只会报错，不会导致数据库中出现脏数据。

![mq-11](picture/中间件整理.assets/mq-11.png)

## 5.可靠传输

或者说，如何处理消息丢失的问题？

### RabbitMQ

![rabbitmq-message-lose](picture/中间件整理.assets/rabbitmq-message-lose.png)

#### 生产者弄丢了数据（confirm）

##### 事务机制

1. 生产者发送数据之前开启Rabbitmq事务channel.txSelect，然后发送消息；
2. 如果消息没有被接收到，生产者会收到异常报错，此时可以回滚事务channel.txRollback，然后重新发送；
3. 如果消息被接受，则可以提交事务channel.txCommit。

```java
// 开启事务
channel.txSelect
try {
    // 这里发送消息
} catch (Exception e) {
    channel.txRollback

    // 这里再次重发这条消息
}

// 提交事务
channel.txCommitCopy to clipboardErrorCopied
```

缺点：
RabbitMQ的事务机制是同步的，会导致吞吐量大幅下降。

##### confirm机制

生产者可以开启confitm模式：

1. 每次写消息时会给消息分配一个唯一id；
2. 如果RabbitMQ收到了该消息，会回调生产者的ack接口，表示接受成功；
3. 如果RabbitMQ接受消息失败，会回调生产者的nack接口，表示接受或处理失败，生产者在nack方法内进行重试发送；

```java
//开启confirm模式

channel.confirm();

// 发送消息，然后就不管了
send();

/**
 * 消息成功被接受后回调
 */
public void ack(String messageId){

}

/**
 * 消息接受失败时回调
 */
public void nack(String messageId){
    send();
}
```

如果因为网络原因，这两个方法都没有被回调，生产者可以自己维护消息id的状态，对一些超时的消息，根据状态进行重发。

> 事务机制是同步的，你提交一个事务之后会**阻塞**在那儿
>
> `confirm` 机制是**异步**的，你发送个消息之后就可以发送下一个消息，然后那个消息 RabbitMQ 接收了之后会异步回调你的一个接口通知你这个消息接收到了。

所以一般在生产者这块**避免数据丢失**，都是用 `confirm` 机制的。

#### RabbitMQ 故障（持久化）

就是 RabbitMQ 自己弄丢了数据，这个你必须[**开启 RabbitMQ 的持久化**]()，就是消息写入之后会持久化到磁盘，哪怕是 RabbitMQ 自己挂了，**恢复之后会自动读取之前存储的数据**，一般数据不会丢。除非极其罕见的是，RabbitMQ 还没持久化，自己就挂了，**可能导致少量数据丢失**，但是这个概率较小。

设置持久化有**两个步骤**：（queue和消息的持久化）

- 创建queue时，将其设置为持久化，这样RabbitMQ会持久化queue的元数据；
- 生产者发送消息时，消息的deliveryMode设置为2，此时RabbitMQ就会将消息持久化到磁盘上去

> 由于开启持久化后，**只有消息被持久化到磁盘，才会回调生产者的ack接口**，所以生产者在收不到ack的情况下，可以进行重发，这样哪怕持久化到磁盘前MQ自身挂了，也可以保证恢复后收到重发的消息。

#### 消费端弄丢了数据（手动ack）

这种情况是因为消费者自身问题或网络问题造成的。RabbitMQ有一个消费者的autoAck机制，当消费者消费成功后，会自动通知RabbitMQ已经消费成功，此时如果消费者自身出现异常，就会导致消息丢失。

解决方案如下：

1. 关闭RabbitMQ的消费者autoAck机制；
2. 消费者消费完消息，自身逻辑处理成功后，再进行手动ack确认。（这种情况下，消费者必须要保证自身接口的幂等性）

![rabbitmq-message-lose-solution](picture/中间件整理.assets/rabbitmq-message-lose-solution.png)

### Kafka

#### 生产者会不会弄丢数据？（acks=all）

如果设置了 `acks=all` ，一定不会丢。你的 leader 接收到消息，所有的 follower 都同步到了消息之后，才认为本次写成功了。如果没满足这个条件，生产者会自动不断的重试，重试无限次。

#### Kafka 故障（acks=all）

这种情况通常出现在Leader选举时：

1. 某个partiton的Leader正在和其它Follower同步消息；
2. 这个partiton所在的Broker突然挂了，部分Follower还没同步完成；
3. 新选举的Leader刚好是之前没同步完成的，此时它就缺少了一些数据。


<img src="picture/中间件整理.assets/20200410111352153.png" alt="img" style="zoom:50%;" />

 <img src="picture/中间件整理.assets/20200410111352897.png" alt="img" style="zoom:50%;" />

所以此时一般是要求起码设置如下 4 个参数：

- 给 topic 设置 `replication.factor` 参数：这个值必须大于 1，要求每个 partition 必须有至少 2 个副本。（多个副本）
- 在 Kafka 服务端设置 `min.insync.replicas` 参数：这个值必须大于 1，这个是要求一个 leader 至少感知到有至少一个 follower 还跟自己保持联系，没掉队，这样才能确保 leader 挂了还有一个 follower 吧。（多个follower）
- 在 producer 端设置 `acks=all` ：这个是要求每条数据，必须是**[写入所有 replica 之后，才能认为是写成功了]()**。（ack=all）
- 在 producer 端设置 `retries=MAX` （很大很大很大的一个值，无限次重试的意思）：这个是**要求一旦写入失败，就无限重试**，卡在这里了。（无限重试）

这样配置之后，至少在 Kafka broker 端就可以保证在 leader 所在 broker 发生故障，进行 leader 切换时，数据不会丢失。

#### 消费端弄丢了数据（手动offset）

消费者消费完消息后，会将消息的offset告知Kafka，表示这条消息已经被成功消费。

默认情况下，消费者会自动提交offset，如果此时消费者挂了，那么Kafka依然会认为消费者已经成功消费了该消息，从而出现消息丢失。

所以，Kafka对“消息投递后丢失”这一场景的问题处理方式和RabbitMQ类似，一般消费者需要关闭自动提交offset，等处理完消息后自己手动提交offset，就可以保证消息不会丢。但依然一样可能会出现重复消费问题，比如消费者刚处理完，还没提交offset结果自己挂了或因为网络原因Kafka没收到通知。所以消费者端仍然需要保证接口的幂等性。

## 6.消息顺序性

我举个例子，我们以前做过一个 mysql `binlog` 同步的系统，压力还是非常大的，日同步数据要达到上亿，就是说数据从一个 mysql 库原封不动地同步到另一个 mysql 库里面去（mysql -> mysql）。常见的一点在于说比如大数据 team，就需要同步一个 mysql 库过来，对公司的业务系统的数据做各种复杂的操作。

你在 mysql 里增删改一条数据，对应出来了增删改 3 条 `binlog` 日志，接着这三条 `binlog` 发送到 MQ 里面，再消费出来依次执行，起码得保证人家是按照顺序来的吧？不然本来是：增加、修改、删除；你愣是换了顺序给执行成删除、修改、增加，不全错了么。

本来这个数据同步过来，应该最后这个数据被删除了；结果你搞错了这个顺序，最后这个数据保留下来了，数据同步就出错了。

### RabbitMQ

在RabbitMQ中，当多个消费者对同一个queue进行消费时，是不能保证消息的有序执行的。比如消息按照消息A、消息B、消息C的顺序先后进入一个队列，但是当有多个消费者同时消费时，不能保证消费也按照A、B、C的顺序被依次执行。

所以RabbitMQ的解决方案就是为每一个消费者指定一个专门的队列，如下图：


![img](picture/中间件整理.assets/20200410111449487.png)

上图中，只要消息按照A、B、C的顺序入队，那每个消费者获取并执行消息的顺序也一定是A、B、C。

> 注意，由于可能存在网络延迟的因素，生产者需要确保按顺序投递成功。

### Kafka

首先，Kafka默认会保证同一个partition内的消息都是有序的，所以我们只要能够让生产者在发送消息时将需要保证顺序的几条消息都发送到同一个分区，那么消费者消费时，消息就是有序的。

生产者在发送消息时，会通过以下方式之一确定消息所属的partition：

- 显式指定partition
- 不指定partition，指定key：根据key的hash值与分区数进行运算，确定发送到哪个partition分区
- 不指定partition，不指定key：轮询各分区发送

所以，我们[**只要采用指定分区或指定key的方式就可保证消息的有序性**]()，如下图：


![img](picture/中间件整理.assets/20200410111450251.png)

> 这种情况下，如果消费者内部是单线程去依次执行每个消息，那没有问题；如果是多线程执行，就需要考虑consumer内的消费有序性，一般可以利用内存队列来解决。

消费者内部多线程情况下保证消息有序的方案如下：


![img](picture/中间件整理.assets/20200410111451441.png)

我们一般需要先进行压测，看下如果消费在单一线程下处理消息的吞吐量，如果一秒钟只能处理几十个消息，那实在是太低了，得考虑多线程方案。

### 总结

本章，我们介绍如何保证消息队列的消息有序性，根本思路就是两点：

1. 保证队列内的消息FIFO；
2. 保证一个消费者对应单独的一个队列；

在实际业务中，需要消息有序性的场景其实并不多。

## 7.消息积压

如何解决消息队列的延时以及过期失效问题？有几百万消息持续积压几小时，说说怎么解决？

### 面试官心理分析

消息积压，就是说消息队列里面积压了大量消息，一般是由于生产者投递消息的速率远远大于消息者消费消息的速率。比如消费者端程序挂掉，或者吞吐量变得极小，此时，MQ集群的磁盘可能会很快被写满。

或者是你积压的时间太长了，导致比如 RabbitMQ 设置了消息过期时间后就没了。

### 积压

一个消费者一秒是 1000 条，一秒 3 个消费者是 3000 条，一分钟就是 18 万条。所以如果你积压了几百万到上千万的数据，即使消费者恢复了，也需要大概 1 小时的时间才能恢复过来。

一般这个时候，只能临时紧急扩容了，具体操作步骤和思路如下：

- 先修复 consumer 的问题，确保其恢复消费速度，然后将现有 consumer 都停掉。
- 新建一个 topic，partition 是原来的 10 倍，临时建立好原先 10 倍的 queue 数量。
- 然后写一个临时的分发数据的 consumer 程序，这个程序部署上去消费积压的数据，**消费之后不做耗时的处理**，直接均匀轮询写入临时建立好的 10 倍数量的 queue。
- 接着临时征用 10 倍的机器来部署 consumer，每一批 consumer 消费一个临时 queue 的数据。[相当于是临时将 queue 资源和 consumer 资源扩大 10 倍，以正常的 10 倍速度来消费数据。]()
- 等快速消费完积压数据之后，**得恢复原先部署的架构**，**重新**用原先的 consumer 机器来消费消息。

### 消息过期

假设你用的是 RabbitMQ，RabbtiMQ 是可以设置过期时间的，也就是 TTL。如果消息在 queue 中积压超过一定的时间就会被 RabbitMQ 给清理掉，这个数据就没了。

我们可以采取一个方案，就是**批量重导**。就是大量积压的时候，我们当时就直接丢弃数据了，然后等过了高峰期以后，比如晚上 12 点以后。这个时候我们就开始写程序，将丢失的那批数据，写个临时程序，一点一点的查出来，然后重新灌入 mq 里面去，把白天丢的数据给他补回来。也只能是这样了。

------

### 对于 RocketMQ，官方针对消息积压问题，提供了解决方案。

#### 1. 提高消费并行度

绝大部分消息消费行为都属于 IO 密集型，即可能是操作数据库，或者调用 RPC，这类消费行为的消费速度在于后端数据库或者外系统的吞吐量，通过增加消费并行度，可以提高总的消费吞吐量，但是并行度增加到一定程度，反而会下降。所以，应用必须要设置合理的并行度。 如下有几种修改消费并行度的方法：

同一个 ConsumerGroup 下，通过增加 Consumer 实例数量来提高并行度（需要注意的是超过订阅队列数的 Consumer 实例无效）。可以通过加机器，或者在已有机器启动多个进程的方式。 提高单个 Consumer 的消费并行线程，通过修改参数 consumeThreadMin、consumeThreadMax 实现。

#### 2. 批量方式消费

某些业务流程如果支持批量方式消费，则可以很大程度上提高消费吞吐量，例如订单扣款类应用，一次处理一个订单耗时 1 s，一次处理 10 个订单可能也只耗时 2 s，这样即可大幅度提高消费的吞吐量，通过设置 consumer 的 consumeMessageBatchMaxSize 返个参数，默认是 1，即一次只消费一条消息，例如设置为 N，那么每次消费的消息数小于等于 N。

#### 3. 跳过非重要消息

发生消息堆积时，如果消费速度一直追不上发送速度，如果业务对数据要求不高的话，可以选择丢弃不重要的消息。例如，当某个队列的消息数堆积到 100000 条以上，则尝试丢弃部分或全部消息，这样就可以快速追上发送消息的速度。示例代码如下：

```java
public ConsumeConcurrentlyStatus consumeMessage(
            List<MessageExt> msgs,
            ConsumeConcurrentlyContext context) {
    long offset = msgs.get(0).getQueueOffset();
    String maxOffset =
            msgs.get(0).getProperty(Message.PROPERTY_MAX_OFFSET);
    long diff = Long.parseLong(maxOffset) - offset;
    if (diff > 100000) {
        // TODO 消息堆积情况的特殊处理
        return ConsumeConcurrentlyStatus.CONSUME_SUCCESS;
    }
    // TODO 正常消费过程
    return ConsumeConcurrentlyStatus.CONSUME_SUCCESS;
}Copy to clipboardErrorCopied
```

#### 4. 优化每条消息消费过程

举例如下，某条消息的消费过程如下：

- 根据消息从 DB 查询【数据 1】
- 根据消息从 DB 查询【数据 2】
- 复杂的业务计算
- 向 DB 插入【数据 3】
- 向 DB 插入【数据 4】

这条消息的消费过程中有 4 次与 DB 的 交互，如果按照每次 5ms 计算，那么总共耗时 20ms，假设业务计算耗时 5ms，那么总过耗时 25ms，所以如果能把 4 次 DB 交互优化为 2 次，那么总耗时就可以优化到 15ms，即总体性能提高了 40%。所以应用如果对时延敏感的话，可以把 DB 部署在 SSD 硬盘，相比于 SCSI 磁盘，前者的 RT 会小很多。

## 8.设计mq

### 面试官心理分析

其实聊到这个问题，一般面试官要考察两块：

- 你有没有对某一个消息队列做过较为深入的原理的了解，或者从整体了解把握住一个消息队列的架构原理。
- 看看你的设计能力，给你一个常见的系统，就是消息队列系统，看看你能不能从全局把握一下整体架构设计，给出一些关键点出来。

类似的问题，比如，如果让你来设计一个 Spring 框架你会怎么做？如果让你来设计一个 Dubbo 框架你会怎么做？如果让你来设计一个 MyBatis 框架你会怎么做？

### 面试题剖析

起码你要大概知道那个技术的基本原理、核心组成部分、基本架构构成，然后参照一些开源的技术把一个系统设计出来的思路说一下就好。

比如说这个消息队列系统，我们从以下几个角度来考虑一下：

- [**可伸缩性：**]()就是需要的时候快速扩容，就可以增加吞吐量和容量。参照一下 kafka 的设计理念，broker -> topic -> partition，每个 partition 放一个机器，就存一部分数据。如果现在资源不够了，给 topic 增加 partition，然后做数据迁移，增加机器，不就可以存放更多数据，提供更高的吞吐量了。
- **[持久化：]()**怎么落啊？顺序写，这样就没有磁盘随机读写的寻址开销，磁盘顺序读写的性能是很高的，这就是 kafka 的思路。
- [**可用性：**]() kafka 的高可用保障机制。多副本 -> leader & follower -> broker 挂了重新选举 leader 即可对外服务。
- [**数据保证：**]()参考我们之前说的那个 kafka 数据零丢失方案。

面试官问你这个问题，其实是个开放题，他就是看看你有没有从架构角度整体构思和设计的思维以及能力。

# Kafka文件存储机制那些事

## Kafka是什么

> Kafka是最初由Linkedin公司开发，是一个分布式、分区的、多副本的、多订阅者，基于zookeeper协调的分布式日志系统(也可以当做MQ系统)，常见可以用于web/nginx日志、访问日志，消息服务等等。

一个商业化消息队列的性能好坏，其文件存储机制设计是衡量一个消息队列服务技术水平和最关键指标之一。 下面将从Kafka文件存储机制和物理结构角度，分析Kafka是如何实现高效文件存储，及实际应用效果。

Kafka部分名词解释如下：

- Broker：消息中间件处理结点，一个Kafka节点就是一个broker，多个broker可以组成一个Kafka集群。
- Topic：一类消息，例如page view日志、click日志等都可以以topic的形式存在，Kafka集群能够同时负责多个topic的分发。
- Partition：topic物理上的分组，一个topic可以分为多个partition，[**每个partition是一个有序的队列**]()。
- Segment：partition物理上由多个segment组成。
- offset：[**每个partition都由一系列有序的、不可变的消息组成**]()，这些消息被连续的追加到partition中。partition中的每个消息都有一个连续的序列号叫做offset,用于partition唯一标识一条消息.

分析过程分为以下4个步骤：

- topic中partition存储分布
- partiton中文件存储方式
- partiton中segment文件存储结构
- 在partition中如何通过offset查找message

## 2.1 topic中partition存储分布

假设实验环境中Kafka集群只有一个broker，xxx/message-folder为数据文件存储根目录，在Kafka broker中server.properties文件配置(参数log.dirs=xxx/message-folder)，例如创建2个topic名称分别为report_push、launch_info, partitions数量都为partitions=4 存储路径和目录规则为： xxx/message-folder

```
              |--report_push-0
              |--report_push-1
              |--report_push-2
              |--report_push-3
              |--launch_info-0
              |--launch_info-1
              |--launch_info-2
              |--launch_info-3
```

在Kafka文件存储中，同一个topic下有多个不同partition，每个partition为一个目录，partiton命名规则为topic名称+有序序号，第一个partiton序号从0开始，序号最大值为partitions数量减1。 如果是多broker分布情况，请参考[kafka集群partition分布原理分析](http://blog.csdn.net/lizhitao/article/details/41778193)

## 2.2 partiton中文件存储方式

下面示意图形象说明了partition中文件存储方式:

![image](picture/中间件整理.assets/0ab51510.png)

- 每个partion(目录)相当于一个巨型文件被平均分配到多个大小相等segment(段)数据文件中。但每个段segment file消息数量不一定相等，这种特性方便old segment file快速被删除。
- 每个partiton只需要支持顺序读写就行了，segment文件生命周期由服务端配置参数决定。

这样做的好处就是能快速删除无用文件，有效提高磁盘利用率。

## 2.3 partiton中segment文件存储结构

- segment file组成：由2大部分组成，分别为index file和data file，此2个文件一一对应，成对出现，后缀”.index”和“.log”分别表示为segment索引文件、数据文件.
- segment文件命名规则：partion全局的第一个segment从0开始，后续每个segment文件名为上一个segment文件最后一条消息的offset值。数值最大为64位long大小，19位数字字符长度，没有数字用0填充。

下面文件列表是笔者在Kafka broker上做的一个实验，创建一个topicXXX包含1 partition，设置每个segment大小为500MB,并启动producer向Kafka broker写入大量数据,如下图2所示segment文件列表形象说明了上述2个规则：

![image](picture/中间件整理.assets/69e4b0a6.png)

以上述图2中一对segment file文件为例，说明segment中index<—->data file对应关系物理结构如下：

![image](picture/中间件整理.assets/c415ed42.png)

上述图3中索引文件存储大量元数据，数据文件存储大量消息，索引文件中元数据指向对应数据文件中message的物理偏移地址。 其中以索引文件中元数据3,497为例，依次在数据文件中表示第3个message(在全局partiton表示第368772个message)、以及该消息的物理偏移地址为497。

从上述图3了解到segment data file由许多message组成，下面详细说明message物理结构如下：

![image](picture/中间件整理.assets/355c1d57.png)

### 参数说明：

| 关键字              | 解释说明                                                     |
| :------------------ | :----------------------------------------------------------- |
| 8 byte offset       | 在parition(分区)内的每条消息都有一个有序的id号，这个id号被称为偏移(offset),它可以唯一确定每条消息在parition(分区)内的位置。即offset表示partiion的第多少message |
| 4 byte message size | message大小                                                  |
| 4 byte CRC32        | 用crc32校验message                                           |
| 1 byte “magic”      | 表示本次发布Kafka服务程序协议版本号                          |
| 1 byte “attributes” | 表示为独立版本、或标识压缩类型、或编码类型。                 |
| 4 byte key length   | 表示key的长度,当key为-1时，K byte key字段不填                |
| K byte key          | 可选                                                         |
| value bytes payload | 表示实际消息数据。                                           |

## 2.4 在partition中如何通过offset查找message

例如读取offset=368776的message，需要通过下面2个步骤查找。

- 第一步查找segment file 上述图2为例，其中00000000000000000000.index表示最开始的文件，起始偏移量(offset)为0.第二个文件00000000000000368769.index的消息量起始偏移量为368770 = 368769 + 1.同样，第三个文件00000000000000737337.index的起始偏移量为737338=737337 + 1，其他后续文件依次类推，以起始偏移量命名并排序这些文件，只要根据offset **二分查找**文件列表，就可以快速定位到具体文件。 当offset=368776时定位到00000000000000368769.index|log
- 第二步通过segment file查找message 通过第一步定位到segment file，当offset=368776时，依次定位到00000000000000368769.index的元数据物理位置和00000000000000368769.log的物理偏移地址，然后再通过00000000000000368769.log顺序查找直到offset=368776为止。

从上述图3可知这样做的优点，segment index file采取稀疏索引存储方式，它减少索引文件大小，通过mmap可以直接内存操作，稀疏索引为数据文件的每个对应message设置一个元数据指针,它比稠密索引节省了更多的存储空间，但查找起来需要消耗更多的时间。

## 3 Kafka文件存储机制–实际运行效果

实验环境：

- Kafka集群：由2台虚拟机组成
- cpu：4核
- 物理内存：8GB
- 网卡：千兆网卡
- jvm heap: 4GB
- 详细Kafka服务端配置及其优化请参考：[kafka server.properties配置详解](http://blog.csdn.net/lizhitao/article/details/25667831)

![image](picture/中间件整理.assets/7283e819.png)                              

从上述图5可以看出，Kafka运行时很少有大量读磁盘的操作，主要是定期批量写磁盘操作，因此操作磁盘很高效。这跟Kafka文件存储中读写message的设计是息息相关的。Kafka中读写message有如下特点:

写message

- 消息从java堆转入page cache(即物理内存)。
- 由异步线程刷盘,消息从page cache刷入磁盘。

读message

- 消息直接从page cache转入socket发送出去。
- 当从page cache没有找到相应数据时，此时会产生磁盘IO,从磁盘Load消息到page cache,然后直接从socket发出去

Kafka高效文件存储设计特点

- Kafka把topic中一个parition大文件分成多个小文件段，通过多个小文件段，就容易定期清除或删除已经消费完文件，减少磁盘占用。
- 通过索引信息可以快速定位message和确定response的最大大小。
- 通过index元数据全部映射到memory，可以避免segment file的IO磁盘操作。
- 通过索引文件稀疏存储，可以大幅降低index文件元数据占用空间大小。

# Docker深入理解

## Docker介绍

**要解释清楚Docker，首先要说解释清楚容器（Container）的概念。**

操作系统就是管理计算机的硬件软件和资源，并且为软件运行提供通用服务的系统软件。

- 硬件管理，包括分配CPU时间、内存；从网络、存储设备等IO设备读写数据。
- 软件管理，就是各种软件的运行，线程、进程调度之类的工作。
- 为软件提供运行环境，这个运行环境通常一部分由操作系统内核（Kernel）提供，另一部分由运行库（Runtime Library）提供。

硬件、操作系统、应用程序之间的关系可以简单的用下图表示：

```text
+--------------------------+
|       Applications       |
+--------------------------+
|+------------------------+|
||    Runtime Library     ||
|+------------------------+|
||         Kernel         ||
|+------------------------+|
|     Operating System     |
+-----+--------+-----------+
| CPU | Memory | IO Device |
+-----+--------+-----------+ 
```

随着硬件的性能提升，以及软件种类的丰富，有两种情况变得很常见：

1. 硬件性能过剩——例如一般家用电脑，已经是四核、六核的配置了，除了3A游戏、视频制作等特殊应用外，通常有90%以上时间CPU是闲置的。
2. 软件冲突——因为业务需要，两个或者多个软件之间冲突，或者需要同一个软件的不同版本。

为了解决软件冲突，只能配置多台计算机，或者很麻烦的在同一台电脑上安装多个操作系统，通过重启来进行切换。显然这两个方案都有其缺点：多台计算机成本太高，多操作系统的安装、切换都很麻烦。在硬件性能过剩的时候，硬件虚拟化的普及就很自然而然的提出来了。

所谓[**硬件虚拟化**]()

> [就是某个特殊的软件，仿真出一台或者多台计算机的各种硬件]()，用户可以在这一台虚拟机上安装、运行操作系统（一般叫来宾操作系统，Guest OS）和各种应用，并且把Guest OS和上面应用软件对硬件资源的访问转发到底层的硬件上来实现。

著名的VMware就是这么一个软件，这类软件有一个专用的单词是Hypervisor（维基的Hypervisor词条说另一种叫法是虚拟机监视器，**V**irtual **M**achine **M**onitor）。

Hypervisor根据其对硬件资源的访问方式，可以分为两大类

1. Type I是Hypervisor直接访问硬件资源，通常会有另一个操作系统运行于Hypervisor之上来对硬件资源，例如VMware EXSi，Windows的Hyper-V，Linux的Xen；
2. Type II是Hypervisor和普通的应用一样，运行在某个操作系统（例如Windows或者Linux等，这里称之为宿主机操作系统，Host OS）之上，Hypervisor通过Host OS访问硬件资源，例如VMware Workstation，Virtual Box等。

```text
                             +-----+-----+-----+-----+
                             |App A|App B|App C|App D|
+-----+-----+-----+-----+    +-----+-----+-----+-----+
|App A|App B|App C|App D|    |Guest|Guest|Guest|Guest|
+-----+-----+-----+-----+    | OS0 | OS1 | OS2 | OS3 |
|Guest|Guest|Guest|Guest|    +-----+-----+-----+-----+
| OS0 | OS1 | OS2 | OS3 |    |        Hypervisor     |
+-----+-----+-----+-----+    +-----------------------+
|        Hypervisor     |    |         Host OS       |
+-----------------------+    +-----------------------+
|        Hardware       |    |        Hardware       |
+-----------------------+    +-----------------------+
          Type I                       Type II
```

[**虚拟机的一个缺点在于Guest OS通常会占用不少硬件资源。**]()

例如Windows安装开机不运行任何运用，就需要占用2~3G内存，20~30G硬盘空间。即使是没有图形界面的Linux，根据发行版以及安装软件的不同也会占用100~1G内存，1~4G硬盘空间。而且为了应用系统运行的性能，往往还要给每台虚拟机留出更多的内存容量。虽然不少Hypervisor支持动态内存，但基本上都会降低虚拟机的性能。如果说这样的资源占用少量的虚拟机还可以接受的话，同时运行十数台数十台虚拟机的时候，浪费的硬件资源就相当可观了。通常来说，其中相当大部分甚至全部Guest OS都是相同的。

> 能不能所有的应用使用同一个的操作系统减少硬件资源的浪费，但是又能避免包括运行库在内的软件冲突呢？
>

[**操作系统层虚拟化——容器**]()概念的提出，就是为了解决这个问题。

在Linux可以通过**[控制组]()**（Control Group，通常简写为cgroup）隔离，并[**把应用和运行库打包在一起**]()，来实现这个目的。容器和Type II虚拟机、物理机的区别见下图：

```text
+-----+-----+-----+-----+                                   +-----+-----+-----+-----+
|App A|App B|App C|App D|     +-----+-----+-----+-----+     |App A|App B|App C|App D|
+-----+-----+-----+-----+     |App A|App B|App C|App D|     +-----+-----+-----+-----+
|+---------------------+|     +-----+-----+-----+-----+     |Guest|Guest|Guest|Guest|
||   Runtime Library   ||     |Lib A|Lib B|Lib C|Lib D|     | OS0 | OS1 | OS2 | OS3 |
|+---------------------+|     +-----+-----+-----+-----+     +-----+-----+-----+-----+
||       Kernel        ||     |    Container Engine   |     |        Hypervisor     |
|+---------------------+|     +-----------------------+     +-----------------------+
|   Operating System    |     |         Host OS       |     |         Host OS       |
+-----------------------+     +-----------------------+     +-----------------------+
|       Hardware        |     |        Hardware       |     |        Hardware       |
+-----------------------+     +-----------------------+     +-----------------------+
    Physical Machine                  Container                 Type II Hypervisor
```

上图中，每一个App和Lib的组合，就是一个容器。也就是Docker图标里面的一个集装箱。和虚拟机相比，容器有以下优点：

1. 迅速启动：没有虚拟机硬件的初始化，没有Guest OS的启动过程，可以节约很多启动时间，这就是容器的“开箱即用”。
2. 占用资源少：没有运行Guest OS所需的内存开销，无需为虚拟机预留运行内存，无需安装、运行App不需要的运行库/操作系统服务，内存占用、存储空间占用都小的多。

[**当然，和虚拟机相比，因为共用内核，只靠cgroup隔离，应用之间的隔离是不如虚拟机彻底的**]()，如果某个应用运行时导致内核崩溃，所有的容器都会崩溃。而虚拟机内的应用崩溃，理论上是不会影响其它虚拟机以及上面运行的应用的，除非是硬件或者Hypervisor有Bug。

[**Docker把App和Lib的文件打包成为一个镜像**]()，并且采用类似多次快照的存储技术，例如aufs/device mapper/btrfs/zfs等，可以实现：

1. 多个App可以共用相同的底层镜像（初始的操作系统镜像）
2. App运行时的IO操作和镜像文件隔离；
3. 通过挂载包含不同配置/数据文件的目录或者卷（Volume），[**单个App镜像可以同时用来运行无数个不同业务的容器。**]()

```text
+---------+  +---------+  +---------+    +-----+ +-----+ +-----+
| abc.com |  | def.com |  | xyz.com |    | DB1 | | DB2 | | DB3 |    
+----+----+  +----+----+  +----+----+    +--+--+ +--+--+ +--+--+    
     |            |            |            |       |       |
+----+----+  +----+----+  +----+----+    +--+--+ +--+--+ +--+--+    
|   abc   |  | def.com |  | xyz.com |    | DB1 | | DB2 | | DB3 |
| config  |  | config  |  | config  |    | conf| | conf| | conf|
|  data   |  |  data   |  |  data   |    | data| | data| | data|
+----+----+  +----+----+  +----+----+    +--+--+ +--+--+ +--+--+
     |            |            |            |       |       |
     +------------+------------+            +-------+-------+
                  |                                 |
           +------+------+                   +------+------+          
           | Nginx Image |                   | MySQL Image |
           +------+------+                   +------+------+
                  |                                 |
                  +----------------+----------------+
                                   |
                            +------+-------+ 
                            | Alpine Image |
                            +------+-------+
```

上图是基于一个Alpine Linux的镜像，分别建立了Nginx和MySQL的镜像，并且挂载不同的配置/数据同时运行3个网站应用3个数据库应用的示意图。

此外，Docker公司提供公共的镜像仓库（Docker称之为Repository），Github connect，自动构建镜像，大大简化了应用分发、部署、升级流程。加上Docker可以非常方便的建立各种自定义的镜像文件，这些都是Docker成为最流行的容器技术的重要因素。

通过以上这些技术的组合，最后的结果就是，

- 绝大部分应用，开发者都可以通过docker build创建镜像，通过docker push上传镜像，用户通过docker pull下载镜像，用docker run运行应用。
- 用户不需要再去关心如何搭建环境，如何安装，如何解决不同发行版的库冲突
- 而且通常不会需要消耗更多的硬件资源，不会明显降低性能。这就是其他答主所说的标准化、集装箱的原因所在。

题外话：Windows因为采用微内核，且**内核与各种运行库耦合紧密**，虽然从Windows 10/2016开始也支持容器，但事实上还是通过Hyper-V运行不同的虚拟机进行内核级隔离——虽然也有线程级的隔离，但只有Windows Server支持，并且只能运行相同版本的镜像[1]。而且即使是Hyper-V，也只支持运行更低版本的镜像而不能运行更高版本的镜像。另外Windows容器的镜像体积通常还是很大。

## Docker Component

<img src="picture/中间件整理.assets/4269c5a4.png" alt="docker组成" style="zoom:67%;" />

docker组成

Docker是CS架构，主要由下面三部分组成： 

* Docker daemon: 运行在宿主机上，Docker守护进程，用户通过Docker client(Docker命令)与Docker daemon交互 
* Docker client: Docker 命令行工具，是用户使用Docker的主要方式，Docker client与Docker daemon通信并将结果返回给用户，Docker client也可以通过socket或者RESTful api访问远程的Docker daemon *
* Docker hub/registry: 共享和管理Docker镜像，用户可以上传或者下载上面的镜像，官方地址为https://registry.hub.docker.com/，也可以搭建自己私有的[Docker registry](https://github.com/docker/docker-registry)。

了解了Docker的组成，再来了解一下Docker的两个主要概念： 

* Docker image：镜像是只读的，镜像中包含有需要运行的文件。镜像用来创建container，一个镜像可以运行多个container；镜像可以通过Dockerfile创建，也可以从Docker hub/registry上下载。 
* Docker container：容器是Docker的运行组件，启动一个镜像就是一个容器，容器是一个隔离环境，多个容器之间不会相互影响，保证容器中的程序运行在一个相对安全的环境中。

## Docker网络

Docker的网络功能相对简单，没有过多复杂的配置，Docker默认使用birdge桥接方式与容器通信，启动Docker后，宿主机上会产生`docker0`这样一个虚拟网络接口， docker0不是一个普通的网络接口， 它是一个虚拟的以太网桥，可以为绑定到docker0上面的网络接口自动转发数据包，这样可以使容器与宿主机之间相互通信。

每次Docker创建一个容器，会产生一对虚拟接口，在宿主机上执行`ifconfig`，会发现多了一个类似`veth****`这样的网络接口，它会绑定到docker0上，由于所有容器都绑定到docker0上，容器之间也就可以通信。

在宿主机上执行ifconfig，会看到docker0这个网络接口， 启动一个container，再次执行`ifconfig`, 会有一个类似`veth****`的interface，每个container的缺省路由是宿主机上docker0的ip，在container中执行`netstat -r`可以看到如下图所示内容：

![container路由](picture/中间件整理.assets/4a9b820d.png)

container路由



容器中的默认网关跟docker0的地址是一样的：

![docker0](picture/中间件整理.assets/27b63799.png)

docker0

当容器退出之后，veth***虚拟接口也会被销毁。

除bridge方式，Docker还支持host、container、none三种网络通信方式，使用其它通信方式，只要在Docker启动时，指定–net参数即可，比如:

```
docker run -i -t  --net=host ubuntu /bin/bash
```

host方式可以让容器无需创建自己的网络协议栈，而直接访问宿主机的网络接口，在容器中执行ip addr会发现与宿主机的网络配置是一样的，host方式让容器直接使用宿主机的网络接口，传输数据的效率会更加高效，避免bridge方式带来的额外开销，但是这种方式也可以让容器访问宿主机的D-bus等网络服务，可能会带来意想不到的安全问题，应谨慎使用host方式；container方式可以让容器共享一个已经存在容易的网络配置； none方式不会对容器的网络做任务配置，需要用户自己去定制。

## Docker 使用

首先要在宿主机上安装Docker，Docker安装参考[官方安装文档](https://docs.docker.com/installation/)。 Docker命令也比较类似Git，支持push以及pull操作上传以及下载Docker镜像。 查看当前Docker的版本

```
docker version
```

查看当前系统Docker信息

```
docker info
```

查看宿主机上的镜像，Docker镜像保存在`/var/lib/docker`目录下:

```
docker images
```

从Docker hub上下载某个镜像:

```
docker pull ubuntu:latest
docker pull ubuntu:latest
```

执行`docker pull ubuntu`会将Ubuntu这个仓库下面的所有镜像下载到本地repository。

启动一个容器使用`docker run`:

```
docker run -i -t ubuntu /bin/bash                       启动一个容器
docker run -i -t --rm ubuntu /bin/bash                  --rm表示容器退出后立即删除该容器
docker run -t -i --name test_container ubuntu /bin/bash --name指定容器的名称，否则会随机分配一个名称
docker run -t -i --net=host ubuntu /bin/bash            --net=host容器以Host方式进行网络通信
docker run -t -i -v /host:/container ubuntu /bin/bash   -v绑定挂在一个Volume，在宿主机和Docker容器中共享文件或目录
```

查看当前有哪些容器正在运行，使用`docker ps`:

```
xzs@host:~(0)$ docker ps
CONTAINER ID     IMAGE                COMMAND        CREATED         STATUS          PORTS    NAMES
50a1261f7a8b     docker_test:latest   "/bin/bash"    7 seconds ago   Up 6 seconds             sleepy_ptolemy
#目前只有一个container id为50a1261f7a8b的容器正在运行
```

启动或停止某个container使用`docker start/stop container_id`:

```
xzs@host:~(0)$ docker stop 50a1261f7a8b
50a1261f7a8b

xzs@host:~(0)$ docker ps -a | grep 50a1261f7a8b
50a1261f7a8b   docker_test:latest   "/bin/bash"   2 minutes ago   Exited (0) 14 seconds ago   sleepy_ptolemy
#执行docker stop后，该容器的状态变更为Exited
```

使用`docker commit`可以将container的变化作为一个新的镜像，比如:

```
xzs@host:~(0)$ docker commit -m="test docker commit" 50a1261f7a8b docker_test
55831c956ebf46a1f9036504abb1b29d7e12166f18f779cccce66f5dc85de38e

xzs@host:~(0)$ docker images | grep docker_test
docker_test                            latest              55831c956ebf        10 seconds ago      290.7 MB
```

除了从Docker hub上下载镜像，也可以写Dockerfile创建一个镜像，以创建一个Django程序为例，Dockerfile如下所示：

```
xzs@host:/tmp/docker(0)$ cat Dockerfile
FROM ubuntu:12.04
MAINTAINER Your Name

RUN apt-get update
RUN apt-get install -y python-software-properties python-pip

ADD myproject /opt/code

RUN pip install -r /opt/code/requirement.txt
```

写完Dockerfile，在Dockerfile所在目录执行`docker build`创建镜像:

```
docker build -t docker_test .
docker run -i -t docker_test /bin/bash -c "cd /opt/code;python manage.py runserver 0.0.0.0:8080"
```

将制作的镜像上传到private registry:

```
docker tag test docker.example.com/test
docker push docker.example.com/test
```

经过长时间使用，主机上存储了很多已无用的镜像，想将它们删除则用`docker rm`或者`docker rmi`，比如:

```
docker rm container_id
docker rmi image_id
```

## Docker生态



Docker生态中还有一个非常重要的容器管理工具–Kubernetes，它是Google开源的用于在集群环境中管理、维护、自动扩展容器，通过Kubernetes可以很方便地在多个机器上管理和部署容器服务。现在已经得到IBM、Microsoft、RedHat等多个大公司的支持。

在Kubernetes中pod是一个基本单元，一个pod可以是提供相同功能的多个container，这些容器会被部署在同一个minion上。Replication controller定义了多个pod或者容器需要运行，如果当前集群中运行的pod或容器达不到配置的数量，replication controller会调度容器在多个minion上运行，保证集群中的pod数量。service则定义真实对外提供的服务，一个service会对应后端运行的多个container。Kubernetes的架构由一个master和多个minion组成，master通过api提供服务，接受kubectl的请求来调度管理整个集群。minion是运行Kubelet的机器，它接受master的指令创建pod或者容器。

